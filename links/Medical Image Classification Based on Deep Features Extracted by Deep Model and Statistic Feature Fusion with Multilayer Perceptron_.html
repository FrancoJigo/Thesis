<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0053)https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/ -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <!-- AppResources meta begin -->
        <script type="text/javascript">var ncbi_startTime = new Date();</script>
        <!-- AppResources meta end -->
        
        <!-- TemplateResources meta begin -->
        <meta name="paf_template" content="">

        <!-- TemplateResources meta end -->
        
        <!-- Logger begin -->
        <meta name="ncbi_db" content="pmc"><meta name="ncbi_pdid" content="article"><meta name="ncbi_acc" content=""><meta name="ncbi_domain" content="cin"><meta name="ncbi_report" content="record"><meta name="ncbi_type" content="fulltext"><meta name="ncbi_objectid" content=""><meta name="ncbi_pcid" content="/articles/PMC6157177/"><meta name="ncbi_app" content="pmc">
        <!-- Logger end -->
        
        <title>Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron‬</title>
        
        <!-- AppResources external_resources begin -->
        <link rel="stylesheet" href="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/jig.min.css"><script type="text/javascript" src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/jig.min.js.download"></script>

        <!-- AppResources external_resources end -->
        
        <!-- Page meta begin -->
        <meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE"><link rel="canonical" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/"><link rel="schema.DC" href="http://purl.org/DC/elements/1.0/"><meta name="citation_journal_title" content="Computational Intelligence and Neuroscience"><meta name="citation_title" content="Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron‬"><meta name="citation_authors" content="ZhiFei Lai, HuiFang Deng"><meta name="citation_date" content="2018"><meta name="citation_volume" content="2018"><meta name="citation_doi" content="10.1155/2018/2061516"><meta name="citation_abstract_html_url" content="/pmc/articles/PMC6157177/?report=abstract"><meta name="citation_pmid" content="30298088"><meta name="DC.Title" content="Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron‬"><meta name="DC.Type" content="Text"><meta name="DC.Publisher" content="Hindawi Limited"><meta name="DC.Contributor" content="ZhiFei Lai"><meta name="DC.Contributor" content="HuiFang Deng"><meta name="DC.Date" content="2018"><meta name="DC.Identifier" content="10.1155/2018/2061516"><meta name="DC.Language" content="en"><meta property="og:title" content="Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron‬"><meta property="og:type" content="article"><meta property="og:description" content="Medical image classification is a key technique of Computer-Aided Diagnosis (CAD) systems. Traditional methods rely mainly on the shape, color, and/or texture features as well as their combinations, most of which are problem-specific and have shown to ..."><meta property="og:url" content="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/"><meta property="og:site_name" content="PubMed Central (PMC)"><meta property="og:image" content="https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-logo-share.png"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ncbi"><meta name="ncbi_feature" content="associated_data"><link rel="stylesheet" href="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/pmc.min.css" type="text/css"><link rel="stylesheet" href="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/pmc_extras_prnt.min.css" type="text/css" media="print"><script type="text/javascript" src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/common.min.js.download">//</script><script type="text/javascript" src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/NcbiTagServer.min.js.download">//</script><meta name="citationexporter" content="backend:&#39;https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/&#39;"><script type="text/javascript" src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/jquery.citationexporter.min.js.download">//</script><link rel="stylesheet" href="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/citationexporter.css" type="text/css"><script type="text/javascript" src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/MathJax.js.download"></script><script type="text/javascript">window.name="mainwindow";</script><style type="text/css">.pmc-wm {background:transparent repeat-y top left;background-image:url(/corehtml/pmc/pmcgifs/wm-cin.gif);background-size: auto, contain}</style><style type="text/css">.print-view{display:block}</style><style type="text/css">
        div.pmc_para_cit li.highlight,
        div.pmc_para_cit li.highlight .one_line_source
        { background: #E0E0E0; }
        a.bibr.highlight { background: #E0E0E0; } 
      </style><meta name="cited_in_systematic_reviews" content=""><link rel="alternate" type="application/epub+zip" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/epub/"><link rel="alternate" type="application/pdf" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/pdf/CIN2018-2061516.pdf">

        <!-- Page meta end -->
    <link rel="shortcut icon" href="https://www.ncbi.nlm.nih.gov/favicon.ico"><meta name="ncbi_phid" content="CE895B0ACE93DFA10000000001BF0111.m_8">
<meta name="referrer" content="origin-when-cross-origin"><link type="text/css" rel="stylesheet" href="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/4143404.css"><link type="text/css" rel="stylesheet" href="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/4157116.css" media="print"><script async="1" src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/analytics.js.download"></script><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?rev=2.6.1') format('woff'), url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?rev=2.6.1') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?rev=2.6.1') format('woff'), url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?rev=2.6.1') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?rev=2.6.1') format('woff'), url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?rev=2.6.1') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?rev=2.6.1') format('woff'), url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?rev=2.6.1') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?rev=2.6.1') format('woff'), url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?rev=2.6.1') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?rev=2.6.1') format('woff'), url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?rev=2.6.1') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?rev=2.6.1') format('woff'), url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?rev=2.6.1') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?rev=2.6.1') format('woff'), url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?rev=2.6.1') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?rev=2.6.1') format('woff'), url('https://www.ncbi.nlm.nih.gov/core/mathjax/2.6.1/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?rev=2.6.1') format('opentype')}
</style></head>
    <body class="article" id="ui-ncbiexternallink-4"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>
        <div class="grid">
            <div class="col twelve_col nomargin shadow">
                <!-- System messages like service outage or JS required; this is handled by the TemplateResources portlet -->
                <div class="sysmessages">
                    <noscript>
	<p class="nojs">
	<strong>Warning:</strong>
	The NCBI web site requires JavaScript to function. 
	<a href="/guide/browsers/#enablejs" title="Learn how to enable JavaScript" target="_blank">more...</a>
	</p>
	</noscript>
                </div>
                <!--/.sysmessage-->
                <div class="wrap">
                    <div class="page">
                        <div class="top">
                            <div class="universal_header" id="universal_header"><ul class="inline_list jig-ncbimenu ui-ncbimenu resources_list ui-ncbibasicmenu orientation_hort" id="navcontent" role="menubar"><li class="ui-ncbimenu-item-leaf ui-ncbimenu-item-first ui-helper-reset ui-ncbimenu-item-no-hlt" role="presentation"><a class="ui-ncbimenu-link-first" href="https://www.ncbi.nlm.nih.gov/" role="menuitem" title="NCBI Home" id="ncbihome" accesskey="1"><span class="offscreen_noflow">NCBI</span><img src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/28977" class="ncbi_logo" title="NCBI" alt="NCBI Logo"></a></li><li class="offscreen_noflow ui-ncbimenu-item-skip access"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#maincontent" title="Skip to the content" tabindex="0" accesskey="3">Skip to main
                        content</a></li><li class="offscreen_noflow ui-ncbimenu-item-skip access"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#navcontent" title="Skip to the navigation" tabindex="0" accesskey="4">Skip to
                        navigation</a></li><li id="resource-menu" class="topmenu ui-helper-reset ui-ncbimenu-item-first ui-helper-reset" role="presentation"><a class="ui-ncbimenu-first-link-has-submenu ui-ncbimenu-link-first topanchor expandDown" href="https://www.ncbi.nlm.nih.gov/static/header_footer_ajax/submenu/#resources" tabindex="-1" role="menuitem">Resources</a></li><li id="all-howtos-menu" class="topmenu ui-helper-reset ui-ncbimenu-item-first" role="presentation"><a class="ui-ncbimenu-first-link-has-submenu ui-ncbimenu-link-first topanchor expandDown" href="https://www.ncbi.nlm.nih.gov/static/header_footer_ajax/submenu/#howto" tabindex="-1" role="menuitem">How To</a></li><li class="offscreen_noflow ui-ncbimenu-item-skip access"><a href="https://www.ncbi.nlm.nih.gov/guide/browsers/#accesskeys" title="About My NCBI Accesskeys" tabindex="0" accesskey="0">About NCBI Accesskeys</a></li></ul><div class="myncbi" style="top: -0.4em;"><iframe src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/emyncbi.html" style="border:none; height:2.5em; width:40em;" scrolling="no"></iframe></div></div>
                            <div class="header">
    <div class="res_logo">
  <h1 class="img_logo"><a href="https://www.ncbi.nlm.nih.gov/pmc/" class="pmc_logo offscreen">PMC</a></h1>
  <div class="NLMLogo">
    <a href="https://www.nlm.nih.gov/" title="US National Library of Medicine">US National Library of Medicine</a>
    <br>
    <a href="https://www.nih.gov/" title="National Institutes of Health">National Institutes of Health</a>
  </div>
</div>
    <div class="search"><form method="get" action="https://www.ncbi.nlm.nih.gov/pmc/"><div class="search_form"><label for="database" class="offscreen_noflow">Search database</label><select id="database"><optgroup label="Recent"><option value="pmc" selected="selected" class="last" data-ac_dict="pmc-search-autocomplete">PMC</option></optgroup><optgroup label="All"><option value="gquery">All Databases</option><option value="assembly">Assembly</option><option value="biocollections">Biocollections</option><option value="bioproject">BioProject</option><option value="biosample">BioSample</option><option value="biosystems">BioSystems</option><option value="books">Books</option><option value="clinvar">ClinVar</option><option value="clone">Clone</option><option value="cdd">Conserved Domains</option><option value="gap">dbGaP</option><option value="dbvar">dbVar</option><option value="nucest">EST</option><option value="gene">Gene</option><option value="genome">Genome</option><option value="gds">GEO DataSets</option><option value="geoprofiles">GEO Profiles</option><option value="nucgss">GSS</option><option value="gtr">GTR</option><option value="homologene">HomoloGene</option><option value="ipg">Identical Protein Groups</option><option value="medgen">MedGen</option><option value="mesh">MeSH</option><option value="ncbisearch">NCBI Web Site</option><option value="nlmcatalog">NLM Catalog</option><option value="nuccore">Nucleotide</option><option value="omim">OMIM</option><option value="pmc" data-ac_dict="pmc-search-autocomplete">PMC</option><option value="popset">PopSet</option><option value="probe">Probe</option><option value="protein">Protein</option><option value="proteinclusters">Protein Clusters</option><option value="pcassay">PubChem BioAssay</option><option value="pccompound">PubChem Compound</option><option value="pcsubstance">PubChem Substance</option><option value="pubmed">PubMed</option><option value="snp">SNP</option><option value="sparcle">Sparcle</option><option value="sra">SRA</option><option value="structure">Structure</option><option value="taxonomy">Taxonomy</option><option value="toolkit">ToolKit</option><option value="toolkitall">ToolKitAll</option><option value="toolkitbookgh">ToolKitBookgh</option><option value="unigene">UniGene</option></optgroup></select><div class="nowrap"><label for="term" class="offscreen_noflow" accesskey="/">Search term</label><div class="nowrap"><div class="jig-ncbiclearbutton-wrap ui-ncbiclearbutton-wrap box-shadow"><input type="text" name="term" id="term" title="Search PMC. Use up and down arrows to choose an item from the autocomplete." value="" class="jig-ncbiclearbutton jig-ncbiautocomplete" data-jigconfig="dictionary:&#39;pmc-search-autocomplete&#39;,disableUrl:&#39;NcbiSearchBarAutoComplCtrl&#39;" autocomplete="off" data-sbconfig="ds:&#39;no&#39;,pjs:&#39;no&#39;,afs:&#39;yes&#39;" aria-haspopup="true" aria-autocomplete="list" role="textbox"><a class="reset" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" style="visibility: hidden;"><img src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/clear.png" alt="Clear input"></a></div></div><button id="search" type="submit" class="button_search nowrap" cmd="go">Search</button></div></div></form><ul class="searchlinks inline_list"><li>
                        <a href="https://www.ncbi.nlm.nih.gov/pmc/advanced/">Advanced</a>
                    </li><li>
                        <a href="https://www.ncbi.nlm.nih.gov/pmc/journals/">Journal list</a>
                    </li><li class="help">
                        <a target="_blank" href="https://www.ncbi.nlm.nih.gov/books/NBK3825/">Help</a>
                    </li></ul></div>
</div>

                            
                            
                        <!--<component id="Page" label="headcontent"/>-->
                            
                        </div>
                        <div class="content">
                            <!-- site messages -->
                            <div class="container">
    <div id="maincontent" class="content eight_col col">
        <div class="navlink-box">
            <ul class="page-breadcrumbs inline_list small"><li class="journal-list"><a href="https://www.ncbi.nlm.nih.gov/pmc/journals/" class="navlink">Journal List</a></li><li class="archive"><a class="navlink" href="https://www.ncbi.nlm.nih.gov/pmc/journals/526/">Comput Intell Neurosci</a></li><li class="issue-page"><a class="navlink" href="https://www.ncbi.nlm.nih.gov/pmc/issues/306880/">v.2018; 2018</a></li><li class="accid">PMC6157177</li></ul>
        </div>

        <!-- Journal banner -->
        <div class="pmc-page-banner whole_rhythm"><div><img src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/logo-cin.gif" alt="Logo of cin" usemap="#logo-imagemap"><map id="logo-imagemap" name="logo-imagemap"><area shape="rect" coords="0,0,500,75" alt="Computational Intelligence and Neuroscience" title="Computational Intelligence and Neuroscience" href="http://www.hindawi.com/journals/cin" ref="reftype=publisher&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CBanner&amp;TO=Publisher%7COther%7CN/A" target="pmc_ext"></map></div> </div>
        
        <!--component id='MainPortlet' label='search-reference'/-->
        
        <!-- Book content -->
        <div class="">
            
        
            
            <div class="hide-overflow article lit-style content pmc-wm slang-all page-box"><!--main-content--><div class="jig-ncbiinpagenav" data-jigconfig="smoothScroll: false, allHeadingLevels: [&#39;h2&#39;], headingExclude: &#39;:hidden&#39;" id="ui-ncbiinpagenav-1"><div class="fm-sec half_rhythm no_top_margin"><div class="fm-citation half_rhythm no_top_margin clearfix"><div class="inline_block eight_col va_top"><div><div><span class="cit"><span role="menubar"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" role="menuitem" aria-expanded="false" aria-haspopup="true">Comput Intell Neurosci</a></span>. 2018; 2018: 2061516. </span></div><div><span class="fm-vol-iss-date">Published online 2018 Sep 12. </span>  <span class="doi">doi:&nbsp;<a href="https://dx.doi.org/10.1155%2F2018%2F2061516" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CFront%20Matter&amp;TO=Content%20Provider%7CCrosslink%7CDOI">10.1155/2018/2061516</a></span></div></div></div><div class="inline_block four_col va_top show-overflow align_right"><div class="fm-citation-ids"><div class="fm-citation-pmcid"><span class="fm-citation-ids-label">PMCID: </span><span>PMC6157177</span></div><div class="fm-citation-pmid">PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/30298088">30298088</a></div></div></div></div><h1 class="content-title">Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron<sup>‬</sup></h1><div class="half_rhythm"><div class="contrib-group fm-author"><a href="https://www.ncbi.nlm.nih.gov/pubmed/?term=Lai%20Z%5BAuthor%5D&amp;cauthor=true&amp;cauthor_uid=30298088" class="affpopup" co-rid="_co_idm140658771654768" co-class="co-affbox">ZhiFei Lai</a><sup><img src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/corrauth.gif" alt="corresponding author"></sup><sup></sup> and  <a href="https://www.ncbi.nlm.nih.gov/pubmed/?term=Deng%20H%5BAuthor%5D&amp;cauthor=true&amp;cauthor_uid=30298088" class="affpopup" co-rid="_co_idm140658772105712" co-class="co-affbox">HuiFang Deng</a><sup></sup></div><div style="display:none" class="contrib-group aff-tip"><div id="_co_idm140658771654768"><h3 class="no_margin">ZhiFei Lai</h3><p>Department of Computer Science and Engineering, South China University of Technology, Guangzhou 510006, China</p><div>Find articles by <a href="https://www.ncbi.nlm.nih.gov/pubmed/?term=Lai%20Z%5BAuthor%5D&amp;cauthor=true&amp;cauthor_uid=30298088">ZhiFei Lai</a></div></div><div id="_co_idm140658772105712"><h3 class="no_margin">HuiFang Deng</h3><p>Department of Computer Science and Engineering, South China University of Technology, Guangzhou 510006, China</p><div>Find articles by <a href="https://www.ncbi.nlm.nih.gov/pubmed/?term=Deng%20H%5BAuthor%5D&amp;cauthor=true&amp;cauthor_uid=30298088">HuiFang Deng</a></div></div></div></div><div class="fm-panel half_rhythm"><div class="togglers"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" class="pmctoggle" rid="idm140658775605584_ai">Author information</a> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" class="pmctoggle" rid="idm140658775605584_an">Article notes</a> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" class="pmctoggle" rid="idm140658775605584_cpl">Copyright and License information</a> <a href="https://www.ncbi.nlm.nih.gov/pmc/about/disclaimer/">Disclaimer</a></div><div class="fm-authors-info fm-panel hide half_rhythm" id="idm140658775605584_ai" style="display:none"><div class="fm-affl" lang="en" id="I1">Department of Computer Science and Engineering, South China University of Technology, Guangzhou 510006, China</div><div><sup><img src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/corrauth.gif" alt="corresponding author"></sup>Corresponding author.</div><div><span class="contrib-email" id="idm140658771654768">ZhiFei Lai: <a href="mailto:dev@null" data-email="moc.361@3435fzl" class="oemail">moc.361@3435fzl</a></span> </div><div id="idm140658768779824">Academic Editor: José Alfredo Hernández-Pérez</div></div><div class="fm-article-notes fm-panel hide half_rhythm" id="idm140658775605584_an" style="display:none"><div class="fm-pubdate half_rhythm">Received 2018 Mar 3; Accepted 2018 Aug 9.</div></div><div class="permissions fm-panel half_rhythm hide" id="idm140658775605584_cpl" style="display:none"><div class="fm-copyright half_rhythm"><a href="https://www.ncbi.nlm.nih.gov/pmc/about/copyright/">Copyright</a>  © 2018 ZhiFei Lai and HuiFang Deng.</div><div class="license half_rhythm">This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</div></div></div><div id="pmclinksbox" class="links-box whole_rhythm hidden"></div></div><div class="sec"></div><div id="ass-data" class="tsec fm-sec whole_rhythm" data-section="Featured_PMC_Datacitation"><div class="goto jig-ncbiinpagenav-goto-container"><span role="menubar"><a class="tgt_dark page-toc-label jig-ncbiinpagenav-goto-heading" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" title="Go to other sections in this page" role="menuitem" aria-expanded="false" aria-haspopup="true">Go to:</a></span></div><h2 class="ui-helper-clearfix" id="ui-ncbiinpagenav-heading-3">Associated Data</h2><dl data-length="153" class="box-data-avail whole_rhythm no_bottom_margin"><dt><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" rid="data-avl-stmnt" data-ga-action="click_feat_toggler" data-ga-label="Data Availability Statement" class="pmctoggle">Data Availability Statement</a></dt><dd id="data-avl-stmnt" style="display: none;"><p class="p p-first-last">The ISIC dataset is taken from the website <a href="https://challenge.kitware.com/" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CBody&amp;TO=External%7CLink%7CURI" target="_blank">https://challenge.kitware.com/</a>. And the histology 2828 dataset is taken from <a href="http://www.informed.unal.edu.co/" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CBody&amp;TO=External%7CLink%7CURI" target="_blank">http://www.informed.unal.edu.co/</a>.</p></dd></dl></div><div id="idm140658773571104" lang="en" class="tsec sec"><div class="goto jig-ncbiinpagenav-goto-container"><a class="tgt_dark page-toc-label jig-ncbiinpagenav-goto-heading" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" title="Go to other sections in this page" role="button" aria-expanded="false" aria-haspopup="true">Go to:</a></div><h2 class="head no_bottom_margin ui-helper-clearfix" id="idm140658773571104title">Abstract</h2><!--article-meta--><div><p id="idm140658773570976" class="p p-first-last">Medical image classification is a key technique of Computer-Aided Diagnosis (CAD) systems. Traditional methods rely mainly on the shape, color, and/or texture features as well as their combinations, most of which are problem-specific and have shown to be complementary in medical images, which leads to a system that lacks the ability to make representations of high-level problem domain concepts and that has poor model generalization ability. Recent deep learning methods provide an effective way to construct an end-to-end model that can compute final classification labels with the raw pixels of medical images. However, due to the high resolution of the medical images and the small dataset size, deep learning models suffer from high computational costs and limitations in the model layers and channels. To solve these problems, in this paper, we propose a deep learning model that integrates Coding Network with Multilayer Perceptron (CNMP), which combines high-level features that are extracted from a deep convolutional neural network and some selected traditional features. The construction of the proposed model includes the following steps. First, we train a deep convolutional neural network as a coding network in a supervised manner, and the result is that it can code the raw pixels of medical images into feature vectors that represent high-level concepts for classification. Second, we extract a set of selected traditional features based on background knowledge of medical images. Finally, we design an efficient model that is based on neural networks to fuse the different feature groups obtained in the first and second step. We evaluate the proposed approach on two benchmark medical image datasets: HIS2828 and ISIC2017. We achieve an overall classification accuracy of 90.1% and 90.2%, respectively, which are higher than the current successful methods.</p></div></div><div id="sec1" class="tsec sec"><div class="goto jig-ncbiinpagenav-goto-container"><a class="tgt_dark page-toc-label jig-ncbiinpagenav-goto-heading" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" title="Go to other sections in this page" role="button" aria-expanded="false" aria-haspopup="true">Go to:</a></div><h2 class="head no_bottom_margin ui-helper-clearfix" id="sec1title">1. Introduction</h2><p id="idm140658773570208" class="p p-first">With the rapid development of digital image acquisition and storage technologies, image understanding by computer programs has become an attractive and active topic in the machine learning field and in application-specific studies [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B1" rid="B1" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">1</a>]. Toward building an intelligent Computer-Aided Diagnosis (CAD) system, fast and accurate annotation or the grading of medical images has become a key technique in CAD systems, in most medical fields. For example, many people are diagnosed with skin cancer in the United States every year [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B2" rid="B2" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">2</a>]. If detected at an earlier stage, it would save many lives. A large number of research papers are reported in the area of medical image classification. However, medical images obtained from different sources may be variant from focusing region, contrast, and white balance. In addition, medical images usually have inner structures with different textures and pixels density. If we used only traditional features to classify medical images, it would be difficult to characterize certain classes efficiently [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B3" rid="B3" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">3</a>]. In the past few years, deep learning has become one of the hottest research areas in computer science and computer applications. Because of the advances in deep learning, many researchers have attempted to use this new technique to address nonmedical images. Hinton et al. [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B4" rid="B4" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">4</a>] first discussed the framework of the deep model. Henceforth, a variety of deep models have been proposed to solve image problems. Krizhevsky et al. [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B5" rid="B5" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">5</a>] trained the deep convolutional neural network to classify images in ImageNet Large-Scale Visual Recognition Challenge 2010 (ILSVRC-2010), achieving state-of-the-art performance. In Reference [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B6" rid="B6" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">6</a>], the authors discussed the effect of the deep mode depth on the performance in an image recognition task. Inspired by these successful studies, this approach has already attracted considerable work on leveraging this new methodology to solve medical image classification problems.</p><p id="idm140658773562320">Medical image classification is one of the most important problems in the image recognition area, and its aim is to classify medical images into different categories to help doctors in disease diagnosis or further research. Overall, medical image classification can be divided into two steps. The first step is extracting effective features from the image. The second step is using the features to build models that classify the image dataset. In the past, doctors usually used their professional experience to extract features to classify the medical images into different classes, which is usually a difficult, boring, and time-consuming task. This approach is prone to leading to instability or nonrepeatable outcomes. Considering the research until now, medical image classification application research has had great merit. The researchers' efforts have led to a large number of published studies in this area. However, at present, we still cannot accomplish this mission efficiently. If we could finish the classification work excellently, then the results would help medical doctors to diagnose diseases with further study. Therefore, how to effectively solve this task is of great importance.</p><p id="idm140658779803728">Considering past work, we have observed that a large number of previous studies [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B7" rid="B7" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">7</a>–<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B13" rid="B13" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">13</a>] used shallow models for medical image classification, which rely mainly on the shape, color, and/or texture features as well as their combinations, before deep architectures appeared. However, for all of these models, the largest problem is that the extracted features are often referred to as low-level features; these features lack representation ability for high-level problem domain concepts, and their generalization ability is rather poor. In contrast, deep architectures [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B14" rid="B14" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">14</a>–<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B16" rid="B16" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">16</a>] have achieved a large amount of success in the nonmedical image field. Deep learning-based methods, which are the most breathtaking branch of the machine learning field, provide an effective way to construct an end-to-end model that can compute final classification labels with the raw pixels of medical images. The applications of deep models in the medical image analysis domain require great effort to catch up with other areas of imaging because deep architectures require large datasets to obtain outstanding features. However, medical images are usually difficult to acquire, and thus, medical datasets are typically relatively small. Therefore, the approach is apt to lead to overfitting of the model if we directly use a deep model to address a small dataset. Except for these problems, the interpretability of the model has been proven to be rather poor, and training a deep model usually requires a large amount of computation. To overcome these concerns about traditional methods versus deep models, we present a special novel deep model that combines traditional features; this model can not only take full advantage of the existing doctors experiences but also utilize deep architectures to automatically extract high-level features for classify the medical images.</p><p id="idm140658847830624">In this paper, we will focus on this novel and effective method of learning multiscale features that combine deep models with traditional image characteristics, which is referred to as CNMP. The main reason for applying this method to medical image classification is that we want to help doctors save time and energy by automatically classifying images. Moreover, it is noteworthy that the notable factor of our method is extracting features from the corresponding images, for which the deep model can automatically extract features and the traditional algorithms manually extract handcrafted features. This approach can simultaneously use both high- and low-level representations of an image and avoid any single representation or feature. Furthermore, it can automatically fuse two types of features, thus avoiding tiresome parameter selection.</p><p id="idm140658847829504">There are at least two challenges to medical image classification, as follows:</p><ol class="enumerated" style="list-style-type:lower-roman"><!--
list-behavior=enumerated
prefix-word=
mark-type=lower-roman
max-label-size=0
--><li><p id="idm140658779450080">How can we extract effective features from a small medical image dataset? In general, medical image datasets are so small that we cannot obtain sufficient information to extract discriminative features. Without regard to the size of the image dataset, even if the proposed method can gain very good classification accuracy, the actual application value is extremely limited. In Reference [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B5" rid="B5" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">5</a>], a new data augmentation method is proposed to avoid small datasets leading to acquiring nonvalid features. Then, they used an extension dataset to gain good performance of their model. Therefore, it is meaningful to find a method that can extract discriminative features from a small dataset.</p></li><li><p id="idm140658774176256">How to quickly and efficiently fuse different types of features from different models? It appears easy to formulate the idea of directly combining the feature vectors into a larger feature vector and determining one proportion parameter between different features. However, this method usually requires trial experiments to train the parameters and cannot obtain a better outcome. If we could design a more favorable fusion approach, then it would gain better accuracy performance than any of these methods. Therefore, there is a great demand to effectively fuse the features.</p></li></ol><p></p><p id="idm140658775086816">The main contributions of this paper can be summarized as follows:</p><ol class="enumerated" style="list-style-type:lower-roman"><!--
list-behavior=enumerated
prefix-word=
mark-type=lower-roman
max-label-size=0
--><li><p id="idm140658775086064">We have proposed a deep model that combines high-level features with traditional features to classify medical images. It directly trained the deep convolutional neural network called the coding network to extract high-level features rather than using domain-transferred convolutional neural networks such as domain-transferred convolutional neural networks (DT-CNNs) described in [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B17" rid="B17" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">17</a>]. By means of adding traditional medical image features, the interpretability of the deep model and achieve the best performance could be improved.</p></li><li><p id="idm140658771303024">We have implemented two approaches to fusing the high-level features and traditional features. One method is to assign a fixed argument representation of the proposition between the high-level features and traditional features, in that the traditional procedure is boring, time-consuming, and difficult to put into practice. To conquer these limitations, another approach is proposed, which is a new framework that can not only fuse the features together but also automatically adjust their proportions.</p></li></ol><p></p><p id="idm140658777072688" class="p p-last">The remainder of this paper is organized as follows: we review the previous related work on image classification in <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#sec2" rid="sec2" class=" sec">Section 2</a>. The detailed description of the algorithm will be described in <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#sec3" rid="sec3" class=" sec">Section 3</a>. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#sec4" rid="sec4" class=" sec"> Section 4</a> presents the experimental results on two different datasets. Last, conclusions and future work are given in <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#sec5" rid="sec5" class=" sec">Section 5</a>.</p></div><div id="sec2" class="tsec sec"><div class="goto jig-ncbiinpagenav-goto-container"><a class="tgt_dark page-toc-label jig-ncbiinpagenav-goto-heading" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" title="Go to other sections in this page" role="button" aria-expanded="false" aria-haspopup="true">Go to:</a></div><h2 class="head no_bottom_margin ui-helper-clearfix" id="sec2title">2. Related Work</h2><p id="idm140658736651632" class="p p-first">There are many methods that have been proposed to solve these challenging problems on image classification, which can be categorized into two types of methods: traditional methods and deep model methods. Traditional methods include color and texture [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B7" rid="B7" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">7</a>–<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B10" rid="B10" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">10</a>], random forests [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B11" rid="B11" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">11</a>], and support vector machines [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B12" rid="B12" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">12</a>, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B13" rid="B13" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">13</a>]. Studies on deep models to classify medical images include [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B14" rid="B14" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">14</a>–<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B16" rid="B16" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">16</a>]. In this section, we will first give a detailed introduction to the previous work on image classification. Then, some literature [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B17" rid="B17" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">17</a>–<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B19" rid="B19" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">19</a>] about feature fusion for image classification tasks will be reviewed.</p><p id="idm140658779335360">In Reference [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B7" rid="B7" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">7</a>], the authors have addressed two systems for the detection of melanomas in dermoscopy images using texture and color features. One system uses global features to classify skin lesions, and another system employs local features. The results were demonstrated on a dataset of 176 dermoscopy images from Hospital Pedro Hispano. Iyatomi et al. [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B8" rid="B8" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">8</a>] proposed an Internet-based melanoma screening system based on shape, color, and texture features. This system gained a sensitivity (SE) of 86% and a specificity (SP) of 86% on 1200 dermoscopy images. Stoecker et al. [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B9" rid="B9" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">9</a>] analyzed the areas of granularity between melanoma and similar areas in nonmelanoma skin lesions with a combination of color and texture features. Their paper used the receiver operating characteristic (ROC) curve to display the systems best separation performance on a dataset with 88 melanomas and 200 nonmelanoma lesions. Riaz et al. [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B10" rid="B10" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">10</a>] first deployed a novel region-based texture and color descriptors to identify cancer in images. In their model, texture features are based on Gabor filters, and they use homomorphic filtering to obtain color features, which can address the problem of different rotations, scaling, and illumination.</p><p id="idm140658768979168">Ramirez et al. [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B11" rid="B11" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">11</a>] proposed a variant of random forests on single photon emission computed tomography (SPECT) image classification to help diagnose Alzheimer's disease (AD). First, they extracted score features using partial least squares from the image datasets to structure the random forests. Using this system as a classifier helped to classify all of the images. The specific process is to classify the image to the closest centroid recessively until reaching a leaf of a single tree, which is the classification of the image. The most important characteristic of this algorithm is that it can extend from the previous model, a process referred as to incremental learning, without retraining the images from scratch.</p><p id="idm140658776580240">In Reference [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B12" rid="B12" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">12</a>], the authors proposed a classifier that is based on a fractional Fourier transform and nonparallel support vector machine to classify magnetic resonance brain images into pathological brain image and healthy brain image categories. Thus, it was a binary classification task. For a given image, the system first used a weighted-type fractional Fourier transform to extract the spectrum features, and then, it utilized principal component analysis to reduce the dimensionality of the spectrum features. Finally, its incorporated spectrum features were fed into support vector machines. However, in this paper, the dataset that contains 90 T2-weighted MR brain images is relatively small. Although it has obtained good performance, it is clear that it is not adapted to a larger dataset.</p><p id="idm140658775279248">Li et al. [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B14" rid="B14" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">14</a>] trained a customized convolutional neural network (CNN) to classify lung image patches. In this model, the system contained only one convolutional layer to extract the deep features, to overcome the overfitting problem, and it obtained the best classification performance compared with scale-invariant feature transform (SIFT) features, rotation-invariant local binary pattern (LBP) features, and unsupervised feature learning using the restricted Boltzmann machine (RBM). In Reference [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B20" rid="B20" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">20</a>], the authors first proposed simple deep learning architecture called principal component analysis network (PCANet) that had been used by [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B21" rid="B21" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">21</a>] combined with the spatial distribution information of color images to achieve the state-of-the-art classification accuracy in various databases. In Reference [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B15" rid="B15" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">15</a>], the authors employed a CNN trained by ImageNet to identify different types of pathologies in chest X-ray images. They achieved the best accuracy performance by combining the features extracted from a CNN and handcrafted features. Shin et al. [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B16" rid="B16" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">16</a>] discussed why transfer learning can be useful to address medical images. Additionally, they proved their results on thoracoabdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. Rakotomamonjy et al. [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B22" rid="B22" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">22</a>] employed scattering transform which first proposed by [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B23" rid="B23" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">23</a>] to extract features combined with local binary patterns (LBP) and local quinary patterns (LQP) for lung cancer detection which proved to be robust to small deformations in the images. And they verified the performances and effectiveness on the 2D-Hela dataset and Pap smear dataset. Cruzroa et al. [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B24" rid="B24" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">24</a>] presented a deep learning approach for automatic detection of invasive ductal carcinoma (IDC) tissue regions in whole slide images (WSI) of breast cancer (BCa) which verified through a dataset from 162 patients diagnosed with IDC achieving balanced accuracy 84.23%.</p><p id="idm140658770492048">Ahn et al. [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B17" rid="B17" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">17</a>] proposed a method that combined domain-transferred convolutional neural networks (DT-CNNS) with a sparse spatial pyramid (SSP) to classify X-ray images. In this paper, they used VGG19 (19 layers CNN) proposed by [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B6" rid="B6" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">6</a>] as the transferred network, which could ignore the medical image characteristics. However, this approach provided a new train of thought to solve this problem. In Reference [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B25" rid="B25" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">25</a>], the authors first proposed multiscale high-level feature representations for face verification, which they termed Deep hidden Identity features (DeepID). The multiscale features fuse the features extracted from the third and fourth layers of the CNN model. In Reference [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B19" rid="B19" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">19</a>], the authors presented a logistic regression-based fusion method that can fuse shape and color features without being tied to any of them. Their model implicitly weighted the visual words to overcome the shortcomings of not considering their statistical dependences. Li et al. [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B26" rid="B26" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">26</a>] employed kernel principal component analysis (KPCA) as the fusion method to find nonlinear relationships of the extracted color and texture features which is then used as the maximum likelihood approach for automatic selection of optimal feature set from the fused feature.</p><p id="idm140658768924208" class="p p-last">All of the above algorithms have some problems in that they used transferred convolutional neural networks or directly employed traditional method to classify the medical images. In traditional methods, regardless of which features (color features, texture features, or shape features) are used, it is not adequate to classify the medical images solely by those features that are gained through experience. For deep models, the transfer-learning network finds it very easy to ignore the characteristics of medical images. In addition, most of the literature about medical image classification is on binary classification. In practice, we usually need to perform a multiclass classification task. To solve these problems and improve the performance of medical image classification, we present our new algorithm.</p></div><div id="sec3" class="tsec sec"><div class="goto jig-ncbiinpagenav-goto-container"><a class="tgt_dark page-toc-label jig-ncbiinpagenav-goto-heading" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" title="Go to other sections in this page" role="button" aria-expanded="false" aria-haspopup="true">Go to:</a></div><h2 class="head no_bottom_margin ui-helper-clearfix" id="sec3title">3. Methodology of the Proposed Model</h2><p id="idm140658770770272" class="p p-first">In this section, we will highlight the key components of the CNMP model, providing a description of the coding network and the traditional feature extraction in subsections <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#sec3.1" rid="sec3.1" class=" sec">3.1</a> and <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#sec3.2" rid="sec3.2" class=" sec">3.2</a>. In <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#sec3.3" rid="sec3.3" class=" sec">subsection 3.3</a>, we introduce the detailed fusion process. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig1/" target="figure" class="fig-table-link figpopup" rid-figpopup="fig1" rid-ob="ob-fig1" co-legend-rid="lgnd_fig1"><span> Figure 1</span></a> shows the detailed procedure of our algorithm.</p><!--fig ft0--><!--fig mode=article f1--><div class="fig iconblock whole_rhythm clearfix" id="fig1" co-legend-rid="lgnd_fig1"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig1/" target="figure" rid-figpopup="fig1" rid-ob="ob-fig1"><!--fig/graphic|fig/alternatives/graphic mode="anchored" m1--></a><div data-largeobj="" data-largeobj-link-rid="largeobj_idm140658772415840" class="figure"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig1/" target="figure" rid-figpopup="fig1" rid-ob="ob-fig1"></a><a class="inline_block ts_canvas" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=6157177_CIN2018-2061516.001.jpg" target="tileshopwindow"><div class="ts_bar small" title="Click on image to zoom"></div><img alt="An external file that holds a picture, illustration, etc.
Object name is CIN2018-2061516.001.jpg" title="Click on image to zoom" class="tileshop" src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/CIN2018-2061516.001.jpg"></a></div><div id="largeobj_idm140658772415840" class="largeobj-link align_right" style="display: none"><a target="object" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig1/?report=objectonly">Open in a separate window</a></div><div class="icnblk_cntnt" id="lgnd_fig1"><div><a class="figpopup" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig1/" target="figure" rid-figpopup="fig1" rid-ob="ob-fig1">Figure 1</a></div><!--caption a7--><div class="caption"><p id="idm140658772329152">Framework of the approach.</p></div></div></div><p id="idm140658777848960" class="p">Since the detailed introduction of LeNet-5 [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B27" rid="B27" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">27</a>] in the 1990s, the convolutional neural network (CNN) has been widely used in image classification, video recognition, and object detection, and it has achieved the excellent performance in these areas. The CNN usually contains convolutional layers, pooling layers, one or more fully connected layers, and a softmax layer. The convolutional layers combined with the pooling layers are used for extracting features. The softmax layer is regarded as the classifier. The main design principles of the deep model are as follows: (1) to perform the image preprocessing, such as subtracting the mean RGB value and ZCA whitening; (2) choosing the proper activation function [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B28" rid="B28" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">28</a>]; (3) the initial weights are also important. If the initial weights are too small, then the deep network would not be able to learn, and if they are too large, then the initial weights would undergo divergence [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B29" rid="B29" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">29</a>]; (4) data augmentation, such as extracting random patches from the original image, horizontally flipping them in the image [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B5" rid="B5" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">5</a>], which is especially important in medical image analysis; (5) using dropout to reduce overfitting and local response normalization [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B5" rid="B5" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">5</a>] to reduce error rates are equally important; (6) choose the proper learning rate. To decay the learning rate in each epoch is the common usage; (7) deep network architecture is the most important principle. This would be confirmed on [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B30" rid="B30" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">30</a>] and [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B6" rid="B6" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">6</a>] that they achieved the state-of-the-art results on ILSVRC-2013 and ILSVRC-2014, respectively. According to the above patterns, we train a CNN as a coding network in the following way.</p><div id="sec3.1" class="sec"><h3 id="sec3.1title">3.1. Coding Network</h3><div id="sec3.1.1" class="sec sec-first"><p></p><h4 id="sec3.1.1title" class="inline">3.1.1. The Structure of a Coding Network </h4><p id="idm140658775344768" class="p p-first-last">When training the coding network, the input of the coding network is a fixed-size 140 × 140 RGB image. Before feeding the medical image into the network, every image pixel value subtracts the mean RGB value. The coding network consists of a series of convolutional layers and pooling layers. The convolutional layer employs a filter with a receptive field of 11 × 11, 9 × 9, and 8 × 8 and a 1 pixel stride and 0 pixel padding. The convolution operation is defined as follows:</p><div class="disp-formula" id="EEq1"><div class="f"><span class="MathJax_Preview"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" style=""><nobr><span class="math" id="M1" overflow="scroll" role="math" style="width: 15.123em; display: inline-block;"><span style="display: inline-block; position: relative; width: 12.2em; height: 0px; font-size: 124%;"><span style="position: absolute; clip: rect(0.809em, 1011.95em, 2.926em, -999.997em); top: -2.114em; left: 0.003em;"><span class="mrow" id="MathJax-Span-2"><span class="mtable" id="MathJax-Span-3" style="padding-right: 0.154em; padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 11.948em; height: 0px;"><span style="position: absolute; clip: rect(2.674em, 1011.85em, 4.791em, -999.997em); top: -3.979em; left: 0.003em;"><span style="display: inline-block; position: relative; width: 11.948em; height: 0px;"><span style="position: absolute; clip: rect(2.674em, 1011.85em, 4.791em, -999.997em); top: -3.979em; left: 50%; margin-left: -5.945em;"><span class="mtd" id="MathJax-Span-4"><span class="mrow" id="MathJax-Span-5"><span class="msubsup" id="MathJax-Span-6"><span style="display: inline-block; position: relative; width: 0.96em; height: 0px;"><span style="position: absolute; clip: rect(3.38em, 1000.51em, 4.337em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-7"><span class="mi" id="MathJax-Span-8" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.531em, 1000.41em, 4.136em, -999.997em); top: -4.332em; left: 0.557em;"><span class="mrow" id="MathJax-Span-9"><span class="mi" id="MathJax-Span-10" style="font-size: 70.7%; font-family: MathJax_Math-italic;">r</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.38em, 1000.36em, 4.287em, -999.997em); top: -3.677em; left: 0.507em;"><span class="mrow" id="MathJax-Span-11"><span class="mi" id="MathJax-Span-12" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-13" style="font-family: MathJax_Main; padding-left: 0.305em;">=</span><span class="mi" id="MathJax-Span-14" style="font-family: MathJax_Math-italic; padding-left: 0.305em;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.053em;"></span></span><span class="mfenced" id="MathJax-Span-15" style="padding-left: 0.154em;"><span class="mo" id="MathJax-Span-29" style="vertical-align: 0.003em;"><span style="font-family: MathJax_Size2;">(</span></span><span class="mrow" id="MathJax-Span-17"><span class="msubsup" id="MathJax-Span-18"><span style="display: inline-block; position: relative; width: 0.809em; height: 0px;"><span style="position: absolute; clip: rect(3.128em, 1000.41em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-19"><span class="mi" id="MathJax-Span-20" style="font-family: MathJax_Math-italic;">b</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.531em, 1000.41em, 4.136em, -999.997em); top: -4.383em; left: 0.456em;"><span class="mrow" id="MathJax-Span-21"><span class="mi" id="MathJax-Span-22" style="font-size: 70.7%; font-family: MathJax_Math-italic;">r</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.38em, 1000.36em, 4.287em, -999.997em); top: -3.727em; left: 0.456em;"><span class="mrow" id="MathJax-Span-23"><span class="mi" id="MathJax-Span-24" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-25" style="font-family: MathJax_Main; padding-left: 0.204em;">+</span><span class="mstyle" id="MathJax-Span-26" style="padding-left: 0.204em;"><span class="mrow" id="MathJax-Span-27"><span class="mo" id="MathJax-Span-28" style="font-family: MathJax_Size2; vertical-align: 0.003em;">∑</span><span class="mrow" id="MathJax-Span-30" style="padding-left: 0.154em;"><span class="msubsup" id="MathJax-Span-31"><span style="display: inline-block; position: relative; width: 2.019em; height: 0px;"><span style="position: absolute; clip: rect(3.38em, 1000.71em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-32"><span class="mi" id="MathJax-Span-33" style="font-family: MathJax_Math-italic;">w</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.38em, 1001.31em, 4.136em, -999.997em); top: -4.332em; left: 0.708em;"><span class="mrow" id="MathJax-Span-34"><span class="mi" id="MathJax-Span-35" style="font-size: 70.7%; font-family: MathJax_Math-italic;">r</span><span class="mo" id="MathJax-Span-36" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-37" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.38em, 1000.81em, 4.287em, -999.997em); top: -3.677em; left: 0.708em;"><span class="mrow" id="MathJax-Span-38"><span class="mi" id="MathJax-Span-39" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-40" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-41" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mtext" id="MathJax-Span-42" style="font-family: MathJax_Main;"> </span><span class="mi" id="MathJax-Span-43" style="font-family: MathJax_Main;">∗</span><span class="mtext" id="MathJax-Span-44" style="font-family: MathJax_Main;"> </span><span class="msubsup" id="MathJax-Span-45"><span style="display: inline-block; position: relative; width: 0.96em; height: 0px;"><span style="position: absolute; clip: rect(3.38em, 1000.51em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-46"><span class="mi" id="MathJax-Span-47" style="font-family: MathJax_Math-italic;">x</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.531em, 1000.41em, 4.136em, -999.997em); top: -4.332em; left: 0.557em;"><span class="mrow" id="MathJax-Span-48"><span class="mi" id="MathJax-Span-49" style="font-size: 70.7%; font-family: MathJax_Math-italic;">r</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.38em, 1000.3em, 4.136em, -999.997em); top: -3.677em; left: 0.557em;"><span class="mrow" id="MathJax-Span-50"><span class="mi" id="MathJax-Span-51" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span></span></span></span><span class="mo" id="MathJax-Span-52" style="vertical-align: 0.003em;"><span style="font-family: MathJax_Size2;">)</span></span></span><span class="mo" id="MathJax-Span-53" style="font-family: MathJax_Main;">.</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.119em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.378em;"></span></span></nobr></span></div><script type="math/mml" id="MathJax-Element-1"><math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block" id="M1" overflow="scroll"><mtable><mtr><mtd><msubsup><mrow><mi>y</mi></mrow><mrow><mi>j</mi></mrow><mrow><mi>r</mi></mrow></msubsup><mo>=</mo><mi>f</mi><mfenced open="(" close=")" separators="|"><mrow><msubsup><mrow><mi>b</mi></mrow><mrow><mi>j</mi></mrow><mrow><mi>r</mi></mrow></msubsup><mo>+</mo><mstyle displaystyle="true"><mo stretchy="true">∑</mo><mrow><msubsup><mrow><mi>w</mi></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mrow><mi>r</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mtext> </mtext><mi>∗</mi><mtext> </mtext><msubsup><mrow><mi>x</mi></mrow><mrow><mi>i</mi></mrow><mrow><mi>r</mi></mrow></msubsup></mrow></mstyle></mrow></mfenced><mo>.</mo></mtd></mtr></mtable></math></script></div><div class="l">(1)</div></div><p>Here, <em>r</em> is the r-th layer in the coding network and <em>f</em> is the activation function, which we would discuss in the next subsection; <em>x</em><sub><em>i</em></sub> and <em>y</em><sub><em>j</em></sub> are the i-th input feature map and the j-th output feature map; <em>w</em><sub><em>i</em>,<em>j</em></sub> is the convolution kernel between <em>x</em><sub><em>i</em></sub> and <em>y</em><sub><em>j</em></sub>; <em>b</em><sub><em>j</em></sub> is the bias; and <em>∗</em> is the convolution operation. The pooling layer is performed by max-pooling with a 5 × 5 window with a stride of 2 that signifies the overlapping pooling. The pooling operation is defined as</p><div class="disp-formula" id="EEq2"><div class="f"><span class="MathJax_Preview"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0" style=""><nobr><span class="math" id="M2" overflow="scroll" role="math" style="width: 15.527em; display: inline-block;"><span style="display: inline-block; position: relative; width: 12.503em; height: 0px; font-size: 124%;"><span style="position: absolute; clip: rect(0.708em, 1012.3em, 3.027em, -999.997em); top: -2.114em; left: 0.003em;"><span class="mrow" id="MathJax-Span-55"><span class="mtable" id="MathJax-Span-56" style="padding-right: 0.154em; padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 12.251em; height: 0px;"><span style="position: absolute; clip: rect(2.573em, 1012.15em, 4.892em, -999.997em); top: -3.979em; left: 0.003em;"><span style="display: inline-block; position: relative; width: 12.251em; height: 0px;"><span style="position: absolute; clip: rect(2.674em, 1012.15em, 5.043em, -999.997em); top: -4.08em; left: 50%; margin-left: -6.096em;"><span class="mtd" id="MathJax-Span-57"><span class="mrow" id="MathJax-Span-58"><span class="msubsup" id="MathJax-Span-59"><span style="display: inline-block; position: relative; width: 1.414em; height: 0px;"><span style="position: absolute; clip: rect(3.38em, 1000.51em, 4.337em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-60"><span class="mi" id="MathJax-Span-61" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.38em, 1000.3em, 4.136em, -999.997em); top: -4.332em; left: 0.557em;"><span class="mrow" id="MathJax-Span-62"><span class="mi" id="MathJax-Span-63" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.329em, 1000.91em, 4.287em, -999.997em); top: -3.677em; left: 0.507em;"><span class="mrow" id="MathJax-Span-64"><span class="mi" id="MathJax-Span-65" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span class="mo" id="MathJax-Span-66" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-67" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-68" style="font-family: MathJax_Main; padding-left: 0.305em;">=</span><span class="munder" id="MathJax-Span-69" style="padding-left: 0.305em;"><span style="display: inline-block; position: relative; width: 3.027em; height: 0px;"><span style="position: absolute; clip: rect(3.38em, 1001.87em, 4.136em, -999.997em); top: -3.979em; left: 0.607em;"><span class="mrow" id="MathJax-Span-70"><span class="mi" id="MathJax-Span-71" style="font-family: MathJax_Main;">max</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.38em, 1003.03em, 4.388em, -999.997em); top: -3.324em; left: 0.003em;"><span class="mrow" id="MathJax-Span-72"><span class="mn" id="MathJax-Span-73" style="font-size: 70.7%; font-family: MathJax_Main;">0</span><span class="mo" id="MathJax-Span-74" style="font-size: 70.7%; font-family: MathJax_Main;">≤</span><span class="mi" id="MathJax-Span-75" style="font-size: 70.7%; font-family: MathJax_Math-italic;">m</span><span class="mo" id="MathJax-Span-76" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-77" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-78" style="font-size: 70.7%; font-family: MathJax_Main;">&lt;</span><span class="mn" id="MathJax-Span-79" style="font-size: 70.7%; font-family: MathJax_Main;">5</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mfenced" id="MathJax-Span-80" style="padding-left: 0.154em;"><span class="mo" id="MathJax-Span-81" style="vertical-align: 0.003em;"><span style="font-family: MathJax_Size2;">(</span></span><span class="mrow" id="MathJax-Span-82"><span class="msubsup" id="MathJax-Span-83"><span style="display: inline-block; position: relative; width: 4.74em; height: 0px;"><span style="position: absolute; clip: rect(3.38em, 1000.51em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-84"><span class="mi" id="MathJax-Span-85" style="font-family: MathJax_Math-italic;">x</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.38em, 1000.3em, 4.136em, -999.997em); top: -4.332em; left: 0.557em;"><span class="mrow" id="MathJax-Span-86"><span class="mi" id="MathJax-Span-87" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.329em, 1004.19em, 4.287em, -999.997em); top: -3.677em; left: 0.557em;"><span class="mrow" id="MathJax-Span-88"><span class="mi" id="MathJax-Span-89" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span class="mo" id="MathJax-Span-90" style="font-size: 70.7%; font-family: MathJax_Main;">⋅</span><span class="mn" id="MathJax-Span-91" style="font-size: 70.7%; font-family: MathJax_Main;">5</span><span class="mo" id="MathJax-Span-92" style="font-size: 70.7%; font-family: MathJax_Main;">+</span><span class="mi" id="MathJax-Span-93" style="font-size: 70.7%; font-family: MathJax_Math-italic;">m</span><span class="mo" id="MathJax-Span-94" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-95" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span class="mo" id="MathJax-Span-96" style="font-size: 70.7%; font-family: MathJax_Main;">⋅</span><span class="mn" id="MathJax-Span-97" style="font-size: 70.7%; font-family: MathJax_Main;">5</span><span class="mo" id="MathJax-Span-98" style="font-size: 70.7%; font-family: MathJax_Main;">+</span><span class="mi" id="MathJax-Span-99" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span class="mo" id="MathJax-Span-100" style="vertical-align: 0.003em;"><span style="font-family: MathJax_Size2;">)</span></span></span><span class="mo" id="MathJax-Span-101" style="font-family: MathJax_Main;">,</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.119em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.997em; border-left: 0px solid; width: 0px; height: 2.628em;"></span></span></nobr></span></div><script type="math/mml" id="MathJax-Element-2"><math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block" id="M2" overflow="scroll"><mtable><mtr><mtd><msubsup><mrow><mi>y</mi></mrow><mrow><mi>j</mi><mo>,</mo><mi>k</mi></mrow><mrow><mi>i</mi></mrow></msubsup><mo>=</mo><munder><mrow><mi mathvariant="normal">max</mi></mrow><mrow><mn>0</mn><mo>≤</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo>&lt;</mo><mn>5</mn></mrow></munder><mfenced open="(" close=")" separators="|"><mrow><msubsup><mrow><mi>x</mi></mrow><mrow><mi>j</mi><mo>·</mo><mn>5</mn><mo>+</mo><mi>m</mi><mo>,</mo><mi>k</mi><mo>·</mo><mn>5</mn><mo>+</mo><mi>n</mi></mrow><mrow><mi>i</mi></mrow></msubsup></mrow></mfenced><mo>,</mo></mtd></mtr></mtable></math></script></div><div class="l">(2)</div></div><p>where each element in the output feature map <em>y</em><sup><em>i</em></sup> is pooling from the 5 × 5 overlapping local region in the input feature map <em>x</em><sup><em>i</em></sup>. At the last layer of the coding network, we use softmax as a classifier to classify the medical image. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/table/tab1/" target="table" class="fig-table-link figpopup" rid-figpopup="tab1" rid-ob="ob-tab1" co-legend-rid=""><span> Table 1</span></a> shows the detailed configuration of the coding network.</p><!--table ft1--><!--table-wrap mode="anchored" t5--><div class="table-wrap table anchored whole_rhythm" id="tab1"><h3>Table 1</h3><!--caption a7--><div class="caption"><p id="idm140658766575984">The configuration of the coding network.</p></div><div data-largeobj="" data-largeobj-link-rid="largeobj_idm140658766575728" class="xtable"><table frame="hsides" rules="groups" class="rendered small default_table"><thead><tr><th align="left" rowspan="1" colspan="1">Type</th><th align="center" rowspan="1" colspan="1">Patch size/stride</th><th align="center" rowspan="1" colspan="1">Output size</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Convolution</td><td align="center" rowspan="1" colspan="1">11 × 11/1</td><td align="center" rowspan="1" colspan="1">130 × 130 × 32</td></tr><tr><td align="left" rowspan="1" colspan="1">Convolution</td><td align="center" rowspan="1" colspan="1">11 × 11/1</td><td align="center" rowspan="1" colspan="1">120 × 120 × 32</td></tr><tr><td align="left" rowspan="1" colspan="1">Max pool</td><td align="center" rowspan="1" colspan="1">5 × 5/2</td><td align="center" rowspan="1" colspan="1">58 × 58 × 32</td></tr><tr><td align="left" rowspan="1" colspan="1">Convolution</td><td align="center" rowspan="1" colspan="1">9 × 9/1</td><td align="center" rowspan="1" colspan="1">50 × 50 × 64</td></tr><tr><td align="left" rowspan="1" colspan="1">Max pool</td><td align="center" rowspan="1" colspan="1">5  ×  5/2</td><td align="center" rowspan="1" colspan="1">23 × 23 × 64</td></tr><tr><td align="left" rowspan="1" colspan="1">Convolution</td><td align="center" rowspan="1" colspan="1">8 × 8/1</td><td align="center" rowspan="1" colspan="1">16 × 16 × 128</td></tr><tr><td align="left" rowspan="1" colspan="1">Convolution</td><td align="center" rowspan="1" colspan="1">9 × 9/1</td><td align="center" rowspan="1" colspan="1">8 × 8 × 256</td></tr><tr><td align="left" rowspan="1" colspan="1">Convolution</td><td align="center" rowspan="1" colspan="1">8 × 8/1</td><td align="center" rowspan="1" colspan="1">1 × 1 × 256</td></tr><tr><td align="left" rowspan="1" colspan="1">Rasterize</td><td align="left" rowspan="1" colspan="1"> </td><td align="center" rowspan="1" colspan="1">1 × 1 × 4</td></tr><tr><td align="left" rowspan="1" colspan="1">Softmax layer</td><td align="left" rowspan="1" colspan="1"> </td><td align="center" rowspan="1" colspan="1">1 × 1 × 4</td></tr></tbody></table></div><div id="largeobj_idm140658766575728" class="largeobj-link align_right" style="display: none"><a target="object" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/table/tab1/?report=objectonly">Open in a separate window</a></div></div></div><div id="sec3.1.2" class="sec"><p></p><h4 id="sec3.1.2title" class="inline">3.1.2. Activation Function </h4><p id="idm140658768835312" class="p p-first-last">The original activation functions are the sigmoid function <em>f</em>(<em>x</em>)=(1+<em>e</em><sup>−<em>x</em></sup>)<sup>−1</sup> and tanh function <em>f</em>(<em>x</em>)=tanh(<em>x</em>); their derivatives can be expressed by themselves, and they can map the larger change values into a smaller range. However, the two functions have common problems in that the convergence rate is rather slow and there is a gradient diffusion problem. To make them computationally efficient and reduce the gradient diffusion effect, we will follow Hinton [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B28" rid="B28" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">28</a>] to use Rectified Linear Units (ReLus) <em>f</em>(<em>x</em>)=max(0, <em>x</em>) as the activation function of the coding network. Additionally, in this paper, authors turn out that ReLus can converge faster than sigmoid or tanh.</p></div><div id="sec3.1.3" class="sec sec-last"><p></p><h4 id="sec3.1.3title" class="inline">3.1.3. Softmax </h4><p id="idm140658775441440" class="p p-first-last">At the last layer of our network, we connect in a softmax layer, which can predict <em>n</em> different classes through computing the probability of belonging to each category. In the last layer within our network, the feature will rasterize into <em>x</em>, which is a column feature vector:</p><div class="disp-formula" id="EEq3"><div class="f"><span class="MathJax_Preview"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-3-Frame" tabindex="0" style=""><nobr><span class="math" id="M3" overflow="scroll" role="math" style="width: 14.216em; display: inline-block;"><span style="display: inline-block; position: relative; width: 11.444em; height: 0px; font-size: 124%;"><span style="position: absolute; clip: rect(0.406em, 1011.24em, 3.329em, -999.997em); top: -2.114em; left: 0.003em;"><span class="mrow" id="MathJax-Span-103"><span class="mtable" id="MathJax-Span-104" style="padding-right: 0.154em; padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 11.192em; height: 0px;"><span style="position: absolute; clip: rect(2.271em, 1011.14em, 5.194em, -999.997em); top: -3.979em; left: 0.003em;"><span style="display: inline-block; position: relative; width: 11.192em; height: 0px;"><span style="position: absolute; clip: rect(2.472em, 1011.14em, 5.446em, -999.997em); top: -4.181em; left: 50%; margin-left: -5.592em;"><span class="mtd" id="MathJax-Span-105"><span class="mrow" id="MathJax-Span-106"><span class="mi" id="MathJax-Span-107" style="font-family: MathJax_Math-italic;">p</span><span class="mfenced" id="MathJax-Span-108" style="padding-left: 0.154em;"><span class="mo" id="MathJax-Span-109" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mrow" id="MathJax-Span-110"><span class="mi" id="MathJax-Span-111" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-112" style="font-family: MathJax_Main; padding-left: 0.305em;">=</span><span class="mi" id="MathJax-Span-113" style="font-family: MathJax_Math-italic; padding-left: 0.305em;">j</span><span class="mfenced" id="MathJax-Span-114" style="padding-left: 0.154em;"><span class="mo" id="MathJax-Span-115" style="vertical-align: 0.003em;"><span style="font-family: MathJax_Main;">|</span></span><span class="mrow" id="MathJax-Span-116"><span class="mi" id="MathJax-Span-117" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-118" style="font-family: MathJax_Main;">,</span><span class="mtext" id="MathJax-Span-119" style="font-family: MathJax_Main; padding-left: 0.154em;"> </span><span class="mi" id="MathJax-Span-120" style="font-family: MathJax_Math-italic;">θ</span></span></span></span><span class="mo" id="MathJax-Span-121" style=""><span style="font-family: MathJax_Main;">)</span></span></span><span class="mo" id="MathJax-Span-122" style="font-family: MathJax_Main; padding-left: 0.305em;">=</span><span class="mfrac" id="MathJax-Span-123" style="padding-left: 0.305em;"><span style="display: inline-block; position: relative; width: 3.48em; height: 0px; margin-right: 0.103em; margin-left: 0.103em;"><span style="position: absolute; clip: rect(2.876em, 1001.31em, 4.136em, -999.997em); top: -4.383em; left: 50%; margin-left: -0.653em;"><span class="mrow" id="MathJax-Span-124"><span class="msup" id="MathJax-Span-125"><span style="display: inline-block; position: relative; width: 1.313em; height: 0px;"><span style="position: absolute; clip: rect(3.531em, 1000.3em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-126"><span class="mi" id="MathJax-Span-127" style="font-size: 70.7%; font-family: MathJax_Math-italic;">e</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -4.383em; left: 0.355em;"><span class="mrow" id="MathJax-Span-128"><span class="msubsup" id="MathJax-Span-129"><span style="display: inline-block; position: relative; width: 0.607em; height: 0px;"><span style="position: absolute; clip: rect(3.48em, 1000.25em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-130"><span class="mi" id="MathJax-Span-131" style="font-size: 50%; font-family: MathJax_Math-italic;">θ</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.48em, 1000.41em, 4.136em, -999.997em); top: -4.131em; left: 0.255em;"><span class="mrow" id="MathJax-Span-132"><span class="mi" id="MathJax-Span-133" style="font-size: 50%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.053em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.48em, 1000.25em, 4.236em, -999.997em); top: -3.727em; left: 0.255em;"><span class="mrow" id="MathJax-Span-134"><span class="mi" id="MathJax-Span-135" style="font-size: 50%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mi" id="MathJax-Span-136" style="font-size: 50%; font-family: MathJax_Math-italic;">x</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(2.876em, 1003.38em, 4.589em, -999.997em); top: -3.122em; left: 50%; margin-left: -1.661em;"><span class="mrow" id="MathJax-Span-137"><span class="msubsup" id="MathJax-Span-138"><span style="display: inline-block; position: relative; width: 1.918em; height: 0px;"><span style="position: absolute; clip: rect(3.178em, 1000.96em, 4.438em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-139"><span class="msup" id="MathJax-Span-140"><span style="display: inline-block; position: relative; width: 1.011em; height: 0px;"><span style="position: absolute; clip: rect(3.178em, 1000.96em, 4.438em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-141"><span class="mstyle" id="MathJax-Span-142"><span class="mrow" id="MathJax-Span-143"><span class="mrow" id="MathJax-Span-144"><span class="mo" id="MathJax-Span-145" style="font-size: 70.7%; font-family: MathJax_Size2; vertical-align: 0.003em;">∑</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.48em, 1000.3em, 4.136em, -999.997em); top: -4.433em; left: 1.011em;"><span class="mrow" id="MathJax-Span-146"><span class="mi" id="MathJax-Span-147" style="font-size: 50%; font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.48em, 1000.91em, 4.236em, -999.997em); top: -3.627em; left: 1.011em;"><span class="mrow" id="MathJax-Span-148"><span class="mi" id="MathJax-Span-149" style="font-size: 50%; font-family: MathJax_Math-italic;">j</span><span class="mo" id="MathJax-Span-150" style="font-size: 50%; font-family: MathJax_Main;">=</span><span class="mn" id="MathJax-Span-151" style="font-size: 50%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msup" id="MathJax-Span-152" style="padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 1.313em; height: 0px;"><span style="position: absolute; clip: rect(3.531em, 1000.3em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-153"><span class="mi" id="MathJax-Span-154" style="font-size: 70.7%; font-family: MathJax_Math-italic;">e</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -4.383em; left: 0.355em;"><span class="mrow" id="MathJax-Span-155"><span class="msubsup" id="MathJax-Span-156"><span style="display: inline-block; position: relative; width: 0.607em; height: 0px;"><span style="position: absolute; clip: rect(3.48em, 1000.25em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-157"><span class="mi" id="MathJax-Span-158" style="font-size: 50%; font-family: MathJax_Math-italic;">θ</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.48em, 1000.41em, 4.136em, -999.997em); top: -4.131em; left: 0.255em;"><span class="mrow" id="MathJax-Span-159"><span class="mi" id="MathJax-Span-160" style="font-size: 50%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.053em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.48em, 1000.25em, 4.236em, -999.997em); top: -3.727em; left: 0.255em;"><span class="mrow" id="MathJax-Span-161"><span class="mi" id="MathJax-Span-162" style="font-size: 50%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mi" id="MathJax-Span-163" style="font-size: 50%; font-family: MathJax_Math-italic;">x</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(0.859em, 1003.48em, 1.212em, -999.997em); top: -1.258em; left: 0.003em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-top: 1.3px solid; width: 3.48em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.061em;"></span></span></span></span><span class="mo" id="MathJax-Span-164" style="font-family: MathJax_Main;">,</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.119em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.372em; border-left: 0px solid; width: 0px; height: 3.378em;"></span></span></nobr></span></div><script type="math/mml" id="MathJax-Element-3"><math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block" id="M3" overflow="scroll"><mtable><mtr><mtd><mi>p</mi><mfenced open="(" close=")" separators="|"><mrow><mi>y</mi><mo>=</mo><mi>j</mi><mfenced open="|" close="" separators="|"><mrow><mi>x</mi><mo>,</mo><mtext> </mtext><mi>θ</mi></mrow></mfenced></mrow></mfenced><mo>=</mo><mfrac><mrow><msup><mrow><mi>e</mi></mrow><mrow><msubsup><mrow><mi>θ</mi></mrow><mrow><mi>j</mi></mrow><mrow><mi>T</mi></mrow></msubsup><mi>x</mi></mrow></msup></mrow><mrow><msubsup><mrow><msup><mrow><mstyle displaystyle="true"><mrow><mo stretchy="false">∑</mo></mrow></mstyle></mrow><mrow></mrow></msup></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>k</mi></mrow></msubsup><msup><mrow><mi>e</mi></mrow><mrow><msubsup><mrow><mi>θ</mi></mrow><mrow><mi>j</mi></mrow><mrow><mi>T</mi></mrow></msubsup><mi>x</mi></mrow></msup></mrow></mfrac><mo>,</mo></mtd></mtr></mtable></math></script></div><div class="l">(3)</div></div><p>where the target contains <em>k</em> classes, and <em>θ</em><sub><em>j</em></sub><sup><em>T</em></sup> is the weight vector.</p></div></div><div id="sec3.2" class="sec"><h3 id="sec3.2title">3.2. Traditional Feature Extraction</h3><p id="idm140658773790784" class="p p-first">According to Reference [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B7" rid="B7" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">7</a>, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B8" rid="B8" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">8</a>, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B31" rid="B31" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">31</a>, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B32" rid="B32" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">32</a>], the most commonly used features in medical image classification include shape features, color histogram features, color moment features, and texture features. Most of the previous studies employ global features to classify medical images. The texture and color moment are the most commonly used features that are used in identifying targets, and they are together global features. Celebi et al. [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B32" rid="B32" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">32</a>] extracted color, texture features, and added shape features that were fed into an SVM classifier, achieving SE = 93% and SP = 92%. Rubegni et al. [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B33" rid="B33" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">33</a>] used color moment features and texture features to achieve good performance on a dataset with 217 melanomas and 588 images. Therefore, we follow the authors to employ the color moment and texture features as the traditional features. Texture is a statistical distribution feature that can describe the innate properties of an image surface. It is based on multiple pixel area computing instead of single pixels. Instead, the color moment is based on a single pixel. It is not very sensitive to the angle or size of the image. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/table/tab2/" target="table" class="fig-table-link figpopup" rid-figpopup="tab2" rid-ob="ob-tab2" co-legend-rid=""><span> Table 2</span></a> lists the commonly used terminologies in this section.</p><!--table ft1--><!--table-wrap mode="anchored" t5--><div class="table-wrap table anchored whole_rhythm" id="tab2"><h3>Table 2</h3><!--caption a7--><div class="caption"><p id="idm140658777788688">Summary of the symbols.</p></div><div data-largeobj="" data-largeobj-link-rid="largeobj_idm140658777788432" class="xtable"><table frame="hsides" rules="groups" class="rendered small default_table"><thead><tr><th align="left" rowspan="1" colspan="1">Symbols</th><th align="center" rowspan="1" colspan="1">The detail description</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">
<em>G</em>
</td><td align="center" rowspan="1" colspan="1">The gray-level co-occurrence matrix</td></tr><tr><td align="left" rowspan="1" colspan="1">
<em>s</em>
</td><td align="center" rowspan="1" colspan="1">The size of <em>G</em></td></tr><tr><td align="left" rowspan="1" colspan="1">
<em>G</em>(<em>i</em>, <em>j</em>)</td><td align="center" rowspan="1" colspan="1">The <em>i</em>-th row <em>j</em>-th column element in <em>G</em></td></tr><tr><td align="left" rowspan="1" colspan="1">
<em>μ</em>
<sub><em>x</em></sub>, <em>μ</em><sub><em>y</em></sub></td><td align="center" rowspan="1" colspan="1">
<em>μ</em>
<sub><em>x</em></sub>, <em>μ</em><sub><em>y</em></sub> are the means the marginal distribution of <em>G</em></td></tr><tr><td align="left" rowspan="1" colspan="1">
<em>σ</em>
<sub><em>x</em></sub>, <em>σ</em><sub><em>y</em></sub></td><td align="center" rowspan="1" colspan="1">
<em>σ</em>
<sub><em>x</em></sub>, <em>σ</em><sub><em>y</em></sub> are the standard deviations of the marginal distribution of <em>G</em></td></tr><tr><td align="left" rowspan="1" colspan="1">
<em>P</em>
</td><td align="center" rowspan="1" colspan="1">A matrix for representation of the image</td></tr><tr><td align="left" rowspan="1" colspan="1">
<em>N</em>
</td><td align="center" rowspan="1" colspan="1">The number of pixels in <em>P</em></td></tr><tr><td align="left" rowspan="1" colspan="1">
<em>P</em>(<em>i</em>, <em>j</em>)</td><td align="center" rowspan="1" colspan="1">The <em>j</em>-th pixel of the <em>i</em>-th channel in <em>P</em></td></tr><tr><td align="left" rowspan="1" colspan="1">
<em>A</em>
<sub><em>i</em></sub>
</td><td align="center" rowspan="1" colspan="1">The mean of the <em>i</em>-th channel in <em>P</em></td></tr><tr><td align="left" rowspan="1" colspan="1">
<em>V</em>
<sub><em>i</em></sub>
</td><td align="center" rowspan="1" colspan="1">The variance of the <em>i</em>-th channel in <em>P</em></td></tr><tr><td align="left" rowspan="1" colspan="1">
<em>S</em>
<sub><em>i</em></sub>
</td><td align="center" rowspan="1" colspan="1">The skewness of the <em>i</em>-th channel in <em>P</em></td></tr></tbody></table></div><div id="largeobj_idm140658777788432" class="largeobj-link align_right" style="display: none"><a target="object" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/table/tab2/?report=objectonly">Open in a separate window</a></div></div><p id="idm140658812325424">To compute the image texture features, we will first acquire the gray-level co-occurrence matrix <strong>G</strong> by the image itself, which can compute the statistics of pairs of neighboring pixels [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B34" rid="B34" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">34</a>]. Then, we employ the angular second moment (ASM), entropy (ENT), contrast (CON), and correlation (COR) to signify the texture features that can be derived by the matrix <strong>G</strong>, which fix the distance between two pixels to one with the angle at 0°, 45°, 90°, or 135°. The definitions are as follows:</p><div class="disp-formula" id="EEq4"><div class="f"><span class="MathJax_Preview"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-4-Frame" tabindex="0" style=""><nobr><span class="math" id="M4" overflow="scroll" role="math" style="width: 14.216em; display: inline-block;"><span style="display: inline-block; position: relative; width: 11.444em; height: 0px; font-size: 124%;"><span style="position: absolute; clip: rect(0.91em, 1011.19em, 2.825em, -999.997em); top: -2.114em; left: 0.003em;"><span class="mrow" id="MathJax-Span-166"><span class="mtable" id="MathJax-Span-167" style="padding-right: 0.154em; padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 11.192em; height: 0px;"><span style="position: absolute; clip: rect(2.775em, 1011.14em, 4.69em, -999.997em); top: -3.979em; left: 0.003em;"><span style="display: inline-block; position: relative; width: 11.192em; height: 0px;"><span style="position: absolute; clip: rect(2.825em, 1011.14em, 4.74em, -999.997em); top: -4.03em; left: 50%; margin-left: -5.592em;"><span class="mtd" id="MathJax-Span-168"><span class="mrow" id="MathJax-Span-169"><span class="mi" id="MathJax-Span-170" style="font-family: MathJax_Main;">ASM</span><span class="mo" id="MathJax-Span-171" style="font-family: MathJax_Main; padding-left: 0.305em;">=</span><span class="munderover" id="MathJax-Span-172" style="padding-left: 0.305em;"><span style="display: inline-block; position: relative; width: 1.867em; height: 0px;"><span style="position: absolute; clip: rect(2.876em, 1001.41em, 4.589em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-173"><span class="mstyle" id="MathJax-Span-174"><span class="mrow" id="MathJax-Span-175"><span class="mrow" id="MathJax-Span-176"><span class="mo" id="MathJax-Span-177" style="font-family: MathJax_Size2; vertical-align: 0.003em;">∑</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.531em, 1000.41em, 4.136em, -999.997em); top: -4.635em; left: 1.464em;"><span class="mrow" id="MathJax-Span-178"><span class="mi" id="MathJax-Span-179" style="font-size: 70.7%; font-family: MathJax_Math-italic;">s</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.38em, 1000.3em, 4.136em, -999.997em); top: -3.475em; left: 1.464em;"><span class="mrow" id="MathJax-Span-180"><span class="mi" id="MathJax-Span-181" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="munderover" id="MathJax-Span-182" style="padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 1.867em; height: 0px;"><span style="position: absolute; clip: rect(2.876em, 1001.41em, 4.589em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-183"><span class="mstyle" id="MathJax-Span-184"><span class="mrow" id="MathJax-Span-185"><span class="mrow" id="MathJax-Span-186"><span class="mo" id="MathJax-Span-187" style="font-family: MathJax_Size2; vertical-align: 0.003em;">∑</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.531em, 1000.41em, 4.136em, -999.997em); top: -4.635em; left: 1.464em;"><span class="mrow" id="MathJax-Span-188"><span class="mi" id="MathJax-Span-189" style="font-size: 70.7%; font-family: MathJax_Math-italic;">s</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.38em, 1000.36em, 4.287em, -999.997em); top: -3.475em; left: 1.464em;"><span class="mrow" id="MathJax-Span-190"><span class="mi" id="MathJax-Span-191" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mi" id="MathJax-Span-192" style="font-family: MathJax_Main-bold; padding-left: 0.154em;">G</span><span class="msup" id="MathJax-Span-193"><span style="display: inline-block; position: relative; width: 2.422em; height: 0px;"><span style="position: absolute; clip: rect(3.077em, 1001.87em, 4.388em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-194"><span class="mfenced" id="MathJax-Span-195"><span class="mo" id="MathJax-Span-196" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mrow" id="MathJax-Span-197"><span class="mi" id="MathJax-Span-198" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-199" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-200" style="font-family: MathJax_Math-italic; padding-left: 0.154em;">j</span></span><span class="mo" id="MathJax-Span-201" style=""><span style="font-family: MathJax_Main;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -4.433em; left: 1.968em;"><span class="mrow" id="MathJax-Span-202"><span class="mn" id="MathJax-Span-203" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-204" style="font-family: MathJax_Main;">.</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.119em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.747em; border-left: 0px solid; width: 0px; height: 2.128em;"></span></span></nobr></span></div><script type="math/mml" id="MathJax-Element-4"><math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block" id="M4" overflow="scroll"><mtable><mtr><mtd><mi mathvariant="normal">ASM</mi><mo>=</mo><munderover><mrow><mstyle displaystyle="true"><mrow><mo stretchy="true">∑</mo></mrow></mstyle></mrow><mrow><mi>i</mi></mrow><mrow><mi>s</mi></mrow></munderover><munderover><mrow><mstyle displaystyle="true"><mrow><mo stretchy="true">∑</mo></mrow></mstyle></mrow><mrow><mi>j</mi></mrow><mrow><mi>s</mi></mrow></munderover><mi mathvariant="bold">G</mi><msup><mrow><mfenced open="(" close=")" separators="|"><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></mfenced></mrow><mrow><mn>2</mn></mrow></msup><mo>.</mo></mtd></mtr></mtable></math></script></div><div class="l">(4)</div></div><p></p><p id="idm140658776737776">The angular second moment is the sum of squares of every element in the matrix <strong>G</strong>. It is representative of homogeneity of the image and roughness of the texture. If the elements in matrix <strong>G</strong> are almost the same, then the ASM value will be small. Otherwise, the ASM will be large.</p><div class="disp-formula" id="EEq5"><div class="f"><span class="MathJax_Preview"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-5-Frame" tabindex="0" style=""><nobr><span class="math" id="M5" overflow="scroll" role="math" style="width: 21.726em; display: inline-block;"><span style="display: inline-block; position: relative; width: 17.492em; height: 0px; font-size: 124%;"><span style="position: absolute; clip: rect(0.91em, 1017.24em, 2.825em, -999.997em); top: -2.114em; left: 0.003em;"><span class="mrow" id="MathJax-Span-206"><span class="mtable" id="MathJax-Span-207" style="padding-right: 0.154em; padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 17.24em; height: 0px;"><span style="position: absolute; clip: rect(2.775em, 1017.14em, 4.69em, -999.997em); top: -3.979em; left: 0.003em;"><span style="display: inline-block; position: relative; width: 17.24em; height: 0px;"><span style="position: absolute; clip: rect(2.876em, 1017.14em, 4.74em, -999.997em); top: -4.08em; left: 50%; margin-left: -8.616em;"><span class="mtd" id="MathJax-Span-208"><span class="mrow" id="MathJax-Span-209"><span class="mi" id="MathJax-Span-210" style="font-family: MathJax_Main;">ENT</span><span class="mo" id="MathJax-Span-211" style="font-family: MathJax_Main; padding-left: 0.305em;">=</span><span class="mo" id="MathJax-Span-212" style="font-family: MathJax_Main; padding-left: 0.305em;">−</span><span class="munder" id="MathJax-Span-213" style="padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 1.767em; height: 0px;"><span style="position: absolute; clip: rect(2.876em, 1001.41em, 4.589em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-214"><span class="mstyle" id="MathJax-Span-215"><span class="mrow" id="MathJax-Span-216"><span class="mrow" id="MathJax-Span-217"><span class="mo" id="MathJax-Span-218" style="font-family: MathJax_Size2; vertical-align: 0.003em;">∑</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.475em; left: 1.464em;"><span class="mrow" id="MathJax-Span-219"><span class="mi" id="MathJax-Span-220" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="munder" id="MathJax-Span-221" style="padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 1.817em; height: 0px;"><span style="position: absolute; clip: rect(2.876em, 1001.41em, 4.589em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-222"><span class="mstyle" id="MathJax-Span-223"><span class="mrow" id="MathJax-Span-224"><span class="mrow" id="MathJax-Span-225"><span class="mo" id="MathJax-Span-226" style="font-family: MathJax_Size2; vertical-align: 0.003em;">∑</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.475em; left: 1.464em;"><span class="mrow" id="MathJax-Span-227"><span class="mi" id="MathJax-Span-228" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mi" id="MathJax-Span-229" style="font-family: MathJax_Main-bold; padding-left: 0.154em;">G</span><span class="mfenced" id="MathJax-Span-230" style="padding-left: 0.154em;"><span class="mo" id="MathJax-Span-231" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mrow" id="MathJax-Span-232"><span class="mi" id="MathJax-Span-233" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-234" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-235" style="font-family: MathJax_Math-italic; padding-left: 0.154em;">j</span></span><span class="mo" id="MathJax-Span-236" style=""><span style="font-family: MathJax_Main;">)</span></span></span><span class="mtext" id="MathJax-Span-237" style="font-family: MathJax_Main;"> </span><span class="mi" id="MathJax-Span-238" style="font-family: MathJax_Main; padding-left: 0.154em;">log</span><span class="mfenced" id="MathJax-Span-239" style="padding-left: 0.154em;"><span class="mo" id="MathJax-Span-240" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mrow" id="MathJax-Span-241"><span class="mi" id="MathJax-Span-242" style="font-family: MathJax_Main-bold;">G</span><span class="mfenced" id="MathJax-Span-243" style="padding-left: 0.154em;"><span class="mo" id="MathJax-Span-244" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mrow" id="MathJax-Span-245"><span class="mi" id="MathJax-Span-246" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-247" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-248" style="font-family: MathJax_Math-italic; padding-left: 0.154em;">j</span></span><span class="mo" id="MathJax-Span-249" style=""><span style="font-family: MathJax_Main;">)</span></span></span></span><span class="mo" id="MathJax-Span-250" style=""><span style="font-family: MathJax_Main;">)</span></span></span><span class="mo" id="MathJax-Span-251" style="font-family: MathJax_Main;">.</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.119em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.747em; border-left: 0px solid; width: 0px; height: 2.066em;"></span></span></nobr></span></div><script type="math/mml" id="MathJax-Element-5"><math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block" id="M5" overflow="scroll"><mtable><mtr><mtd><mi mathvariant="normal">ENT</mi><mo>=</mo><mo>−</mo><munder><mrow><mstyle displaystyle="true"><mrow><mo stretchy="true">∑</mo></mrow></mstyle></mrow><mrow><mi>i</mi></mrow></munder><munder><mrow><mstyle displaystyle="true"><mrow><mo stretchy="true">∑</mo></mrow></mstyle></mrow><mrow><mi>j</mi></mrow></munder><mi mathvariant="bold">G</mi><mfenced open="(" close=")" separators="|"><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></mfenced><mtext> </mtext><mi mathvariant="normal">log</mi><mfenced open="(" close=")" separators="|"><mrow><mi mathvariant="bold">G</mi><mfenced open="(" close=")" separators="|"><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></mfenced></mrow></mfenced><mo>.</mo></mtd></mtr></mtable></math></script></div><div class="l">(5)</div></div><p></p><p id="idm140658771482112">Entropy is a measurement of the uncertainty, and it can be used to denote the uncertain information about the image. When all of the elements in matrix <strong>G</strong> are equal, the image contains the largest amount of uncertain information based on the largest ENT value. By this time, the distribution of the gray values in the image is very complicated.</p><div class="disp-formula" id="EEq6"><div class="f"><span class="MathJax_Preview"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-6-Frame" tabindex="0" style=""><nobr><span class="math" id="M6" overflow="scroll" role="math" style="width: 17.644em; display: inline-block;"><span style="display: inline-block; position: relative; width: 14.216em; height: 0px; font-size: 124%;"><span style="position: absolute; clip: rect(0.91em, 1013.96em, 2.825em, -999.997em); top: -2.114em; left: 0.003em;"><span class="mrow" id="MathJax-Span-253"><span class="mtable" id="MathJax-Span-254" style="padding-right: 0.154em; padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 13.964em; height: 0px;"><span style="position: absolute; clip: rect(2.775em, 1013.91em, 4.69em, -999.997em); top: -3.979em; left: 0.003em;"><span style="display: inline-block; position: relative; width: 13.964em; height: 0px;"><span style="position: absolute; clip: rect(2.876em, 1013.91em, 4.74em, -999.997em); top: -4.08em; left: 50%; margin-left: -7.004em;"><span class="mtd" id="MathJax-Span-255"><span class="mrow" id="MathJax-Span-256"><span class="mi" id="MathJax-Span-257" style="font-family: MathJax_Main;">CON</span><span class="mo" id="MathJax-Span-258" style="font-family: MathJax_Main; padding-left: 0.305em;">=</span><span class="munder" id="MathJax-Span-259" style="padding-left: 0.305em;"><span style="display: inline-block; position: relative; width: 1.767em; height: 0px;"><span style="position: absolute; clip: rect(2.876em, 1001.41em, 4.589em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-260"><span class="mstyle" id="MathJax-Span-261"><span class="mrow" id="MathJax-Span-262"><span class="mrow" id="MathJax-Span-263"><span class="mo" id="MathJax-Span-264" style="font-family: MathJax_Size2; vertical-align: 0.003em;">∑</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.475em; left: 1.464em;"><span class="mrow" id="MathJax-Span-265"><span class="mi" id="MathJax-Span-266" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="munder" id="MathJax-Span-267" style="padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 1.817em; height: 0px;"><span style="position: absolute; clip: rect(2.876em, 1001.41em, 4.589em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-268"><span class="mstyle" id="MathJax-Span-269"><span class="mrow" id="MathJax-Span-270"><span class="mrow" id="MathJax-Span-271"><span class="mo" id="MathJax-Span-272" style="font-family: MathJax_Size2; vertical-align: 0.003em;">∑</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.475em; left: 1.464em;"><span class="mrow" id="MathJax-Span-273"><span class="mi" id="MathJax-Span-274" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msup" id="MathJax-Span-275" style="padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 3.128em; height: 0px;"><span style="position: absolute; clip: rect(3.077em, 1002.62em, 4.388em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-276"><span class="mfenced" id="MathJax-Span-277"><span class="mo" id="MathJax-Span-278" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mrow" id="MathJax-Span-279"><span class="mi" id="MathJax-Span-280" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-281" style="font-family: MathJax_Main; padding-left: 0.204em;">−</span><span class="mi" id="MathJax-Span-282" style="font-family: MathJax_Math-italic; padding-left: 0.204em;">j</span></span><span class="mo" id="MathJax-Span-283" style=""><span style="font-family: MathJax_Main;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -4.433em; left: 2.724em;"><span class="mrow" id="MathJax-Span-284"><span class="mn" id="MathJax-Span-285" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mi" id="MathJax-Span-286" style="font-family: MathJax_Main-bold;">G</span><span class="mfenced" id="MathJax-Span-287" style="padding-left: 0.154em;"><span class="mo" id="MathJax-Span-288" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mrow" id="MathJax-Span-289"><span class="mi" id="MathJax-Span-290" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-291" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-292" style="font-family: MathJax_Math-italic; padding-left: 0.154em;">j</span></span><span class="mo" id="MathJax-Span-293" style=""><span style="font-family: MathJax_Main;">)</span></span></span><span class="mo" id="MathJax-Span-294" style="font-family: MathJax_Main;">.</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.119em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.747em; border-left: 0px solid; width: 0px; height: 2.066em;"></span></span></nobr></span></div><script type="math/mml" id="MathJax-Element-6"><math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block" id="M6" overflow="scroll"><mtable><mtr><mtd><mi mathvariant="normal">CON</mi><mo>=</mo><munder><mrow><mstyle displaystyle="true"><mrow><mo stretchy="true">∑</mo></mrow></mstyle></mrow><mrow><mi>i</mi></mrow></munder><munder><mrow><mstyle displaystyle="true"><mrow><mo stretchy="true">∑</mo></mrow></mstyle></mrow><mrow><mi>j</mi></mrow></munder><msup><mrow><mfenced open="(" close=")" separators="|"><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow></mfenced></mrow><mrow><mn>2</mn></mrow></msup><mi mathvariant="bold">G</mi><mfenced open="(" close=")" separators="|"><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></mfenced><mo>.</mo></mtd></mtr></mtable></math></script></div><div class="l">(6)</div></div><p></p><p id="idm140658777328400">Contrast is a measure of how the data in the image are distributed or the image clarity. The larger the value of CON, the clearer the image.</p><div class="disp-formula" id="EEq7"><div class="f"><span class="MathJax_Preview"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-7-Frame" tabindex="0" style=""><nobr><span class="math" id="M7" overflow="scroll" role="math" style="width: 14.821em; display: inline-block;"><span style="display: inline-block; position: relative; width: 11.948em; height: 0px; font-size: 124%;"><span style="position: absolute; clip: rect(0.658em, 1011.7em, 3.077em, -999.997em); top: -2.114em; left: 0.003em;"><span class="mrow" id="MathJax-Span-296"><span class="mtable" id="MathJax-Span-297" style="padding-right: 0.154em; padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 11.696em; height: 0px;"><span style="position: absolute; clip: rect(2.523em, 1011.65em, 4.942em, -999.997em); top: -3.979em; left: 0.003em;"><span style="display: inline-block; position: relative; width: 11.696em; height: 0px;"><span style="position: absolute; clip: rect(2.271em, 1011.65em, 4.69em, -999.997em); top: -3.727em; left: 50%; margin-left: -5.844em;"><span class="mtd" id="MathJax-Span-298"><span class="mrow" id="MathJax-Span-299"><span class="mtext" id="MathJax-Span-300" style="font-family: MathJax_Main;">COR</span><span class="mo" id="MathJax-Span-301" style="font-family: MathJax_Main; padding-left: 0.305em;">=</span><span class="mfrac" id="MathJax-Span-302" style="padding-left: 0.305em;"><span style="display: inline-block; position: relative; width: 7.563em; height: 0px; margin-right: 0.103em; margin-left: 0.103em;"><span style="position: absolute; clip: rect(3.128em, 1007.46em, 4.589em, -999.997em); top: -4.836em; left: 50%; margin-left: -3.727em;"><span class="mrow" id="MathJax-Span-303"><span class="msubsup" id="MathJax-Span-304"><span style="display: inline-block; position: relative; width: 1.313em; height: 0px;"><span style="position: absolute; clip: rect(3.178em, 1000.96em, 4.438em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-305"><span class="msup" id="MathJax-Span-306"><span style="display: inline-block; position: relative; width: 1.011em; height: 0px;"><span style="position: absolute; clip: rect(3.178em, 1000.96em, 4.438em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-307"><span class="mstyle" id="MathJax-Span-308"><span class="mrow" id="MathJax-Span-309"><span class="mrow" id="MathJax-Span-310"><span class="mo" id="MathJax-Span-311" style="font-size: 70.7%; font-family: MathJax_Size2; vertical-align: 0.003em;">∑</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.632em, 1000.3em, 4.136em, -999.997em); top: -4.433em; left: 1.011em;"><span class="mrow" id="MathJax-Span-312"><span class="mi" id="MathJax-Span-313" style="font-size: 50%; font-family: MathJax_Math-italic;">s</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.48em, 1000.2em, 4.136em, -999.997em); top: -3.627em; left: 1.011em;"><span class="mrow" id="MathJax-Span-314"><span class="mi" id="MathJax-Span-315" style="font-size: 50%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-316" style="padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 1.313em; height: 0px;"><span style="position: absolute; clip: rect(3.178em, 1000.96em, 4.438em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-317"><span class="msup" id="MathJax-Span-318"><span style="display: inline-block; position: relative; width: 1.011em; height: 0px;"><span style="position: absolute; clip: rect(3.178em, 1000.96em, 4.438em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-319"><span class="mstyle" id="MathJax-Span-320"><span class="mrow" id="MathJax-Span-321"><span class="mrow" id="MathJax-Span-322"><span class="mo" id="MathJax-Span-323" style="font-size: 70.7%; font-family: MathJax_Size2; vertical-align: 0.003em;">∑</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.632em, 1000.3em, 4.136em, -999.997em); top: -4.433em; left: 1.011em;"><span class="mrow" id="MathJax-Span-324"><span class="mi" id="MathJax-Span-325" style="font-size: 50%; font-family: MathJax_Math-italic;">s</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.48em, 1000.25em, 4.236em, -999.997em); top: -3.627em; left: 1.011em;"><span class="mrow" id="MathJax-Span-326"><span class="mi" id="MathJax-Span-327" style="font-size: 50%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mi" id="MathJax-Span-328" style="font-size: 70.7%; font-family: MathJax_Math-italic; padding-left: 0.255em;">i</span><span class="mi" id="MathJax-Span-329" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span class="mi" id="MathJax-Span-330" style="font-size: 70.7%; font-family: MathJax_Main-bold;">G</span><span class="mfenced" id="MathJax-Span-331"><span class="mo" id="MathJax-Span-332" style=""><span><span style="font-size: 70.7%; font-family: MathJax_Main;">(</span></span></span><span class="mrow" id="MathJax-Span-333"><span class="mi" id="MathJax-Span-334" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-335" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-336" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span><span class="mo" id="MathJax-Span-337" style=""><span><span style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span></span></span><span class="mo" id="MathJax-Span-338" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="msub" id="MathJax-Span-339"><span style="display: inline-block; position: relative; width: 0.759em; height: 0px;"><span style="position: absolute; clip: rect(3.531em, 1000.41em, 4.287em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-340"><span class="mi" id="MathJax-Span-341" style="font-size: 70.7%; font-family: MathJax_Math-italic;">μ</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.778em; left: 0.406em;"><span class="mrow" id="MathJax-Span-342"><span class="mi" id="MathJax-Span-343" style="font-size: 50%; font-family: MathJax_Math-italic;">x</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msub" id="MathJax-Span-344"><span style="display: inline-block; position: relative; width: 0.708em; height: 0px;"><span style="position: absolute; clip: rect(3.531em, 1000.41em, 4.287em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-345"><span class="mi" id="MathJax-Span-346" style="font-size: 70.7%; font-family: MathJax_Math-italic;">μ</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.778em; left: 0.406em;"><span class="mrow" id="MathJax-Span-347"><span class="mi" id="MathJax-Span-348" style="font-size: 50%; font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.531em, 1001.46em, 4.337em, -999.997em); top: -3.627em; left: 50%; margin-left: -0.703em;"><span class="mrow" id="MathJax-Span-349"><span class="msub" id="MathJax-Span-350"><span style="display: inline-block; position: relative; width: 0.759em; height: 0px;"><span style="position: absolute; clip: rect(3.531em, 1000.41em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-351"><span class="mi" id="MathJax-Span-352" style="font-size: 70.7%; font-family: MathJax_Math-italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.879em; left: 0.406em;"><span class="mrow" id="MathJax-Span-353"><span class="mi" id="MathJax-Span-354" style="font-size: 50%; font-family: MathJax_Math-italic;">x</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msub" id="MathJax-Span-355"><span style="display: inline-block; position: relative; width: 0.708em; height: 0px;"><span style="position: absolute; clip: rect(3.531em, 1000.41em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-356"><span class="mi" id="MathJax-Span-357" style="font-size: 70.7%; font-family: MathJax_Math-italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.879em; left: 0.406em;"><span class="mrow" id="MathJax-Span-358"><span class="mi" id="MathJax-Span-359" style="font-size: 50%; font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(0.859em, 1007.56em, 1.212em, -999.997em); top: -1.258em; left: 0.003em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-top: 1.3px solid; width: 7.563em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.061em;"></span></span></span></span><span class="mo" id="MathJax-Span-360" style="font-family: MathJax_Main;">.</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.119em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.059em; border-left: 0px solid; width: 0px; height: 2.753em;"></span></span></nobr></span></div><script type="math/mml" id="MathJax-Element-7"><math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block" id="M7" overflow="scroll"><mtable><mtr><mtd><mtext>COR</mtext><mo>=</mo><mfrac><mrow><msubsup><mrow><msup><mrow><mstyle displaystyle="true"><mrow><mo stretchy="false">∑</mo></mrow></mstyle></mrow><mrow></mrow></msup></mrow><mrow><mi>i</mi></mrow><mrow><mi>s</mi></mrow></msubsup><msubsup><mrow><msup><mrow><mstyle displaystyle="true"><mrow><mo stretchy="false">∑</mo></mrow></mstyle></mrow><mrow></mrow></msup></mrow><mrow><mi>j</mi></mrow><mrow><mi>s</mi></mrow></msubsup><mi>i</mi><mi>j</mi><mi mathvariant="bold">G</mi><mfenced open="(" close=")" separators="|"><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></mfenced><mo>−</mo><msub><mrow><mi>μ</mi></mrow><mrow><mi>x</mi></mrow></msub><msub><mrow><mi>μ</mi></mrow><mrow><mi>y</mi></mrow></msub></mrow><mrow><msub><mrow><mi>σ</mi></mrow><mrow><mi>x</mi></mrow></msub><msub><mrow><mi>σ</mi></mrow><mrow><mi>y</mi></mrow></msub></mrow></mfrac><mo>.</mo></mtd></mtr></mtable></math></script></div><div class="l">(7)</div></div><p></p><p id="idm140658766979728">After calculating ASM, ENT, CON, and COR, we continue to determine the mean and standard deviation of each of the others, which results in a texture feature vector. The feature vector will be used together with the color moment as the traditional features.</p><p id="idm140658766979200" class="p p-last">Color moment features are always represented by the mean, standard deviation, and the third-order color moment. The mean will display the lightness or darkness of the image; the standard deviation can reflect the range of the image color distribution; and the third-order color moment shows the symmetry of the image color distribution. Therefore, this approach leads to a color moment feature vector. The color moment definition is as follows:</p><div class="disp-formula" id="EEq8"><div class="f"><span class="MathJax_Preview"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-8-Frame" tabindex="0" style=""><nobr><span class="math" id="M8" overflow="scroll" role="math" style="width: 16.938em; display: inline-block;"><span style="display: inline-block; position: relative; width: 13.662em; height: 0px; font-size: 124%;"><span style="position: absolute; clip: rect(-2.014em, 1013.41em, 5.748em, -999.997em); top: -2.114em; left: 0.003em;"><span class="mrow" id="MathJax-Span-362"><span class="mtable" id="MathJax-Span-363" style="padding-right: 0.154em; padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 13.41em; height: 0px;"><span style="position: absolute; clip: rect(4.337em, 1013.31em, 12.099em, -999.997em); top: -8.465em; left: 0.003em;"><span style="display: inline-block; position: relative; width: 13.41em; height: 0px;"><span style="position: absolute; clip: rect(2.674em, 1008.97em, 4.74em, -999.997em); top: -6.802em; left: 50%; margin-left: -4.534em;"><span class="mtd" id="MathJax-Span-364"><span class="mrow" id="MathJax-Span-365"><span class="msub" id="MathJax-Span-366"><span style="display: inline-block; position: relative; width: 1.061em; height: 0px;"><span style="position: absolute; clip: rect(3.128em, 1000.71em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-367"><span class="mi" id="MathJax-Span-368" style="font-family: MathJax_Math-italic;">A</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.828em; left: 0.759em;"><span class="mrow" id="MathJax-Span-369"><span class="mi" id="MathJax-Span-370" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-371" style="font-family: MathJax_Main; padding-left: 0.305em;">=</span><span class="mfrac" id="MathJax-Span-372" style="padding-left: 0.305em;"><span style="display: inline-block; position: relative; width: 0.759em; height: 0px; margin-right: 0.103em; margin-left: 0.103em;"><span style="position: absolute; clip: rect(3.38em, 1000.3em, 4.136em, -999.997em); top: -4.383em; left: 50%; margin-left: -0.199em;"><span class="mn" id="MathJax-Span-373" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.329em, 1000.61em, 4.136em, -999.997em); top: -3.576em; left: 50%; margin-left: -0.3em;"><span class="mi" id="MathJax-Span-374" style="font-size: 70.7%; font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.053em;"></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(0.859em, 1000.76em, 1.212em, -999.997em); top: -1.258em; left: 0.003em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-top: 1.3px solid; width: 0.759em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.061em;"></span></span></span></span><span class="munderover" id="MathJax-Span-375" style="padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 2.17em; height: 0px;"><span style="position: absolute; clip: rect(2.876em, 1001.41em, 4.589em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-376"><span class="mstyle" id="MathJax-Span-377"><span class="mrow" id="MathJax-Span-378"><span class="mrow" id="MathJax-Span-379"><span class="mo" id="MathJax-Span-380" style="font-family: MathJax_Size2; vertical-align: 0.003em;">∑</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.329em, 1000.71em, 4.136em, -999.997em); top: -4.635em; left: 1.464em;"><span class="mrow" id="MathJax-Span-381"><span class="mi" id="MathJax-Span-382" style="font-size: 70.7%; font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.053em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.38em, 1000.36em, 4.287em, -999.997em); top: -3.475em; left: 1.464em;"><span class="mrow" id="MathJax-Span-383"><span class="mi" id="MathJax-Span-384" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mi" id="MathJax-Span-385" style="font-family: MathJax_Math-italic; padding-left: 0.154em;">P<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.103em;"></span></span><span class="mfenced" id="MathJax-Span-386" style="padding-left: 0.154em;"><span class="mo" id="MathJax-Span-440" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mrow" id="MathJax-Span-388"><span class="mi" id="MathJax-Span-389" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-390" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-391" style="font-family: MathJax_Math-italic; padding-left: 0.154em;">j</span></span><span class="mo" id="MathJax-Span-441" style=""><span style="font-family: MathJax_Main;">)</span></span></span><span class="mo" id="MathJax-Span-393" style="font-family: MathJax_Main;">,</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(2.371em, 1013.31em, 5.043em, -999.997em); top: -4.282em; left: 50%; margin-left: -6.701em;"><span class="mtd" id="MathJax-Span-394"><span class="mrow" id="MathJax-Span-395"><span class="msub" id="MathJax-Span-396"><span style="display: inline-block; position: relative; width: 0.91em; height: 0px;"><span style="position: absolute; clip: rect(3.128em, 1000.76em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-397"><span class="mi" id="MathJax-Span-398" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.204em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.828em; left: 0.607em;"><span class="mrow" id="MathJax-Span-399"><span class="mi" id="MathJax-Span-400" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-401" style="font-family: MathJax_Main; padding-left: 0.305em;">=</span><span class="msqrt" id="MathJax-Span-402" style="padding-left: 0.305em;"><span style="display: inline-block; position: relative; width: 10.789em; height: 0px;"><span style="position: absolute; clip: rect(2.674em, 1009.78em, 4.74em, -999.997em); top: -3.979em; left: 1.011em;"><span class="mrow" id="MathJax-Span-403"><span class="mrow" id="MathJax-Span-404"><span class="mfrac" id="MathJax-Span-405"><span style="display: inline-block; position: relative; width: 0.759em; height: 0px; margin-right: 0.103em; margin-left: 0.103em;"><span style="position: absolute; clip: rect(3.38em, 1000.3em, 4.136em, -999.997em); top: -4.383em; left: 50%; margin-left: -0.199em;"><span class="mn" id="MathJax-Span-406" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.329em, 1000.61em, 4.136em, -999.997em); top: -3.576em; left: 50%; margin-left: -0.3em;"><span class="mi" id="MathJax-Span-407" style="font-size: 70.7%; font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.053em;"></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(0.859em, 1000.76em, 1.212em, -999.997em); top: -1.258em; left: 0.003em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-top: 1.3px solid; width: 0.759em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.061em;"></span></span></span></span><span class="munderover" id="MathJax-Span-408" style="padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 2.17em; height: 0px;"><span style="position: absolute; clip: rect(2.876em, 1001.41em, 4.589em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-409"><span class="mstyle" id="MathJax-Span-410"><span class="mrow" id="MathJax-Span-411"><span class="mrow" id="MathJax-Span-412"><span class="mo" id="MathJax-Span-413" style="font-family: MathJax_Size2; vertical-align: 0.003em;">∑</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.329em, 1000.71em, 4.136em, -999.997em); top: -4.635em; left: 1.464em;"><span class="mrow" id="MathJax-Span-414"><span class="mi" id="MathJax-Span-415" style="font-size: 70.7%; font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.053em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.38em, 1000.36em, 4.287em, -999.997em); top: -3.475em; left: 1.464em;"><span class="mrow" id="MathJax-Span-416"><span class="mi" id="MathJax-Span-417" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msup" id="MathJax-Span-418" style="padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 6.353em; height: 0px;"><span style="position: absolute; clip: rect(3.077em, 1005.8em, 4.388em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-419"><span class="mfenced" id="MathJax-Span-420"><span class="mo" id="MathJax-Span-442" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mrow" id="MathJax-Span-422"><span class="mi" id="MathJax-Span-423" style="font-family: MathJax_Math-italic;">P<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.103em;"></span></span><span class="mfenced" id="MathJax-Span-424" style="padding-left: 0.154em;"><span class="mo" id="MathJax-Span-443" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mrow" id="MathJax-Span-426"><span class="mi" id="MathJax-Span-427" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-428" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-429" style="font-family: MathJax_Math-italic; padding-left: 0.154em;">j</span></span><span class="mo" id="MathJax-Span-444" style=""><span style="font-family: MathJax_Main;">)</span></span></span><span class="mo" id="MathJax-Span-431" style="font-family: MathJax_Main; padding-left: 0.204em;">−</span><span class="msub" id="MathJax-Span-432" style="padding-left: 0.204em;"><span style="display: inline-block; position: relative; width: 1.061em; height: 0px;"><span style="position: absolute; clip: rect(3.128em, 1000.71em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-433"><span class="mi" id="MathJax-Span-434" style="font-family: MathJax_Math-italic;">A</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.828em; left: 0.759em;"><span class="mrow" id="MathJax-Span-435"><span class="mi" id="MathJax-Span-436" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span class="mo" id="MathJax-Span-445" style=""><span style="font-family: MathJax_Main;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -4.433em; left: 5.9em;"><span class="mrow" id="MathJax-Span-438"><span class="mn" id="MathJax-Span-439" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.581em, 1009.78em, 3.884em, -999.997em); top: -5.139em; left: 1.011em;"><span style="display: inline-block; position: relative; width: 9.781em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -3.979em; left: -0.098em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -3.979em; left: 9.126em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 0.456em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 0.96em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 1.515em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 2.069em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 2.623em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 3.128em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 3.682em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 4.236em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 4.791em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 5.295em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 5.849em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 6.404em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 6.958em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 7.462em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 8.017em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 8.571em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(2.371em, 1001.01em, 5.093em, -999.997em); top: -4.03em; left: 0.003em;"><span style="font-family: MathJax_Size3;">√</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-446" style="font-family: MathJax_Main;">,</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(2.371em, 1013.31em, 5.043em, -999.997em); top: -1.409em; left: 50%; margin-left: -6.701em;"><span class="mtd" id="MathJax-Span-447"><span class="mrow" id="MathJax-Span-448"><span class="msub" id="MathJax-Span-449"><span style="display: inline-block; position: relative; width: 0.91em; height: 0px;"><span style="position: absolute; clip: rect(3.128em, 1000.66em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-450"><span class="mi" id="MathJax-Span-451" style="font-family: MathJax_Math-italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.053em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.828em; left: 0.607em;"><span class="mrow" id="MathJax-Span-452"><span class="mi" id="MathJax-Span-453" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-454" style="font-family: MathJax_Main; padding-left: 0.305em;">=</span><span class="mroot" id="MathJax-Span-455" style="padding-left: 0.305em;"><span style="display: inline-block; position: relative; width: 10.789em; height: 0px;"><span style="position: absolute; clip: rect(2.674em, 1009.78em, 4.74em, -999.997em); top: -3.979em; left: 1.011em;"><span class="mrow" id="MathJax-Span-456"><span class="mfrac" id="MathJax-Span-457"><span style="display: inline-block; position: relative; width: 0.759em; height: 0px; margin-right: 0.103em; margin-left: 0.103em;"><span style="position: absolute; clip: rect(3.38em, 1000.3em, 4.136em, -999.997em); top: -4.383em; left: 50%; margin-left: -0.199em;"><span class="mn" id="MathJax-Span-458" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.329em, 1000.61em, 4.136em, -999.997em); top: -3.576em; left: 50%; margin-left: -0.3em;"><span class="mi" id="MathJax-Span-459" style="font-size: 70.7%; font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.053em;"></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(0.859em, 1000.76em, 1.212em, -999.997em); top: -1.258em; left: 0.003em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-top: 1.3px solid; width: 0.759em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.061em;"></span></span></span></span><span class="munderover" id="MathJax-Span-460" style="padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 2.17em; height: 0px;"><span style="position: absolute; clip: rect(2.876em, 1001.41em, 4.589em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-461"><span class="mstyle" id="MathJax-Span-462"><span class="mrow" id="MathJax-Span-463"><span class="mrow" id="MathJax-Span-464"><span class="mo" id="MathJax-Span-465" style="font-family: MathJax_Size2; vertical-align: 0.003em;">∑</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.329em, 1000.71em, 4.136em, -999.997em); top: -4.635em; left: 1.464em;"><span class="mrow" id="MathJax-Span-466"><span class="mi" id="MathJax-Span-467" style="font-size: 70.7%; font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.053em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.38em, 1000.36em, 4.287em, -999.997em); top: -3.475em; left: 1.464em;"><span class="mrow" id="MathJax-Span-468"><span class="mi" id="MathJax-Span-469" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msup" id="MathJax-Span-470" style="padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 6.353em; height: 0px;"><span style="position: absolute; clip: rect(3.077em, 1005.8em, 4.388em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-471"><span class="mfenced" id="MathJax-Span-472"><span class="mo" id="MathJax-Span-473" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mrow" id="MathJax-Span-474"><span class="mi" id="MathJax-Span-475" style="font-family: MathJax_Math-italic;">P<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.103em;"></span></span><span class="mfenced" id="MathJax-Span-476" style="padding-left: 0.154em;"><span class="mo" id="MathJax-Span-477" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mrow" id="MathJax-Span-478"><span class="mi" id="MathJax-Span-479" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-480" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-481" style="font-family: MathJax_Math-italic; padding-left: 0.154em;">j</span></span><span class="mo" id="MathJax-Span-482" style=""><span style="font-family: MathJax_Main;">)</span></span></span><span class="mo" id="MathJax-Span-483" style="font-family: MathJax_Main; padding-left: 0.204em;">−</span><span class="msub" id="MathJax-Span-484" style="padding-left: 0.204em;"><span style="display: inline-block; position: relative; width: 1.061em; height: 0px;"><span style="position: absolute; clip: rect(3.128em, 1000.71em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-485"><span class="mi" id="MathJax-Span-486" style="font-family: MathJax_Math-italic;">A</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.828em; left: 0.759em;"><span class="mrow" id="MathJax-Span-487"><span class="mi" id="MathJax-Span-488" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span class="mo" id="MathJax-Span-489" style=""><span style="font-family: MathJax_Main;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -4.433em; left: 5.9em;"><span class="mrow" id="MathJax-Span-490"><span class="mn" id="MathJax-Span-491" style="font-size: 70.7%; font-family: MathJax_Main;">3</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.581em, 1009.78em, 3.884em, -999.997em); top: -5.139em; left: 1.011em;"><span style="display: inline-block; position: relative; width: 9.781em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -3.979em; left: -0.098em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -3.979em; left: 9.126em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 0.456em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 0.96em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 1.515em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 2.069em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 2.623em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 3.128em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 3.682em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 4.236em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 4.791em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 5.295em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 5.849em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 6.404em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 6.958em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 7.462em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 8.017em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -3.979em; left: 8.571em;">−<span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(2.371em, 1001.01em, 5.093em, -999.997em); top: -4.03em; left: 0.003em;"><span style="font-family: MathJax_Size3;">√</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.48em, 1000.25em, 4.136em, -999.997em); top: -4.332em; left: 0.406em;"><span class="mrow" id="MathJax-Span-492"><span class="mn" id="MathJax-Span-493" style="font-size: 50%; font-family: MathJax_Main;">3</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-494" style="font-family: MathJax_Main;">.</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span><span style="display: inline-block; width: 0px; height: 8.47em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.119em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -4.372em; border-left: 0px solid; width: 0px; height: 9.378em;"></span></span></nobr></span></div><script type="math/mml" id="MathJax-Element-8"><math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block" id="M8" overflow="scroll"><mtable><mtr><mtd><msub><mrow><mi>A</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mrow><mstyle displaystyle="true"><mrow><mo stretchy="true">∑</mo></mrow></mstyle></mrow><mrow><mi>j</mi></mrow><mrow><mi>N</mi></mrow></munderover><mi>P</mi><mfenced open="(" close=")" separators="|"><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></mfenced><mo>,</mo></mtd></mtr><mtr><mtd><msub><mrow><mi>V</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mrow><mstyle displaystyle="true"><mrow><mo stretchy="true">∑</mo></mrow></mstyle></mrow><mrow><mi>j</mi></mrow><mrow><mi>N</mi></mrow></munderover><msup><mrow><mfenced open="(" close=")" separators="|"><mrow><mi>P</mi><mfenced open="(" close=")" separators="|"><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></mfenced><mo>−</mo><msub><mrow><mi>A</mi></mrow><mrow><mi>i</mi></mrow></msub></mrow></mfenced></mrow><mrow><mn>2</mn></mrow></msup></mrow></msqrt><mo>,</mo></mtd></mtr><mtr><mtd><msub><mrow><mi>S</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>=</mo><mroot><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mrow><mstyle displaystyle="true"><mrow><mo stretchy="true">∑</mo></mrow></mstyle></mrow><mrow><mi>j</mi></mrow><mrow><mi>N</mi></mrow></munderover><msup><mrow><mfenced open="(" close=")" separators="|"><mrow><mi>P</mi><mfenced open="(" close=")" separators="|"><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></mfenced><mo>−</mo><msub><mrow><mi>A</mi></mrow><mrow><mi>i</mi></mrow></msub></mrow></mfenced></mrow><mrow><mn>3</mn></mrow></msup></mrow><mrow><mn>3</mn></mrow></mroot><mo>.</mo></mtd></mtr></mtable></math></script></div><div class="l">(8)</div></div><p></p></div><div id="sec3.3" class="sec sec-last"><h3 id="sec3.3title">3.3. Feature Fusion</h3><p id="idm140658767525600" class="p p-first">After extracting the high-level features and traditional features, we design two different fusion approaches to fuse the features. The first method is to set a fixed proportion λ, which calls the <em>R</em> feature fusion. The fusion feature for classification is computed as follows:</p><div class="disp-formula" id="EEq9"><div class="f">NF =&nbsp;<em>λ</em>&nbsp;·&nbsp;LF +&nbsp;(1&nbsp;−&nbsp;<em>λ</em>)&nbsp;·&nbsp;HF,&nbsp;</div><div class="l">(9)</div></div><p>where NF is the fusion feature, and LF and HF indicate traditional features and high-level features, respectively. The <em>λ</em> is the weight parameter that signifies the importance between two different features. This method is very easy to implement because it is locally weighted. Once we have obtained the parameter <em>λ</em>, there is no need to recalculate. The feature fusion will feed into softmax to accomplish the last classification task. However, this method is only for linear feature fusion, and it required a large number of experiments to obtain the parameter <em>λ</em>. Above all, it is difficult to fuse the features to effectively represent the images. Additionally, if it changes to another dataset, then the same experiment is required to obtain the parameter <em>λ</em> again.</p><p id="idm140658769757728" class="p p-last">To solve these problems, we propose another approach, which can automatically adjust the proportion between high-level features and traditional features, to avoid the boring, time-consuming process of obtaining the parameter <em>λ</em>. The method is to train a multilayer perceptron neural network that can fuse the features in nonlinear feature space. The fusion feature (RF) operation is defined as follows:</p><div class="disp-formula" id="EEq10"><div class="f"><span class="MathJax_Preview"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-9-Frame" tabindex="0" style=""><nobr><span class="math" id="M10" overflow="scroll" role="math" style="width: 23.541em; display: inline-block;"><span style="display: inline-block; position: relative; width: 18.954em; height: 0px; font-size: 124%;"><span style="position: absolute; clip: rect(0.809em, 1018.75em, 2.926em, -999.997em); top: -2.114em; left: 0.003em;"><span class="mrow" id="MathJax-Span-496"><span class="mtable" id="MathJax-Span-497" style="padding-right: 0.154em; padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 18.702em; height: 0px;"><span style="position: absolute; clip: rect(2.674em, 1018.65em, 4.791em, -999.997em); top: -3.979em; left: 0.003em;"><span style="display: inline-block; position: relative; width: 18.702em; height: 0px;"><span style="position: absolute; clip: rect(2.674em, 1018.65em, 4.791em, -999.997em); top: -3.979em; left: 50%; margin-left: -9.372em;"><span class="mtd" id="MathJax-Span-498"><span class="mrow" id="MathJax-Span-499"><span class="mtext" id="MathJax-Span-500" style="font-family: MathJax_Main;">RF</span><span class="mo" id="MathJax-Span-501" style="font-family: MathJax_Main; padding-left: 0.305em;">=</span><span class="mi" id="MathJax-Span-502" style="font-family: MathJax_Main; padding-left: 0.305em;">max</span><span class="mfenced" id="MathJax-Span-503" style="padding-left: 0.154em;"><span class="mo" id="MathJax-Span-504" style="vertical-align: 0.003em;"><span style="font-family: MathJax_Size2;">(</span></span><span class="mrow" id="MathJax-Span-505"><span class="mn" id="MathJax-Span-506" style="font-family: MathJax_Main;">0</span><span class="mo" id="MathJax-Span-507" style="font-family: MathJax_Main;">,</span><span class="mo" id="MathJax-Span-508" style="font-family: MathJax_Main; padding-left: 0.154em;"> </span><span class="mo" id="MathJax-Span-509" style="font-family: MathJax_Main; padding-left: 0.154em;"> </span><span class="munderover" id="MathJax-Span-510" style="padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 1.968em; height: 0px;"><span style="position: absolute; clip: rect(2.876em, 1001.41em, 4.589em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-511"><span class="mstyle" id="MathJax-Span-512"><span class="mrow" id="MathJax-Span-513"><span class="mrow" id="MathJax-Span-514"><span class="mo" id="MathJax-Span-515" style="font-family: MathJax_Size2; vertical-align: 0.003em;">∑</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.531em, 1000.51em, 4.136em, -999.997em); top: -4.635em; left: 1.464em;"><span class="mrow" id="MathJax-Span-516"><span class="mi" id="MathJax-Span-517" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.38em, 1000.3em, 4.136em, -999.997em); top: -3.475em; left: 1.464em;"><span class="mrow" id="MathJax-Span-518"><span class="mi" id="MathJax-Span-519" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msub" id="MathJax-Span-520" style="padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 1.061em; height: 0px;"><span style="position: absolute; clip: rect(3.38em, 1000.71em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-521"><span class="mi" id="MathJax-Span-522" style="font-family: MathJax_Math-italic;">w</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.828em; left: 0.708em;"><span class="mrow" id="MathJax-Span-523"><span class="mi" id="MathJax-Span-524" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msub" id="MathJax-Span-525"><span style="display: inline-block; position: relative; width: 0.607em; height: 0px;"><span style="position: absolute; clip: rect(3.128em, 1000.25em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-526"><span class="mi" id="MathJax-Span-527" style="font-family: MathJax_Math-italic;">l</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.828em; left: 0.305em;"><span class="mrow" id="MathJax-Span-528"><span class="mi" id="MathJax-Span-529" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-530" style="font-family: MathJax_Main; padding-left: 0.204em;">+</span><span class="munderover" id="MathJax-Span-531" style="padding-left: 0.204em;"><span style="display: inline-block; position: relative; width: 2.119em; height: 0px;"><span style="position: absolute; clip: rect(2.876em, 1001.41em, 4.589em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-532"><span class="mstyle" id="MathJax-Span-533"><span class="mrow" id="MathJax-Span-534"><span class="mrow" id="MathJax-Span-535"><span class="mo" id="MathJax-Span-536" style="font-family: MathJax_Size2; vertical-align: 0.003em;">∑</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.531em, 1000.71em, 4.136em, -999.997em); top: -4.635em; left: 1.464em;"><span class="mrow" id="MathJax-Span-537"><span class="mi" id="MathJax-Span-538" style="font-size: 70.7%; font-family: MathJax_Math-italic;">m</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.38em, 1000.36em, 4.287em, -999.997em); top: -3.475em; left: 1.464em;"><span class="mrow" id="MathJax-Span-539"><span class="mi" id="MathJax-Span-540" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msub" id="MathJax-Span-541" style="padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 1.061em; height: 0px;"><span style="position: absolute; clip: rect(3.38em, 1000.71em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-542"><span class="mi" id="MathJax-Span-543" style="font-family: MathJax_Math-italic;">w</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.828em; left: 0.708em;"><span class="mrow" id="MathJax-Span-544"><span class="mi" id="MathJax-Span-545" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msub" id="MathJax-Span-546"><span style="display: inline-block; position: relative; width: 0.96em; height: 0px;"><span style="position: absolute; clip: rect(3.128em, 1000.56em, 4.136em, -999.997em); top: -3.979em; left: 0.003em;"><span class="mrow" id="MathJax-Span-547"><span class="mi" id="MathJax-Span-548" style="font-family: MathJax_Math-italic;">h</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.828em; left: 0.557em;"><span class="mrow" id="MathJax-Span-549"><span class="mi" id="MathJax-Span-550" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-551" style="font-family: MathJax_Main; padding-left: 0.204em;">+</span><span class="mi" id="MathJax-Span-552" style="font-family: MathJax_Math-italic; padding-left: 0.204em;">b</span></span><span class="mo" id="MathJax-Span-553" style="vertical-align: 0.003em;"><span style="font-family: MathJax_Size2;">)</span></span></span><span class="mo" id="MathJax-Span-554" style="font-family: MathJax_Main;">,</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.119em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.378em;"></span></span></nobr></span></div><script type="math/mml" id="MathJax-Element-9"><math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block" id="M10" overflow="scroll"><mtable><mtr><mtd><mtext>RF</mtext><mo>=</mo><mi mathvariant="normal">max</mi><mfenced open="(" close=")" separators="|"><mrow><mn>0</mn><mo>,</mo><mo> </mo><mo> </mo><munderover><mrow><mstyle displaystyle="true"><mrow><mo stretchy="true">∑</mo></mrow></mstyle></mrow><mrow><mi>i</mi></mrow><mrow><mi>n</mi></mrow></munderover><msub><mrow><mi>w</mi></mrow><mrow><mi>i</mi></mrow></msub><msub><mrow><mi>l</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>+</mo><munderover><mrow><mstyle displaystyle="true"><mrow><mo stretchy="true">∑</mo></mrow></mstyle></mrow><mrow><mi>j</mi></mrow><mrow><mi>m</mi></mrow></munderover><msub><mrow><mi>w</mi></mrow><mrow><mi>j</mi></mrow></msub><msub><mrow><mi>h</mi></mrow><mrow><mi>j</mi></mrow></msub><mo>+</mo><mi>b</mi></mrow></mfenced><mo>,</mo></mtd></mtr></mtable></math></script></div><div class="l">(10)</div></div><p>where LF={<em>l</em><sub>1</sub>, <em>l</em><sub>2</sub>, …, <em>l</em><sub><em>i</em></sub>, …, <em>l</em><sub><em>n</em></sub>} and HF={<em>h</em><sub>1</sub>, <em>h</em><sub>2</sub>, …, <em>h</em><sub><em>j</em></sub>, …, <em>h</em><sub><em>m</em></sub>} represent traditional and high-level features and <em>b</em> is the bias. The multilayer perceptron contains a fully connected layer and a softmax layer as a classifier. It is consistent with the kernel function idea, which would map low-dimensional information into high-dimensional information. Therefore, it can gain better discriminative features for medical images than using a linear feature space. This approach will be further demonstrated in the following experiments. In addition, it may greatly reduce the amount of computation in that it would not attempt to determine the same parameter again and again.</p></div></div><div id="sec4" class="tsec sec"><div class="goto jig-ncbiinpagenav-goto-container"><a class="tgt_dark page-toc-label jig-ncbiinpagenav-goto-heading" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" title="Go to other sections in this page" role="button" aria-expanded="false" aria-haspopup="true">Go to:</a></div><h2 class="head no_bottom_margin ui-helper-clearfix" id="sec4title">4. Experiment and Evaluation</h2><p id="idm140658773530976" class="p p-first">We implemented the coding network to extract the high-level features in MatConvnet, which is a matlab toolbox that implements convolutional neural networks, as well as extracted the traditional features based on color moment and texture. We have designed a series of experiments to verify the effectiveness of our method on two benchmark medical image datasets. One is the HIS2828 dataset, and the other is the ISIC2017 dataset. We conducted all of our experiments on a computer with i5-6500 3.2 GHz CPU, 32G main memory, and GTX1060 GPU.</p><div id="sec4.1" class="sec"><h3 id="sec4.1title">4.1. The HIS2828 and ISIC2017 Datasets</h3><p id="idm140658773529648" class="p p-first">The HIS2828 dataset is composed of 4 classes of fundamental tissue images that are representative of different tissue types. Each image is an RGB image of size 720 ∗ 480. This dataset contains 2828 images, which can be listed as follows: 1026 nervous tissue images, 484 connective tissue images, 804 epithelial tissue images, and 514 muscular tissue images, in which we utilize 1, 2, 3, and 4 to represent the labels. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/table/tab3/" target="table" class="fig-table-link figpopup" rid-figpopup="tab3" rid-ob="ob-tab3" co-legend-rid=""><span> Table 3</span></a> displays the composition of the HIS2828 dataset.</p><!--table ft1--><!--table-wrap mode="anchored" t5--><div class="table-wrap table anchored whole_rhythm" id="tab3"><h3>Table 3</h3><!--caption a7--><div class="caption"><p id="idm140658771846128">The composition of the HIS2828 dataset.</p></div><div data-largeobj="" data-largeobj-link-rid="largeobj_idm140658768372544" class="xtable"><table frame="hsides" rules="groups" class="rendered small default_table"><thead><tr><th align="left" rowspan="1" colspan="1">Image category</th><th align="center" rowspan="1" colspan="1">Number of images</th><th align="center" rowspan="1" colspan="1">Label</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Nervous tissue</td><td align="center" rowspan="1" colspan="1">1026</td><td align="center" rowspan="1" colspan="1">1</td></tr><tr><td align="left" rowspan="1" colspan="1">Connective tissue</td><td align="center" rowspan="1" colspan="1">484</td><td align="center" rowspan="1" colspan="1">2</td></tr><tr><td align="left" rowspan="1" colspan="1">Epithelial tissue</td><td align="center" rowspan="1" colspan="1">804</td><td align="center" rowspan="1" colspan="1">3</td></tr><tr><td align="left" rowspan="1" colspan="1">Muscular tissue</td><td align="center" rowspan="1" colspan="1">514</td><td align="center" rowspan="1" colspan="1">4</td></tr></tbody></table></div><div id="largeobj_idm140658768372544" class="largeobj-link align_right" style="display: none"><a target="object" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/table/tab3/?report=objectonly">Open in a separate window</a></div></div><p id="idm140658776571952">ISIC2017 is a dataset of skin lesions that is provided by The International Skin Imaging Collaboration(ISIC). It includes 2000 images; 374 of them are malignant skin tumors referred to as ”Melanoma” and 1626 of them are benign skin tumors referred to as ”Nevus of Seborrheic Keratosis”. Thus, it is a binary image classification task that distinguishes between (a) Melanoma and (b) Nevus and Seborrheic Keratosis. Each image in this dataset has a different resolution, which we must address. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/table/tab4/" target="table" class="fig-table-link figpopup" rid-figpopup="tab4" rid-ob="ob-tab4" co-legend-rid=""><span> Table 4</span></a> shows the composition of the ISIC2017 dataset.</p><!--table ft1--><!--table-wrap mode="anchored" t5--><div class="table-wrap table anchored whole_rhythm" id="tab4"><h3>Table 4</h3><!--caption a7--><div class="caption"><p id="idm140658770859232">The composition of the ISIC2017 dataset.</p></div><div data-largeobj="" data-largeobj-link-rid="largeobj_idm140658770858976" class="xtable"><table frame="hsides" rules="groups" class="rendered small default_table"><thead><tr><th align="left" rowspan="1" colspan="1">Image category</th><th align="center" rowspan="1" colspan="1">Number of images</th><th align="center" rowspan="1" colspan="1">Label</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Melanoma</td><td align="center" rowspan="1" colspan="1">374</td><td align="center" rowspan="1" colspan="1">1</td></tr><tr><td align="left" rowspan="1" colspan="1">Nevus of seborrheic keratosis</td><td align="center" rowspan="1" colspan="1">1626</td><td align="center" rowspan="1" colspan="1">2</td></tr></tbody></table></div><div id="largeobj_idm140658770858976" class="largeobj-link align_right" style="display: none"><a target="object" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/table/tab4/?report=objectonly">Open in a separate window</a></div></div><p id="idm140658771633088">In order to evaluate our experiments, we employed the following configuration. First, each dataset was divided into a training set, a validation set, and a test set with the ratio 7 : 1 : 2. Then, all the methods were evaluated using 10-fold cross validation. After that, the images were randomly cropped from the original dataset in order to obtain fixed-size 140 × 140 images for feeding them into the coding network. For HIS2828 dataset, each image was randomly cropped to 420 × 420 and then resized to the fixed-size 140 × 140 image. However, for ISIC2017 dataset, prior to resizing to 140 × 140, we extracted random patches with two-thirds of the original height and width for images having different resolutions. This would save the image information to a great extent and reduce the computation complexity effectively. These works can not only obtain the fixed-size but also augment the image samples. In addition, we would flip the image horizontality or verticality to further amplify the image datasets. At test time, the network makes a prediction to each patche and averaging of the predictions made by the softmax layer if the patches belongs to the same image. The influence of image augmentation on the accuracy and running time will be discussed in the following experiments.</p><p id="idm140658779171744" class="p p-last">The network architecture of our coding framework is presented detail in <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/table/tab1/" target="table" class="fig-table-link figpopup" rid-figpopup="tab1" rid-ob="ob-tab1" co-legend-rid=""><span>Table 1</span></a>. As shown in the <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/table/tab1/" target="table" class="fig-table-link figpopup" rid-figpopup="tab1" rid-ob="ob-tab1" co-legend-rid=""><span>Table 1</span></a>, it is able to converge after 45 epoches. Finally, for each convolutional layer, we used ReLus as an activation function. Besides, batch normalization was also employed in order to accelerate deep network training.</p></div><div id="sec4.2" class="sec"><h3 id="sec4.2title">4.2. Accuracy</h3><p id="idm140658765687360" class="p p-first">In this section, we will provide a series of experiments on the accuracy and algorithm running time on two real medical image datasets. The accuracy here is defined as the percentage of correctly classified medical images. To better compare the algorithms, we employ the confusion matrix and receiver operating characteristic (ROC) curve to further evaluate the model. The confusion matrix is a table layout that can describe the number of true positive, false negative, true negative, and false negative in an evaluation of a multiclass image classification algorithm. The ROC curve is a graphic plot that is obtained through computing the true positive rate (TPR) against the false positive rate (FPR) by setting different thresholds, where the definition of TPR and FPR are as follows:</p><div class="disp-formula" id="EEq11"><div class="f"><span class="MathJax_Preview"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-10-Frame" tabindex="0" style=""><nobr><span class="math" id="M11" overflow="scroll" role="math" style="width: 8.621em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.958em; height: 0px; font-size: 124%;"><span style="position: absolute; clip: rect(0.154em, 1006.76em, 3.581em, -999.997em); top: -2.114em; left: 0.003em;"><span class="mrow" id="MathJax-Span-556"><span class="mtable" id="MathJax-Span-557" style="padding-right: 0.154em; padding-left: 0.154em;"><span style="display: inline-block; position: relative; width: 6.706em; height: 0px;"><span style="position: absolute; clip: rect(2.119em, 1006.66em, 5.547em, -999.997em); top: -4.08em; left: 0.003em;"><span style="display: inline-block; position: relative; width: 6.706em; height: 0px;"><span style="position: absolute; clip: rect(2.926em, 1006.66em, 4.589em, -999.997em); top: -4.887em; left: 50%; margin-left: -3.324em;"><span class="mtd" id="MathJax-Span-558"><span class="mrow" id="MathJax-Span-559"><span class="mtext" id="MathJax-Span-560" style="font-family: MathJax_Main;">TPR</span><span class="mo" id="MathJax-Span-561" style="font-family: MathJax_Main; padding-left: 0.305em;">=</span><span class="mfrac" id="MathJax-Span-562" style="padding-left: 0.305em;"><span style="display: inline-block; position: relative; width: 2.674em; height: 0px; margin-right: 0.103em; margin-left: 0.103em;"><span style="position: absolute; clip: rect(3.329em, 1000.96em, 4.136em, -999.997em); top: -4.383em; left: 50%; margin-left: -0.502em;"><span class="mrow" id="MathJax-Span-563"><span class="mtext" id="MathJax-Span-564" style="font-size: 70.7%; font-family: MathJax_Main;">TP</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.329em, 1002.52em, 4.186em, -999.997em); top: -3.576em; left: 50%; margin-left: -1.258em;"><span class="mrow" id="MathJax-Span-565"><span class="mtext" id="MathJax-Span-566" style="font-size: 70.7%; font-family: MathJax_Main;">TP</span><span class="mo" id="MathJax-Span-567" style="font-size: 70.7%; font-family: MathJax_Main;">+</span><span class="mtext" id="MathJax-Span-568" style="font-size: 70.7%; font-family: MathJax_Main;">FN</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(0.859em, 1002.67em, 1.212em, -999.997em); top: -1.258em; left: 0.003em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-top: 1.3px solid; width: 2.674em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.061em;"></span></span></span></span><span class="mo" id="MathJax-Span-569" style="font-family: MathJax_Main;">,</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(2.926em, 1006.55em, 4.589em, -999.997em); top: -3.122em; left: 50%; margin-left: -3.324em;"><span class="mtd" id="MathJax-Span-570"><span class="mrow" id="MathJax-Span-571"><span class="mtext" id="MathJax-Span-572" style="font-family: MathJax_Main;">FPR</span><span class="mo" id="MathJax-Span-573" style="font-family: MathJax_Main; padding-left: 0.305em;">=</span><span class="mfrac" id="MathJax-Span-574" style="padding-left: 0.305em;"><span style="display: inline-block; position: relative; width: 2.674em; height: 0px; margin-right: 0.103em; margin-left: 0.103em;"><span style="position: absolute; clip: rect(3.329em, 1000.91em, 4.136em, -999.997em); top: -4.383em; left: 50%; margin-left: -0.451em;"><span class="mrow" id="MathJax-Span-575"><span class="mtext" id="MathJax-Span-576" style="font-size: 70.7%; font-family: MathJax_Main;">FP</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.329em, 1002.52em, 4.186em, -999.997em); top: -3.576em; left: 50%; margin-left: -1.258em;"><span class="mrow" id="MathJax-Span-577"><span class="mtext" id="MathJax-Span-578" style="font-size: 70.7%; font-family: MathJax_Main;">FP</span><span class="mo" id="MathJax-Span-579" style="font-size: 70.7%; font-family: MathJax_Main;">+</span><span class="mtext" id="MathJax-Span-580" style="font-size: 70.7%; font-family: MathJax_Main;">TN</span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(0.859em, 1002.67em, 1.212em, -999.997em); top: -1.258em; left: 0.003em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-top: 1.3px solid; width: 2.674em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.061em;"></span></span></span></span><span class="mo" id="MathJax-Span-581" style="font-family: MathJax_Main;">,</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.085em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.119em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.684em; border-left: 0px solid; width: 0px; height: 3.941em;"></span></span></nobr></span></div><script type="math/mml" id="MathJax-Element-10"><math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block" id="M11" overflow="scroll"><mtable><mtr><mtd><mtext>TPR</mtext><mo>=</mo><mfrac><mrow><mtext>TP</mtext></mrow><mrow><mtext>TP</mtext><mo>+</mo><mtext>FN</mtext></mrow></mfrac><mo>,</mo></mtd></mtr><mtr><mtd><mtext>FPR</mtext><mo>=</mo><mfrac><mrow><mtext>FP</mtext></mrow><mrow><mtext>FP</mtext><mo>+</mo><mtext>TN</mtext></mrow></mfrac><mo>,</mo></mtd></mtr></mtable></math></script></div><div class="l">(11)</div></div><p>where TP, FN, FP, and TN are true positive, false negative, false positive, and true negative, respectively. It is very useful to exhibit the performance of the binary class image classification algorithm. In the following, the support vector machine (SVM) is determined to be a universal classifier in machine learning before the appearance of the deep learning model; therefore, the SVM (traditional feature) and SVM (traditional and deep feature) that is the concatenation of traditional and deep feature will be compared with the CNMP model. Here, we employ the package of LibSVM-3.17 to train a one-vs-one multiclass classifier with radial basis function (RBF) kernal. To demonstrate the effectiveness of combining features, a comparison with the coding network becomes necessary. Furthermore, compared with <em>R</em> feature fusion and KPCA feature fusion will show that the CNMP contains a better feature fusion approach. The reason why we choose KPCA with the RBF kernal to fuse features is that it could map the feature into nonlinear space as well. The feature fusion vector will feed into softmax to finish classification task.</p><p id="idm140658774112288">The results on the HIS2828 and ISIC2017 datasets with regard to the accuracy are shown in <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/table/tab5/" target="table" class="fig-table-link figpopup" rid-figpopup="tab5" rid-ob="ob-tab5" co-legend-rid=""><span>Table 5</span></a>. Our method achieves the best accuracy rate, which is 90.2% and 90.1%, respectively, on the two datasets. From the table, we can see that the accuracy of SVM (traditional feature) is the lowest. Even if we utilize the coding network to classify the medical image, it would achieve better performance. This finding proves that high-level features can represent a medical image better than traditional features. Our model obtains higher accuracy than the two previous methods, and thus, combining the two different types of features can work very well because the combined features would signify the images from a multiscale perspective. It is obvious that SVM (traditional and deep feature) may work better than coding network and SVM (traditional features). In addition, comparing our model with the <em>R</em> feature fusion and KPCA feature fusion, we can conclude that automatic feature fusion not only obtains better performance but also avoids the tedious process of manually adjusting the parameters.</p><!--table ft1--><!--table-wrap mode="anchored" t5--><div class="table-wrap table anchored whole_rhythm" id="tab5"><h3>Table 5</h3><!--caption a7--><div class="caption"><p id="idm140658781015856">Comparison of the classification algorithms accuracy.</p></div><div data-largeobj="" data-largeobj-link-rid="largeobj_idm140658781015600" class="xtable"><table frame="hsides" rules="groups" class="rendered small default_table"><thead><tr><th align="left" rowspan="1" colspan="1">Algorithm</th><th align="center" rowspan="1" colspan="1">HIS2828</th><th align="center" rowspan="1" colspan="1">ISIC2017</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">SVM (traditional feature)</td><td align="center" rowspan="1" colspan="1">72.17%</td><td align="center" rowspan="1" colspan="1">66.1%</td></tr><tr><td align="left" rowspan="1" colspan="1">Coding network</td><td align="center" rowspan="1" colspan="1">79.5%</td><td align="center" rowspan="1" colspan="1">75%</td></tr><tr><td align="left" rowspan="1" colspan="1">CNMP</td><td align="center" rowspan="1" colspan="1">90.2%</td><td align="center" rowspan="1" colspan="1">90.1%</td></tr><tr><td align="left" rowspan="1" colspan="1">R feature fusion</td><td align="center" rowspan="1" colspan="1">86.3%</td><td align="center" rowspan="1" colspan="1">88.7%</td></tr><tr><td align="left" rowspan="1" colspan="1">SVM (traditional and deep feature)</td><td align="center" rowspan="1" colspan="1">81.1%</td><td align="center" rowspan="1" colspan="1">77.6%</td></tr><tr><td align="left" rowspan="1" colspan="1">KPCA feature fusion</td><td align="center" rowspan="1" colspan="1">84.9%</td><td align="center" rowspan="1" colspan="1">87.4%</td></tr></tbody></table></div><div id="largeobj_idm140658781015600" class="largeobj-link align_right" style="display: none"><a target="object" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/table/tab5/?report=objectonly">Open in a separate window</a></div></div><p id="idm140658774953664">As is known, the overall accuracy cannot be an appropriate measure for evaluating an image classification algorithm, especially when the image dataset has the problem of having an imbalanced distribution. It is obvious that the HIS2018 dataset has the sample imbalance problem. Here, with the purpose of making a better comparison, we employ the confusion matrix to further evaluate the algorithms. In confusion matrix, the first four diagonal cells represent the number and percentage of correct predictions made by the model on the test set. The pink-shaded cells illustrate incorrect predictions, and the percentage corresponds to all data in test set. The gray-shaded cells in the last column of the matrix show the percentage of correctly identified positive predictions called the sensitivity or recall rate, while the gray shaded cells in the last row represents the precision rate of each class. Finally, the last diagonal cell represents the overall accuracy. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig2/" target="figure" class="fig-table-link figpopup" rid-figpopup="fig2" rid-ob="ob-fig2" co-legend-rid="lgnd_fig2"><span> Figure 2</span></a> demonstrates that since nervous tissue and epithelial tissue have more training samples, they can gain higher precision and recall. Moreover, from <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig2/" target="figure" class="fig-table-link figpopup" rid-figpopup="fig2" rid-ob="ob-fig2" co-legend-rid="lgnd_fig2"><span>Figure 2(d)</span></a>, the CNMP algorithm has the highest precision and recall in every category, which proves the efficiency of our model. In <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig2/" target="figure" class="fig-table-link figpopup" rid-figpopup="fig2" rid-ob="ob-fig2" co-legend-rid="lgnd_fig2"><span>Figure 2(c)</span></a>, <em>R</em> feature fusion has the second highest performance, which is close to CNMP in nervous tissue and epithelial tissue, with the exception of the other two categories; <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig2/" target="figure" class="fig-table-link figpopup" rid-figpopup="fig2" rid-ob="ob-fig2" co-legend-rid="lgnd_fig2"><span>Figure 2(a)</span></a> and <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig2/" target="figure" class="fig-table-link figpopup" rid-figpopup="fig2" rid-ob="ob-fig2" co-legend-rid="lgnd_fig2"><span>Figure 2(b)</span></a> show that the coding network can obtain a better outcome than SVM (traditional features). However, the unemployed multistage features restrict the SVM (traditional features) and coding network from achieving better performance. In addition, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig2/" target="figure" class="fig-table-link figpopup" rid-figpopup="fig2" rid-ob="ob-fig2" co-legend-rid="lgnd_fig2"><span>Figure 2(a)</span></a> states that the SVM (traditional features) is the most easily affected by the imbalance problem. From <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig2/" target="figure" class="fig-table-link figpopup" rid-figpopup="fig2" rid-ob="ob-fig2" co-legend-rid="lgnd_fig2"><span>Figure 2(c)</span></a> and <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig2/" target="figure" class="fig-table-link figpopup" rid-figpopup="fig2" rid-ob="ob-fig2" co-legend-rid="lgnd_fig2"><span>Figure 2(e)</span></a>, if it directly concatenated the features, it is a really bad practice. Comparing <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig2/" target="figure" class="fig-table-link figpopup" rid-figpopup="fig2" rid-ob="ob-fig2" co-legend-rid="lgnd_fig2"><span>Figure 2(f)</span></a> with <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig2/" target="figure" class="fig-table-link figpopup" rid-figpopup="fig2" rid-ob="ob-fig2" co-legend-rid="lgnd_fig2"><span>Figure 2(d)</span></a> proves the effectiveness of our fusion strategy again. In general, when the image dataset has an imbalance problem, it is possible for the SVM (traditional features) to obtain poor generalization ability. Instead, the deep model would be good at avoiding this problem to obtain a better outcome.</p><!--fig ft0--><!--fig mode=article f1--><div class="fig iconblock whole_rhythm clearfix" id="fig2" co-legend-rid="lgnd_fig2"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig2/" target="figure" rid-figpopup="fig2" rid-ob="ob-fig2"><!--fig/graphic|fig/alternatives/graphic mode="anchored" m1--></a><div data-largeobj="" data-largeobj-link-rid="largeobj_idm140658740601328" class="figure"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig2/" target="figure" rid-figpopup="fig2" rid-ob="ob-fig2"></a><a class="inline_block ts_canvas" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=6157177_CIN2018-2061516.002.jpg" target="tileshopwindow"><div class="ts_bar small" title="Click on image to zoom"></div><img alt="An external file that holds a picture, illustration, etc.
Object name is CIN2018-2061516.002.jpg" title="Click on image to zoom" class="tileshop" src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/CIN2018-2061516.002.jpg"></a></div><div id="largeobj_idm140658740601328" class="largeobj-link align_right" style=""><a target="object" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig2/?report=objectonly">Open in a separate window</a></div><div class="icnblk_cntnt" id="lgnd_fig2"><div><a class="figpopup" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig2/" target="figure" rid-figpopup="fig2" rid-ob="ob-fig2">Figure 2</a></div><!--caption a7--><div class="caption"><p id="idm140658740601952">Comparison of the confusion matrix on the histology dataset. (a) The confusion matrix of SVM (traditional features). (b) The confusion matrix of coding network. (c) The confusion matrix of R feature fusion. (d) The confusion matrix of CNMP. (e) The confusion matrix of SVM (traditional and deep feature). (f) The confusion matrix of KPCA feature fusion.</p></div></div></div><p id="idm140658775472144" class="p p-last">The receiver operating characteristic (ROC) curve is a graphic plot that is obtained through computing the TPR and FPR by setting different thresholds. It is useful to evaluate the performance of the binary class image classification algorithm since the binary dataset contains a sample imbalanced problem. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig3/" target="figure" class="fig-table-link figpopup" rid-figpopup="fig3" rid-ob="ob-fig3" co-legend-rid="lgnd_fig3"><span> Figure 3</span></a> compares the ROC curve of different algorithms on the ISIC2017 dataset. The closer the curve is to the upper left corner of the axes, the better the performance. It is easy to see that our model obtains the best performance and that <em>R</em> feature fusion has the second best performance. To make a more intuitive comparison, we will determine the area under the curve (AUC) to signify the predictive performance; the larger the value of the AUC, the better the performance. Huang and Ling [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B35" rid="B35" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">35</a>] have proven that the AUC is a statistically consistent and more discriminatory measure than the accuracy. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/table/tab6/" target="table" class="fig-table-link figpopup" rid-figpopup="tab6" rid-ob="ob-tab6" co-legend-rid=""><span> Table 6</span></a> shows the AUC values of the different algorithms. The AUC value of our model is the larger than that of the other three algorithms.</p><!--fig ft0--><!--fig mode=article f1--><div class="fig iconblock whole_rhythm clearfix" id="fig3" co-legend-rid="lgnd_fig3"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig3/" target="figure" rid-figpopup="fig3" rid-ob="ob-fig3"><!--fig/graphic|fig/alternatives/graphic mode="anchored" m1--><div data-largeobj="" data-largeobj-link-rid="largeobj_idm140658767335248" class="figure"><img class="fig-image" alt="An external file that holds a picture, illustration, etc.
Object name is CIN2018-2061516.003.jpg" title="An external file that holds a picture, illustration, etc.
Object name is CIN2018-2061516.003.jpg" src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/CIN2018-2061516.003.jpg"></div></a><div id="largeobj_idm140658767335248" class="largeobj-link align_right" style=""><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig3/" target="figure" rid-figpopup="fig3" rid-ob="ob-fig3"></a><a target="object" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig3/?report=objectonly">Open in a separate window</a></div><div class="icnblk_cntnt" id="lgnd_fig3"><div><a class="figpopup" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig3/" target="figure" rid-figpopup="fig3" rid-ob="ob-fig3">Figure 3</a></div><!--caption a7--><div class="caption"><p id="idm140658779086128">The ROC curve on the ISIC2017 dataset.</p></div></div></div><!--table ft1--><!--table-wrap mode="anchored" t5--><div class="table-wrap table anchored whole_rhythm" id="tab6"><h3>Table 6</h3><!--caption a7--><div class="caption"><p id="idm140658772445696">Comparison of the AUCs on the ISIC dataset.</p></div><div data-largeobj="" data-largeobj-link-rid="largeobj_idm140658772445440" class="xtable"><table frame="hsides" rules="groups" class="rendered small default_table"><thead><tr><th align="left" rowspan="1" colspan="1">Algorithm</th><th align="center" rowspan="1" colspan="1">The AUC of ROC</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">SVM (traditional feature)</td><td align="center" rowspan="1" colspan="1">0.7209</td></tr><tr><td align="left" rowspan="1" colspan="1">Coding network</td><td align="center" rowspan="1" colspan="1">0.8087</td></tr><tr><td align="left" rowspan="1" colspan="1">CNMP</td><td align="center" rowspan="1" colspan="1">0.9585</td></tr><tr><td align="left" rowspan="1" colspan="1">R feature fusion</td><td align="center" rowspan="1" colspan="1">0.9436</td></tr><tr><td align="left" rowspan="1" colspan="1">SVM (traditional and deep feature)</td><td align="center" rowspan="1" colspan="1">0.8210</td></tr><tr><td align="left" rowspan="1" colspan="1">KPCA feature fusion</td><td align="center" rowspan="1" colspan="1">0.9326</td></tr></tbody></table></div><div id="largeobj_idm140658772445440" class="largeobj-link align_right" style="display: none"><a target="object" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/table/tab6/?report=objectonly">Open in a separate window</a></div></div></div><div id="sec4.3" class="sec"><h3 id="sec4.3title">4.3. Running Time</h3><p id="idm140658776986464" class="p p-first-last">In this section, we also exhibit the running time of the four algorithms. The comparison can be seen in <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig4/" target="figure" class="fig-table-link figpopup" rid-figpopup="fig4" rid-ob="ob-fig4" co-legend-rid="lgnd_fig4"><span>Figure 4</span></a>. In the figure, there is no surprise that the SVM (traditional features) can run the fastest. It has several factors that contribute to this accomplishment. (1) It uses little image information along with abandoning a large amount of the image's spatial information. (2) This model must train very few parameters with respect to the deep model. The coding network is the second fastest algorithm. However, its running time greatly exceeds that of SVM (traditional features). The reason for this finding is that the deep model must train a mass of parameters to improve the generalization ability of the model. In addition, it takes advantage of almost all of the information in the image, by integrating the features layer by layer. Furthermore, we can know from this figure that although the <em>R</em> feature fusion and CNMP can achieve the best accuracy performance, their running time is rather long. SVM (traditional and deep feature) has the longest running time because it needs great time to get the high-level feature and traditional feature. Besides this, it must overcome high dimensionality to obtain the classification model. For KPCA feature fusion, it takes great time to fuse the different features.</p><!--fig ft0--><!--fig mode=article f1--><div class="fig iconblock whole_rhythm clearfix" id="fig4" co-legend-rid="lgnd_fig4"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig4/" target="figure" rid-figpopup="fig4" rid-ob="ob-fig4"><!--fig/graphic|fig/alternatives/graphic mode="anchored" m1--><div data-largeobj="" data-largeobj-link-rid="largeobj_idm140658771862128" class="figure"><img class="fig-image" alt="An external file that holds a picture, illustration, etc.
Object name is CIN2018-2061516.004.jpg" title="An external file that holds a picture, illustration, etc.
Object name is CIN2018-2061516.004.jpg" src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/CIN2018-2061516.004.jpg"></div></a><div id="largeobj_idm140658771862128" class="largeobj-link align_right" style=""><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig4/" target="figure" rid-figpopup="fig4" rid-ob="ob-fig4"></a><a target="object" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig4/?report=objectonly">Open in a separate window</a></div><div class="icnblk_cntnt" id="lgnd_fig4"><div><a class="figpopup" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig4/" target="figure" rid-figpopup="fig4" rid-ob="ob-fig4">Figure 4</a></div><!--caption a7--><div class="caption"><p id="idm140658771862384">The running time of different algorithms.</p></div></div></div></div><div id="sec4.4" class="sec sec-last"><h3 id="sec4.4title">4.4. The Variation of <em>m</em></h3><p id="idm140658779851376" class="p p-first">Here, we will conduct further study of how to expand the number of images that influence the algorithm's accuracy and running time. In reference [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B5" rid="B5" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">5</a>], the authors determined that increasing the image samples would affect the deep model performance. Nevertheless, the authors did not further research this matter with qualitative analysis. The relationship between the image augmentation times (<em>m</em>) and the algorithm's accuracy are shown in <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig5/" target="figure" class="fig-table-link figpopup" rid-figpopup="fig5" rid-ob="ob-fig5" co-legend-rid="lgnd_fig5"><span>Figure 5</span></a>. We can clearly determine that when the medical image dataset is relatively small, there is no difference between the deep model and SVM (traditional features). Additionally, it is possible that SVM (traditional features) has better accuracy performance. As the number of images is gradually increased, deep models have been shown to have a powerful generalization ability, especially with our model gaining the best performance. However, from <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig5/" target="figure" class="fig-table-link figpopup" rid-figpopup="fig5" rid-ob="ob-fig5" co-legend-rid="lgnd_fig5"><span>Figure 5</span></a>, it appears strange that SVM (traditional features) is hardly influenced by the increasing number of images. Even the coding network obtained better accuracy than the SVM (traditional feature). Extracting traditional features that cannot be a good abstraction of medical images could explain this finding.</p><!--fig ft0--><!--fig mode=article f1--><div class="fig iconblock whole_rhythm clearfix" id="fig5" co-legend-rid="lgnd_fig5"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig5/" target="figure" rid-figpopup="fig5" rid-ob="ob-fig5"><!--fig/graphic|fig/alternatives/graphic mode="anchored" m1--></a><div data-largeobj="" data-largeobj-link-rid="largeobj_idm140658772396096" class="figure"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig5/" target="figure" rid-figpopup="fig5" rid-ob="ob-fig5"></a><a class="inline_block ts_canvas" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=6157177_CIN2018-2061516.005.jpg" target="tileshopwindow"><div class="ts_bar small" title="Click on image to zoom"></div><img alt="An external file that holds a picture, illustration, etc.
Object name is CIN2018-2061516.005.jpg" title="Click on image to zoom" class="tileshop" src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/CIN2018-2061516.005.jpg"></a></div><div id="largeobj_idm140658772396096" class="largeobj-link align_right" style=""><a target="object" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig5/?report=objectonly">Open in a separate window</a></div><div class="icnblk_cntnt" id="lgnd_fig5"><div><a class="figpopup" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig5/" target="figure" rid-figpopup="fig5" rid-ob="ob-fig5">Figure 5</a></div><!--caption a7--><div class="caption"><p id="idm140658772396736">The variation <em>m</em> influence on algorithm's accuracy.</p></div></div></div><p id="idm140658774177344" class="p p-last">
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig6/" target="figure" class="fig-table-link figpopup" rid-figpopup="fig6" rid-ob="ob-fig6" co-legend-rid="lgnd_fig6"><span>Figure 6</span></a> shows that as the number of images increases, the running time of all of the algorithms increases. It is obvious that the deep model is not on the same order of magnitude as the SVM (traditional features). The running time of SVM (traditional features) is a linear correlation with the image numbers in a restricted range of time. However, although the running time of the deep model also has a linear relationship with the number of images, the line's slope is rather large. In addition, our model and <em>R</em> feature fusion have almost the same routine on the running time as the number of images increases. This finding proves our model's efficiency from the viewpoint of the running time.</p><!--fig ft0--><!--fig mode=article f1--><div class="fig iconblock whole_rhythm clearfix" id="fig6" co-legend-rid="lgnd_fig6"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig6/" target="figure" rid-figpopup="fig6" rid-ob="ob-fig6"><!--fig/graphic|fig/alternatives/graphic mode="anchored" m1--></a><div data-largeobj="" data-largeobj-link-rid="largeobj_idm140658775061136" class="figure"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig6/" target="figure" rid-figpopup="fig6" rid-ob="ob-fig6"></a><a class="inline_block ts_canvas" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=6157177_CIN2018-2061516.006.jpg" target="tileshopwindow"><div class="ts_bar small" title="Click on image to zoom"></div><img alt="An external file that holds a picture, illustration, etc.
Object name is CIN2018-2061516.006.jpg" title="Click on image to zoom" class="tileshop" src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/CIN2018-2061516.006.jpg"></a></div><div id="largeobj_idm140658775061136" class="largeobj-link align_right" style=""><a target="object" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig6/?report=objectonly">Open in a separate window</a></div><div class="icnblk_cntnt" id="lgnd_fig6"><div><a class="figpopup" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/figure/fig6/" target="figure" rid-figpopup="fig6" rid-ob="ob-fig6">Figure 6</a></div><!--caption a7--><div class="caption"><p id="idm140658775061776">The variation <em>m</em> influence on running time.</p></div></div></div></div></div><div id="sec5" class="tsec sec"><div class="goto jig-ncbiinpagenav-goto-container"><a class="tgt_dark page-toc-label jig-ncbiinpagenav-goto-heading" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" title="Go to other sections in this page" role="button" aria-expanded="false" aria-haspopup="true">Go to:</a></div><h2 class="head no_bottom_margin ui-helper-clearfix" id="sec5title">5. Conclusions and Summary</h2><p id="idm140658771564592" class="p p-first-last">In this paper, we presented a new medical image classification algorithm that combines high-level feature extraction from a coding network with traditional image features, and we call it CNMP. As far as we know, this study is the first time that a deep model has been directly utilized by including traditional image features to classify medical images. The experimental results show that our method can achieve an accuracy of 90.2% and 90.1% on the HIS2828 and ISIC2017 image datasets, which outperforms SVM (traditional features), coding network, and <em>R</em> feature fusion by considerable margins. Moreover, we discuss the influence of image extension on the algorithm's accuracy and running time. Future work could consider [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B36" rid="B36" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">36</a>, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B37" rid="B37" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">37</a>] for adding an efficient pruning strategy to greatly reduce the parameters. In addition, we could employ the ”Network in Network”(NIN) [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B38" rid="B38" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">38</a>] in the future, to gain better nonlinear high-level features for representations of medical images, which may achieve better performance than our model. In the aspect of feature fusion strategies, we are interested in developing more methods like multifeature fusion deep networks (MFFDN) [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B39" rid="B39" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">39</a>], based on denoising autoencoder, or metaspace fusion to combine homogeneous representations [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B40" rid="B40" class=" bibr popnode" role="button" aria-expanded="false" aria-haspopup="true">40</a>].</p></div><div id="idm140658777758400" class="tsec bk-sec"><div class="goto jig-ncbiinpagenav-goto-container"><a class="tgt_dark page-toc-label jig-ncbiinpagenav-goto-heading" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" title="Go to other sections in this page" role="button" aria-expanded="false" aria-haspopup="true">Go to:</a></div><h2 class="head no_bottom_margin ui-helper-clearfix" id="idm140658777758400title">Data Availability</h2><!--/article/back/sec/--><p id="idm140658776182720" class="p p-first-last">The ISIC dataset is taken from the website <a href="https://challenge.kitware.com/" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CBody&amp;TO=External%7CLink%7CURI" target="_blank">https://challenge.kitware.com/</a>. And the histology 2828 dataset is taken from <a href="http://www.informed.unal.edu.co/" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CBody&amp;TO=External%7CLink%7CURI" target="_blank">http://www.informed.unal.edu.co/</a>.</p></div><div id="idm140658774320688" class="tsec bk-sec"><div class="goto jig-ncbiinpagenav-goto-container"><a class="tgt_dark page-toc-label jig-ncbiinpagenav-goto-heading" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" title="Go to other sections in this page" role="button" aria-expanded="false" aria-haspopup="true">Go to:</a></div><h2 class="head no_bottom_margin ui-helper-clearfix" id="idm140658774320688title">Conflicts of Interest</h2><!--/article/back/sec/--><p id="idm140658774320304" class="p p-first-last">The authors declare that they have no conflicts of interest.</p></div><div id="idm140658774320048" class="tsec sec"><div class="goto jig-ncbiinpagenav-goto-container"><a class="tgt_dark page-toc-label jig-ncbiinpagenav-goto-heading" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" title="Go to other sections in this page" role="button" aria-expanded="false" aria-haspopup="true">Go to:</a></div><h2 class="head no_bottom_margin ui-helper-clearfix" id="idm140658774320048title">References</h2><div class="ref-list-sec sec" id="reference-list"><div class="ref-cit-blk half_rhythm" id="B1">1. <span class="element-citation">Cruzroa A., Caicedo J. C., Gonzalez F. A. Visual pattern mining in histology image collections using bag of features. <span><span class="ref-journal"><em>Artificial Intelligence in Medicine</em>. </span>2011;<span class="ref-vol">52</span>(2):91–106. doi: 10.1016/j.artmed.2011.04.010.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/21664806" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1016%2Fj.artmed.2011.04.010" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Artificial+Intelligence+in+Medicine&amp;title=Visual+pattern+mining+in+histology+image+collections+using+bag+of+features&amp;author=A.+Cruzroa&amp;author=J.+C.+Caicedo&amp;author=F.+A.+Gonzalez&amp;volume=52&amp;issue=2&amp;publication_year=2011&amp;pages=91-106&amp;pmid=21664806&amp;doi=10.1016/j.artmed.2011.04.010&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B2">2. <span class="element-citation">Esteva A., Kuprel B., Novoa R. A., et al.  Dermatologist-level classification of skin cancer with deep neural networks. <span><span class="ref-journal"><em>Nature</em>. </span>2017;<span class="ref-vol">542</span>(7639):115–118. doi: 10.1038/nature21056.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/28117445" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1038%2Fnature21056" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Nature&amp;title=Dermatologist-level+classification+of+skin+cancer+with+deep+neural+networks&amp;author=A.+Esteva&amp;author=B.+Kuprel&amp;author=R.+A.+Novoa&amp;volume=542&amp;issue=7639&amp;publication_year=2017&amp;pages=115-118&amp;pmid=28117445&amp;doi=10.1038/nature21056&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B3">3. <span class="element-citation">Zare M. R., Mueen A., Awedh M., Seng W. C. Automatic classification of medical x-ray images: hybrid generative-discriminative approach. <span><span class="ref-journal"><em>IET Image Processing</em>. </span>2013;<span class="ref-vol">7</span>(5):523–532. doi: 10.1049/iet-ipr.2013.0049.</span> [<a href="https://dx.doi.org/10.1049%2Fiet-ipr.2013.0049" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IET+Image+Processing&amp;title=Automatic+classification+of+medical+x-ray+images:+hybrid+generative-discriminative+approach&amp;author=M.+R.+Zare&amp;author=A.+Mueen&amp;author=M.+Awedh&amp;author=W.+C.+Seng&amp;volume=7&amp;issue=5&amp;publication_year=2013&amp;pages=523-532&amp;doi=10.1049/iet-ipr.2013.0049&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B4">4. <span class="element-citation">Hinton G. E., Osindero S., Teh Y.-W. A fast learning algorithm for deep belief nets. <span><span class="ref-journal"><em>Neural Computation</em>. </span>2006;<span class="ref-vol">18</span>(7):1527–1554. doi: 10.1162/neco.2006.18.7.1527.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/16764513" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1162%2Fneco.2006.18.7.1527" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Neural+Computation&amp;title=A+fast+learning+algorithm+for+deep+belief+nets&amp;author=G.+E.+Hinton&amp;author=S.+Osindero&amp;author=Y.-W.+Teh&amp;volume=18&amp;issue=7&amp;publication_year=2006&amp;pages=1527-1554&amp;pmid=16764513&amp;doi=10.1162/neco.2006.18.7.1527&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B5">5. <span class="element-citation">Krizhevsky A., Sutskever I., Hinton G. E. Imagenet classification with deep convolutional neural networks. <span><span class="ref-journal"><em>Communications of the ACM</em>. </span>2012;<span class="ref-vol">60</span>(6):1097–1105. doi: 10.1145/3065386.</span> [<a href="https://dx.doi.org/10.1145%2F3065386" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Communications+of+the+ACM&amp;title=Imagenet+classification+with+deep+convolutional+neural+networks&amp;author=A.+Krizhevsky&amp;author=I.+Sutskever&amp;author=G.+E.+Hinton&amp;volume=60&amp;issue=6&amp;publication_year=2012&amp;pages=1097-1105&amp;doi=10.1145/3065386&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B6">6. <span class="element-citation">Simonyan K., Zisserman A. Very deep convolutional networks for large-scale image recognition.   <a href="http://arxiv.org/abs/1409.1556" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">http://arxiv.org/abs/1409.1556</a>.</span></div><div class="ref-cit-blk half_rhythm" id="B7">7. <span class="element-citation">Barata C., Ruela M., Francisco M., Mendonça T., Marques J. S. Two systems for the detection of melanomas in dermoscopy images using texture and color features. <span><span class="ref-journal"><em>IEEE Systems Journal</em>. </span>2014;<span class="ref-vol">8</span>(3):965–979. doi: 10.1109/jsyst.2013.2271540.</span> [<a href="https://dx.doi.org/10.1109%2Fjsyst.2013.2271540" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IEEE+Systems+Journal&amp;title=Two+systems+for+the+detection+of+melanomas+in+dermoscopy+images+using+texture+and+color+features&amp;author=C.+Barata&amp;author=M.+Ruela&amp;author=M.+Francisco&amp;author=T.+Mendon%C3%A7a&amp;author=J.+S.+Marques&amp;volume=8&amp;issue=3&amp;publication_year=2014&amp;pages=965-979&amp;doi=10.1109/jsyst.2013.2271540&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B8">8. <span class="element-citation">Iyatomi H., Oka H., Celebi M. E., et al.  An improved internet-based melanoma screening system with dermatologist-like tumor area extraction algorithm. <span><span class="ref-journal"><em>Computerized Medical Imaging and Graphics</em>. </span>2008;<span class="ref-vol">32</span>(7):566–579. doi: 10.1016/j.compmedimag.2008.06.005.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/18703311" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1016%2Fj.compmedimag.2008.06.005" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Computerized+Medical+Imaging+and+Graphics&amp;title=An+improved+internet-based+melanoma+screening+system+with+dermatologist-like+tumor+area+extraction+algorithm&amp;author=H.+Iyatomi&amp;author=H.+Oka&amp;author=M.+E.+Celebi&amp;volume=32&amp;issue=7&amp;publication_year=2008&amp;pages=566-579&amp;pmid=18703311&amp;doi=10.1016/j.compmedimag.2008.06.005&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B9">9. <span class="element-citation">Stoecker W. V., Wronkiewiecz M., Chowdhury R., et al.  Detection of granularity in dermoscopy images of malignant melanoma using color and texture features. <span><span class="ref-journal"><em>Computerized Medical Imaging and Graphics</em>. </span>2011;<span class="ref-vol">35</span>(2):144–147. doi: 10.1016/j.compmedimag.2010.09.005.</span> <span class="nowrap">[<a class="int-reflink" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3159567/">PMC free article</a>]</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/21036538" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1016%2Fj.compmedimag.2010.09.005" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Computerized+Medical+Imaging+and+Graphics&amp;title=Detection+of+granularity+in+dermoscopy+images+of+malignant+melanoma+using+color+and+texture+features&amp;author=W.+V.+Stoecker&amp;author=M.+Wronkiewiecz&amp;author=R.+Chowdhury&amp;volume=35&amp;issue=2&amp;publication_year=2011&amp;pages=144-147&amp;pmid=21036538&amp;doi=10.1016/j.compmedimag.2010.09.005&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B10">10. <span class="element-citation">Riaz F., Hassan A., Nisar R., Dinis-Ribeiro M., Coimbra M. Content-adaptive region-based color texture descriptors for medical images. <span><span class="ref-journal"><em>IEEE Journal of Biomedical and Health Informatics</em>. </span>2017;<span class="ref-vol">21</span>(1):162–171. doi: 10.1109/jbhi.2015.2492464.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/26513811" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1109%2Fjbhi.2015.2492464" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IEEE+Journal+of+Biomedical+and+Health+Informatics&amp;title=Content-adaptive+region-based+color+texture+descriptors+for+medical+images&amp;author=F.+Riaz&amp;author=A.+Hassan&amp;author=R.+Nisar&amp;author=M.+Dinis-Ribeiro&amp;author=M.+Coimbra&amp;volume=21&amp;issue=1&amp;publication_year=2017&amp;pages=162-171&amp;pmid=26513811&amp;doi=10.1109/jbhi.2015.2492464&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B11">11. <span class="element-citation">Ramirez J., Gorriz J. M., Segovia F., et al.  Computer aided diagnosis system for the alzheimer’s disease based on partial least squares and random forest spect image classification. <span><span class="ref-journal"><em>Neuroscience Letters</em>. </span>2010;<span class="ref-vol">472</span>(2):99–103. doi: 10.1016/j.neulet.2010.01.056.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/20117177" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1016%2Fj.neulet.2010.01.056" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Neuroscience+Letters&amp;title=Computer+aided+diagnosis+system+for+the+alzheimer%E2%80%99s+disease+based+on+partial+least+squares+and+random+forest+spect+image+classification&amp;author=J.+Ramirez&amp;author=J.+M.+Gorriz&amp;author=F.+Segovia&amp;volume=472&amp;issue=2&amp;publication_year=2010&amp;pages=99-103&amp;pmid=20117177&amp;doi=10.1016/j.neulet.2010.01.056&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B12">12. <span class="element-citation">Zhang Y., Chen S., Wang S., Yang J., Phillips P. Magnetic resonance brain image classification based on weighted-type fractional fourier transform and nonparallel support vector machine. <span><span class="ref-journal"><em>International Journal of Imaging Systems and Technology</em>. </span>2015;<span class="ref-vol">25</span>(4):317–327. doi: 10.1002/ima.22144.</span> [<a href="https://dx.doi.org/10.1002%2Fima.22144" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=International+Journal+of+Imaging+Systems+and+Technology&amp;title=Magnetic+resonance+brain+image+classification+based+on+weighted-type+fractional+fourier+transform+and+nonparallel+support+vector+machine&amp;author=Y.+Zhang&amp;author=S.+Chen&amp;author=S.+Wang&amp;author=J.+Yang&amp;author=P.+Phillips&amp;volume=25&amp;issue=4&amp;publication_year=2015&amp;pages=317-327&amp;doi=10.1002/ima.22144&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B13">13. <span class="element-citation">Zhang Y., Dong Z., Liu A., et al.  Magnetic resonance brain image classification via stationary wavelet transform and generalized eigenvalue proximal support vector machine. <span><span class="ref-journal"><em>Journal of Medical Imaging and Health Informatics</em>. </span>2015;<span class="ref-vol">5</span>(7):1395–1403. doi: 10.1166/jmihi.2015.1542.</span> [<a href="https://dx.doi.org/10.1166%2Fjmihi.2015.1542" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Journal+of+Medical+Imaging+and+Health+Informatics&amp;title=Magnetic+resonance+brain+image+classification+via+stationary+wavelet+transform+and+generalized+eigenvalue+proximal+support+vector+machine&amp;author=Y.+Zhang&amp;author=Z.+Dong&amp;author=A.+Liu&amp;volume=5&amp;issue=7&amp;publication_year=2015&amp;pages=1395-1403&amp;doi=10.1166/jmihi.2015.1542&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B14">14. <span class="element-citation">Li Q., Cai W., Wang X., Zhou Y., Feng D. D., Chen M. Medical image classification with convolutional neural network. Proceedings of 13th International Conference on Control Automation Robotics &amp; Vision (ICARCV); December 2014; Singapore. IEEE; pp. 844–848. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Li+Q.+Cai+W.+Wang+X.+Zhou+Y.+Feng+D.+D.+Chen+M.+Medical+image+classification+with+convolutional+neural+network+Proceedings+of+13th+International+Conference+on+Control+Automation+Robotics+&amp;+Vision+(ICARCV)+December+2014+Singapore+IEEE+844+848+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B15">15. <span class="element-citation">Bar Y., Diamant I., Wolf L., Greenspan H. Deep learning with non-medical training used for chest pathology identification. Proceedings of SPIE Medical Imaging; February 2015; Orlando, FL, USA. International Society for Optics and Photonics; p. p. 94140V. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Bar+Y.+Diamant+I.+Wolf+L.+Greenspan+H.+Deep+learning+with+non-medical+training+used+for+chest+pathology+identification+Proceedings+of+SPIE+Medical+Imaging+February+2015+Orlando,+FL,+USA+International+Society+for+Optics+and+Photonics+p.+94140V+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B16">16. <span class="element-citation">Shin H.-C., Roth H. R., Gao M., et al.  Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning. <span><span class="ref-journal"><em>IEEE Transactions on Medical Imaging</em>. </span>2016;<span class="ref-vol">35</span>(5):1285–1298. doi: 10.1109/tmi.2016.2528162.</span> <span class="nowrap">[<a class="int-reflink" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4890616/">PMC free article</a>]</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/26886976" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1109%2Ftmi.2016.2528162" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IEEE+Transactions+on+Medical+Imaging&amp;title=Deep+convolutional+neural+networks+for+computer-aided+detection:+CNN+architectures,+dataset+characteristics+and+transfer+learning&amp;author=H.-C.+Shin&amp;author=H.+R.+Roth&amp;author=M.+Gao&amp;volume=35&amp;issue=5&amp;publication_year=2016&amp;pages=1285-1298&amp;pmid=26886976&amp;doi=10.1109/tmi.2016.2528162&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B17">17. <span class="element-citation">Ahn E., Kumar A., Kim J., Li C., Feng D., Fulham M. X-ray image classification using domain transferred convolutional neural networks and local sparse spatial pyramid. Proceedings of 2016 IEEE International Symposium on Biomedical Imaging (ISBI); April 2016; Prague, Czech Republic. IEEE; pp. 855–858. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Ahn+E.+Kumar+A.+Kim+J.+Li+C.+Feng+D.+Fulham+M.+X-ray+image+classification+using+domain+transferred+convolutional+neural+networks+and+local+sparse+spatial+pyramid+Proceedings+of+2016+IEEE+International+Symposium+on+Biomedical+Imaging+(ISBI)+April+2016+Prague,+Czech+Republic+IEEE+855+858+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B18">18. <span class="element-citation">Ouyang W., Luo P., Zeng X., et al.  DeepID-net: multi-stage and deformable deep convolutional neural networks for object detection.   <a href="http://arxiv.org/abs/1409.3505" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">http://arxiv.org/abs/1409.3505</a>. [<a href="https://www.ncbi.nlm.nih.gov/pubmed/27392342" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>]</span></div><div class="ref-cit-blk half_rhythm" id="B19">19. <span class="element-citation">Fernando B., Fromont E., Muselet D., Sebban M. Discriminative feature fusion for image classification. Proceedings of 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); June 2012; Providence, RI, USA. IEEE; pp. 3434–3441. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Fernando+B.+Fromont+E.+Muselet+D.+Sebban+M.+Discriminative+feature+fusion+for+image+classification+Proceedings+of+2012+IEEE+Conference+on+Computer+Vision+and+Pattern+Recognition+(CVPR)+June+2012+Providence,+RI,+USA+IEEE+3434+3441+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B20">20. <span class="element-citation">Chan T., Jia K., Gao S., Lu J., Zeng Z., Ma Y. PCANet: A simple deep learning baseline for image classification? <span><span class="ref-journal"><em>IEEE Transactions on Image Processing</em>. </span>2015;<span class="ref-vol">24</span>(12):5017–5032. doi: 10.1109/tip.2015.2475625.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/26340772" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1109%2Ftip.2015.2475625" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IEEE+Transactions+on+Image+Processing&amp;title=PCANet:+A+simple+deep+learning+baseline+for+image+classification?&amp;author=T.+Chan&amp;author=K.+Jia&amp;author=S.+Gao&amp;author=J.+Lu&amp;author=Z.+Zeng&amp;volume=24&amp;issue=12&amp;publication_year=2015&amp;pages=5017-5032&amp;pmid=26340772&amp;doi=10.1109/tip.2015.2475625&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B21">21. <span class="element-citation">Zeng R., Wu J., Shao Z., et al.  Color image classification via quaternion principal component analysis network. <span><span class="ref-journal"><em>Neurocomputing</em>. </span>2016;<span class="ref-vol">216</span>:416–428. doi: 10.1016/j.neucom.2016.08.006.</span> [<a href="https://dx.doi.org/10.1016%2Fj.neucom.2016.08.006" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Neurocomputing&amp;title=Color+image+classification+via+quaternion+principal+component+analysis+network&amp;author=R.+Zeng&amp;author=J.+Wu&amp;author=Z.+Shao&amp;volume=216&amp;publication_year=2016&amp;pages=416-428&amp;doi=10.1016/j.neucom.2016.08.006&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B22">22. <span class="element-citation">Rakotomamonjy A., Petitjean C., Salaun M., Thiberville L. Scattering features for lung cancer detection in fibered confocal fluorescence microscopy images. <span><span class="ref-journal"><em>Artificial Intelligence in Medicine</em>. </span>2014;<span class="ref-vol">61</span>(2):105–118. doi: 10.1016/j.artmed.2014.05.003.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/24877616" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1016%2Fj.artmed.2014.05.003" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Artificial+Intelligence+in+Medicine&amp;title=Scattering+features+for+lung+cancer+detection+in+fibered+confocal+fluorescence+microscopy+images&amp;author=A.+Rakotomamonjy&amp;author=C.+Petitjean&amp;author=M.+Salaun&amp;author=L.+Thiberville&amp;volume=61&amp;issue=2&amp;publication_year=2014&amp;pages=105-118&amp;pmid=24877616&amp;doi=10.1016/j.artmed.2014.05.003&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B23">23. <span class="element-citation">Bruna J., Mallat S. Invariant scattering convolution networks. <span><span class="ref-journal"><em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>. </span>2013;<span class="ref-vol">35</span>(8):1872–1886. doi: 10.1109/tpami.2012.230.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/23787341" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1109%2Ftpami.2012.230" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&amp;title=Invariant+scattering+convolution+networks&amp;author=J.+Bruna&amp;author=S.+Mallat&amp;volume=35&amp;issue=8&amp;publication_year=2013&amp;pages=1872-1886&amp;pmid=23787341&amp;doi=10.1109/tpami.2012.230&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B24">24. <span class="element-citation">Cruzroa A., Basavanhally A., Gonzalez F. A., et al.  Automatic detection of invasive ductal carcinoma in whole slide images with convolutional neural networks. Proceedings of SPIE; May 2014; Houston, TX, USA. p. p. 904103. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Cruzroa+A.+Basavanhally+A.+Gonzalez+F.+A.+Automatic+detection+of+invasive+ductal+carcinoma+in+whole+slide+images+with+convolutional+neural+networks+Proceedings+of+SPIE+May+2014+Houston,+TX,+USA+p.+904103+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B25">25. <span class="element-citation">Sun Y., Wang X., Tang X. Deep Learning face representation from predicting 10,000 classes. Proceeding of 2014 IEEE Conference on Computer Vision and Pattern Recognition; June 2014; Columbus, OH, USA. pp. 1891–1898. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Sun+Y.+Wang+X.+Tang+X.+Deep+Learning+face+representation+from+predicting+10,000+classes+Proceeding+of+2014+IEEE+Conference+on+Computer+Vision+and+Pattern+Recognition+June+2014+Columbus,+OH,+USA+1891+1898+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B26">26. <span class="element-citation">Li Z., Liu Y., Hayward R. F., Walker R. A. Color and texture feature fusion using kernel pca with application to object-based vegetation species classification. Proceedings of 2010 IEEE International Conference on Image Processing; September 2010; Hong Kong, China. pp. 2701–2704. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Li+Z.+Liu+Y.+Hayward+R.+F.+Walker+R.+A.+Color+and+texture+feature+fusion+using+kernel+pca+with+application+to+object-based+vegetation+species+classification+Proceedings+of+2010+IEEE+International+Conference+on+Image+Processing+September+2010+Hong+Kong,+China+2701+2704+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B27">27. <span class="element-citation">Lecun Y., Boser B., Denker J. S., et al.  Backpropagation applied to handwritten zip code recognition. <span><span class="ref-journal"><em>Neural Computation</em>. </span>1989;<span class="ref-vol">1</span>(4):541–551. doi: 10.1162/neco.1989.1.4.541.</span> [<a href="https://dx.doi.org/10.1162%2Fneco.1989.1.4.541" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Neural+Computation&amp;title=Backpropagation+applied+to+handwritten+zip+code+recognition&amp;author=Y.+Lecun&amp;author=B.+Boser&amp;author=J.+S.+Denker&amp;volume=1&amp;issue=4&amp;publication_year=1989&amp;pages=541-551&amp;doi=10.1162/neco.1989.1.4.541&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B28">28. <span class="element-citation">Nair V., Hinton G. E. Rectified linear units improve restricted boltzmann machines. Proceedings of International Conference on Machine Learning; June 2010; Haifa, Israel. pp. 807–814. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Nair+V.+Hinton+G.+E.+Rectified+linear+units+improve+restricted+boltzmann+machines+Proceedings+of+International+Conference+on+Machine+Learning+June+2010+Haifa,+Israel+807+814+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B29">29. <span class="element-citation">Mishkin D., Matas J. All you need is a good init.   <a href="http://arxiv.org/abs/1511.06422" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">http://arxiv.org/abs/1511.06422</a>.</span></div><div class="ref-cit-blk half_rhythm" id="B30">30. <span class="element-citation">Szegedy C., Liu W., Jia Y., et al.  Going deeper with convolutions. Proceedings of 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); June 2015; Boston, MA, USA. pp. 1–9. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Szegedy+C.+Liu+W.+Jia+Y.+Going+deeper+with+convolutions+Proceedings+of+2015+IEEE+Conference+on+Computer+Vision+and+Pattern+Recognition+(CVPR)+June+2015+Boston,+MA,+USA+1+9+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B31">31. <span class="element-citation">Stanley R. J., Stoecker W. V., Moss R. H. A relative color approach to color discrimination for malignant melanoma detection in dermoscopy images. <span><span class="ref-journal"><em>Skin Research and Technology</em>. </span>2007;<span class="ref-vol">13</span>(1):62–72. doi: 10.1111/j.1600-0846.2007.00192.x.</span> <span class="nowrap">[<a class="int-reflink" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3184887/">PMC free article</a>]</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/17250534" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1111%2Fj.1600-0846.2007.00192.x" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Skin+Research+and+Technology&amp;title=A+relative+color+approach+to+color+discrimination+for+malignant+melanoma+detection+in+dermoscopy+images&amp;author=R.+J.+Stanley&amp;author=W.+V.+Stoecker&amp;author=R.+H.+Moss&amp;volume=13&amp;issue=1&amp;publication_year=2007&amp;pages=62-72&amp;pmid=17250534&amp;doi=10.1111/j.1600-0846.2007.00192.x&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B32">32. <span class="element-citation">Celebi M. E., Kingravi H. A., Uddin B., et al.  A methodological approach to the classification of dermoscopy images. <span><span class="ref-journal"><em>Computerized Medical Imaging and Graphics</em>. </span>2007;<span class="ref-vol">31</span>(6):362–373. doi: 10.1016/j.compmedimag.2007.01.003.</span> <span class="nowrap">[<a class="int-reflink" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3192405/">PMC free article</a>]</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/17387001" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1016%2Fj.compmedimag.2007.01.003" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Computerized+Medical+Imaging+and+Graphics&amp;title=A+methodological+approach+to+the+classification+of+dermoscopy+images&amp;author=M.+E.+Celebi&amp;author=H.+A.+Kingravi&amp;author=B.+Uddin&amp;volume=31&amp;issue=6&amp;publication_year=2007&amp;pages=362-373&amp;pmid=17387001&amp;doi=10.1016/j.compmedimag.2007.01.003&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B33">33. <span class="element-citation">Rubegni P., Cevenini G., Burroni M., et al.  Automated diagnosis of pigmented skin lesions. <span><span class="ref-journal"><em>International Journal of Cancer</em>. </span>2002;<span class="ref-vol">101</span>(6):576–580. doi: 10.1002/ijc.10620.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/12237900" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1002%2Fijc.10620" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=International+Journal+of+Cancer&amp;title=Automated+diagnosis+of+pigmented+skin+lesions&amp;author=P.+Rubegni&amp;author=G.+Cevenini&amp;author=M.+Burroni&amp;volume=101&amp;issue=6&amp;publication_year=2002&amp;pages=576-580&amp;pmid=12237900&amp;doi=10.1002/ijc.10620&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B34">34. <span class="element-citation">Ramteke R., Khachane M. Automatic medical image classification and abnormality detection using k-nearest neighbour. <span><span class="ref-journal"><em>International Journal of Advanced Computer Reasearch</em>. </span>2012;<span class="ref-vol">2</span>(4):190–196.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=International+Journal+of+Advanced+Computer+Reasearch&amp;title=Automatic+medical+image+classification+and+abnormality+detection+using+k-nearest+neighbour&amp;author=R.+Ramteke&amp;author=M.+Khachane&amp;volume=2&amp;issue=4&amp;publication_year=2012&amp;pages=190-196&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B35">35. <span class="element-citation">Huang J., Ling C. X. Using auc and accuracy in evaluating learning algorithms. <span><span class="ref-journal"><em>IEEE Transactions on knowledge and Data Engineering</em>. </span>2005;<span class="ref-vol">17</span>(3):299–310. doi: 10.1109/tkde.2005.50.</span> [<a href="https://dx.doi.org/10.1109%2Ftkde.2005.50" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IEEE+Transactions+on+knowledge+and+Data+Engineering&amp;title=Using+auc+and+accuracy+in+evaluating+learning+algorithms&amp;author=J.+Huang&amp;author=C.+X.+Ling&amp;volume=17&amp;issue=3&amp;publication_year=2005&amp;pages=299-310&amp;doi=10.1109/tkde.2005.50&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B36">36. <span class="element-citation">Guo Y., Yao A., Chen Y.  <span class="ref-journal"><em>Advances in Neural Information Processing Systems</em>.</span> Cambridge, MA, USA: MIT Press; 2016. Dynamic network surgery for efficient DNNs; pp. 1379–1387. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?title=Advances+in+Neural+Information+Processing+Systems&amp;author=Y.+Guo&amp;author=A.+Yao&amp;author=Y.+Chen&amp;publication_year=2016&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B37">37. <span class="element-citation">Han S., Pool J., Tran J., Dally W. J. Learning both weights and connections for efficient neural networks. 2015. pp. 1135–1143.  <a href="https://arxiv.org/abs/1506.02626" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">https://arxiv.org/abs/1506.02626</a>.</span></div><div class="ref-cit-blk half_rhythm" id="B38">38. <span class="element-citation">Lin M., Chen Q., Yan S. Network in network. Proceedings of International Conference on Learning Representations; April 2014; Banff, Canada.  <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Lin+M.+Chen+Q.+Yan+S.+Network+in+network+Proceedings+of+International+Conference+on+Learning+Representations+April+2014+Banff,+Canada+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B39">39. <span class="element-citation">Ma G., Yang X., Zhang B., Shi Z. Multi-feature fusion deep networks. <span><span class="ref-journal"><em>Neurocomputing</em>. </span>2016;<span class="ref-vol">218</span>:164–171. doi: 10.1016/j.neucom.2016.08.059.</span> [<a href="https://dx.doi.org/10.1016%2Fj.neucom.2016.08.059" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Neurocomputing&amp;title=Multi-feature+fusion+deep+networks&amp;author=G.+Ma&amp;author=X.+Yang&amp;author=B.+Zhang&amp;author=Z.+Shi&amp;volume=218&amp;publication_year=2016&amp;pages=164-171&amp;doi=10.1016/j.neucom.2016.08.059&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B40">40. <span class="element-citation">Tiwari P., Viswanath S., Lee G., Madabhushi A. Multi-modal data fusion schemes for integrated classification of imaging and non-imaging biomedical data. Proceedings of 2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro; March 2011; Chicago, Illinois, USA. pp. 165–168. <span class="nowrap">[<a class="int-reflink" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4335721/">PMC free article</a>]</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/25705325" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Tiwari+P.+Viswanath+S.+Lee+G.+Madabhushi+A.+Multi-modal+data+fusion+schemes+for+integrated+classification+of+imaging+and+non-imaging+biomedical+data+Proceedings+of+2011+IEEE+International+Symposium+on+Biomedical+Imaging:+From+Nano+to+Macro+March+2011+Chicago,+Illinois,+USA+165+168+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span></div></div></div><div style="display: none; width: 200px; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true" class="ui-helper-reset ui-ncbipopper-wrapper ui-ncbilinksmenu"><ul id="ui-ncbiinpagenav-2"><li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#ui-ncbiinpagenav-heading-3">Associated Data</a></li><li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#idm140658773571104title">Abstract</a></li><li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#sec1title">1. Introduction</a></li><li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#sec2title">2. Related Work</a></li><li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#sec3title">3. Methodology of the Proposed Model</a></li><li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#sec4title">4. Experiment and Evaluation</a></li><li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#sec5title">5. Conclusions and Summary</a></li><li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#idm140658777758400title">Data Availability</a></li><li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#idm140658774320688title">Conflicts of Interest</a></li><li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#idm140658774320048title">References</a></li></ul></div></div><!--post-content--><div class="courtesy-note whole_rhythm small"><hr>Articles from <span class="acknowledgment-journal-title">Computational Intelligence and Neuroscience</span> are provided here courtesy of <strong>Hindawi Limited</strong></div></div>
            
            
        
            
        </div>
        <!-- Book content -->
    </div>
    
    <div id="rightcolumn" class="four_col col last">
        <!-- Custom content above discovery portlets -->
        <div class="col6">
            
        </div>
        
        <div xmlns:np="http://ncbi.gov/portal/XSLT/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><div class="format-menu"><h2>Formats:</h2><ul><li class="selected">Article</li> | <li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/?report=reader">PubReader</a></li> | <li class="epub-link"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/epub/">ePub (beta)</a></li> | <li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/pdf/CIN2018-2061516.pdf">PDF (1.4M)</a></li> | <li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" data-citationid="PMC6157177" class="citationexporter ctxp" role="button" aria-expanded="false" aria-haspopup="true">Citation</a></li></ul></div></div><div xmlns:np="http://ncbi.gov/portal/XSLT/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="share-buttons"><h2>Share</h2><ul><li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6157177%2F" role="button" aria-expanded="false" aria-haspopup="true" target="_blank"><img src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/4047626" alt="Share on Facebook">
                             Facebook
                        </a></li><li class="twitter"><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6157177%2F&amp;text=Medical%20Image%20Classification%20Based%20on%20Deep%20Features%20Extracted%20by%20Deep%20Model%20and%20Statistic%20Feature%20Fusion%20with%20Multilayer%20Perceptron%E2%80%AC" role="button" aria-expanded="false" aria-haspopup="true" target="_blank"><img src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/4047627" alt="Share on Twitter">
                             Twitter
                        </a></li><li class="gplus"><a href="https://plus.google.com/share?url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6157177%2F" role="button" aria-expanded="false" aria-haspopup="true" target="_blank"><img src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/4047628" alt="Share on Google Plus">
                             Google+
                        </a></li></ul></div>
        
        <div id="ajax-portlets" data-pmid="30298088" data-aiid="6157177" data-aid="6157177" data-iid="306880" data-domainid="526" data-domain="cin" data-accid="PMC6157177" data-md5="dc79f16cb812224a44ab18f3223d99c5">
<div class="portlet pubmed_favoritesad">
<div class="portlet_head">
<div class="portlet_title"><h3><span>Save items</span></h3></div>
<a name="Shutter" sid="1" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" class="portlet_shutter ui-ncbitoggler-open" title="Show/hide content" remembercollapsed="true" pgsec_name="pmfavad" toggles="ui-portlet_content-5" role="button" aria-expanded="true" style="position: absolute; padding: 0px;"></a>
</div>
<div class="ui-helper-reset" aria-live="assertive"><div class="portlet_content ui-ncbitoggler ui-ncbitoggler-slave-open" id="ui-portlet_content-5" aria-hidden="false"><div xmlns:np="http://ncbi.gov/portal/XSLT/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
<input type="hidden" name="absid" id="absid" value="6157177"><div id="pubmed_favoritesad" class="empty" data-db="pmc">
<span role="menubar"><div class="ui-ncbisetswitch ltd-hover" role="menuitem" aria-expanded="false" aria-haspopup="true"><a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/" class="jig-ncbisetswitch blind collink ltd-hover ui-ncbisetswitch-hasStar" id="favList" data-jigconfig="destSelector:&#39;#favUL&#39;,starExists:true,createCollectionUrl:&#39;#&#39;,manageCollectionsUrl:&#39;/myncbi/collections/&#39;"><span class="star"></span>Add to Favorites</a><a class="ui-ncbisetswitch-button ltd-hover" title="View more options" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#"><span class="ui-icon ui-icon-triangle-1-s ui-ncbisetswitch-down">View more options</span></a></div></span><div style="display: none; width: 200px; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true" class="ui-helper-reset ui-ncbipopper-wrapper ui-ncbilinksmenu ui-ncbisetswitch-popper"><ul id="favUL" class="ui-ncbisetswitch-options"></ul><ul class="ui-ncbisetswitch-actions"><li class="ui-ncbisetswitch-actions-create" style="display: block;"><a class="ui-ncbisetswitch-create-collection" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#">Create collection...</a></li><li class="ui-ncbisetswitch-actions-manage" style="display: block;"><a class="ui-ncbisetswitch-manage-collections" href="https://www.ncbi.nlm.nih.gov/myncbi/collections/">Manage collections...</a></li></ul></div>
</div>
<div class="colloading">loading</div>
<span id="submenu_AddToCollections"><form method="post"><input type="submit" class="button_apply" name="addtocollection" value="1"></form></span>
</div></div></div>
</div><div class="portlet">
<div class="portlet_head">
<div class="portlet_title"><h3><span>Similar articles in PubMed</span></h3></div>
<a name="Shutter" sid="1" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" class="portlet_shutter ui-ncbitoggler-open" title="Show/hide content" remembercollapsed="true" pgsec_name="PBooksDiscovery_RA" toggles="ui-portlet_content-6" role="button" aria-expanded="true" style="position: absolute; padding: 0px;"></a>
</div>
<div class="ui-helper-reset" aria-live="assertive"><div class="portlet_content ui-ncbitoggler ui-ncbitoggler-slave-open" id="ui-portlet_content-6" aria-hidden="false">
<ul>
<li class="brieflinkpopper two_line">
<a class="brieflinkpopperctrl" href="https://www.ncbi.nlm.nih.gov/pubmed/28254085" ref="reftype=relart&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|RelatedArticles&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal" role="button" aria-expanded="false" aria-haspopup="true">A novel end-to-end classifier using domain transferred deep convolutional neural networks for biomedical images.</a><span class="source">[Comput Methods Programs Biomed...]</span><div class="brieflinkpop ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" id="ui-brieflinkpop-7" aria-live="assertive" aria-hidden="true" style="top: -100px; left: -100px; display: none;">
<em xmlns:np="http://ncbi.gov/portal/XSLT/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="author">Pang S, Yu Z, Orgun MA. </em><em xmlns:np="http://ncbi.gov/portal/XSLT/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="cit">Comput Methods Programs Biomed. 2017 Mar; 140:283-293. Epub 2017 Jan 6.</em>
</div>
</li>
<li class="brieflinkpopper two_line">
<a class="brieflinkpopperctrl" href="https://www.ncbi.nlm.nih.gov/pubmed/29544790" ref="reftype=relart&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|RelatedArticles&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal" role="button" aria-expanded="false" aria-haspopup="true">A novel biomedical image indexing and retrieval system via deep preference learning.</a><span class="source">[Comput Methods Programs Biomed...]</span><div class="brieflinkpop ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" id="ui-brieflinkpop-8" aria-live="assertive" aria-hidden="true" style="top: -100px; left: -100px; display: none;">
<em xmlns:np="http://ncbi.gov/portal/XSLT/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="author">Pang S, Orgun MA, Yu Z. </em><em xmlns:np="http://ncbi.gov/portal/XSLT/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="cit">Comput Methods Programs Biomed. 2018 May; 158:53-69. Epub 2018 Feb 6.</em>
</div>
</li>
<li class="brieflinkpopper two_line">
<a class="brieflinkpopperctrl" href="https://www.ncbi.nlm.nih.gov/pubmed/30298337" ref="reftype=relart&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|RelatedArticles&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal" role="button" aria-expanded="false" aria-haspopup="true">Medical Image Analysis using Convolutional Neural Networks: A Review.</a><span class="source">[J Med Syst. 2018]</span><div class="brieflinkpop ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" id="ui-brieflinkpop-9" aria-live="assertive" aria-hidden="true" style="top: -100px; left: -100px; display: none;">
<em xmlns:np="http://ncbi.gov/portal/XSLT/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="author">Anwar SM, Majid M, Qayyum A, Awais M, Alnowami M, Khan MK. </em><em xmlns:np="http://ncbi.gov/portal/XSLT/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="cit">J Med Syst. 2018 Oct 8; 42(11):226. Epub 2018 Oct 8.</em>
</div>
</li>
<li class="brieflinkpopper two_line">
<a class="brieflinkpopperctrl" href="https://www.ncbi.nlm.nih.gov/pubmed/29581722" ref="reftype=relart&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|RelatedArticles&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal" role="button" aria-expanded="false" aria-haspopup="true">A Two-Stream Deep Fusion Framework for High-Resolution Aerial Scene Classification.</a><span class="source">[Comput Intell Neurosci. 2018]</span><div class="brieflinkpop ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" id="ui-brieflinkpop-10" aria-live="assertive" aria-hidden="true" style="top: -100px; left: -100px; display: none;">
<em xmlns:np="http://ncbi.gov/portal/XSLT/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="author">Yu Y, Liu F. </em><em xmlns:np="http://ncbi.gov/portal/XSLT/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="cit">Comput Intell Neurosci. 2018; 2018:8639367. Epub 2018 Jan 18.</em>
</div>
</li>
<li class="brieflinkpopper two_line">
<a class="brieflinkpopperctrl" href="https://www.ncbi.nlm.nih.gov/pubmed/28861708" ref="reftype=relart&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|RelatedArticles&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal" role="button" aria-expanded="false" aria-haspopup="true">Breast cancer cell nuclei classification in histopathology images using deep neural networks.</a><span class="source">[Int J Comput Assist Radiol Sur...]</span><div class="brieflinkpop ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" id="ui-brieflinkpop-11" aria-live="assertive" aria-hidden="true" style="top: -100px; left: -100px; display: none;">
<em xmlns:np="http://ncbi.gov/portal/XSLT/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="author">Feng Y, Zhang L, Yi Z. </em><em xmlns:np="http://ncbi.gov/portal/XSLT/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="cit">Int J Comput Assist Radiol Surg. 2018 Feb; 13(2):179-191. Epub 2017 Aug 31.</em>
</div>
</li>
</ul>
<a class="seemore" href="https://www.ncbi.nlm.nih.gov/sites/entrez?db=pubmed&amp;cmd=link&amp;linkname=pubmed_pubmed_reviews&amp;uid=30298088&amp;log%24=relatedreviews&amp;logdbfrom=pmc" ref="reftype=relart&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|RelatedArticles&amp;TO=Entrez|Pubmed|Reviews&amp;rendering-type=normal">See reviews...</a><a class="seemore" href="https://www.ncbi.nlm.nih.gov/sites/entrez?db=pubmed&amp;cmd=link&amp;linkname=pubmed_pubmed&amp;uid=30298088&amp;log%24=relatedarticles&amp;logdbfrom=pmc" ref="reftype=relart&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|RelatedArticles&amp;TO=Entrez|Pubmed|Related%20Records&amp;rendering-type=normal">See all...</a>
</div></div>
</div><div class="portlet">
<div class="portlet_head">
<div class="portlet_title"><h3><span>Links</span></h3></div>
<a name="Shutter" sid="1" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" class="portlet_shutter ui-ncbitoggler-open" title="Show/hide content" remembercollapsed="true" pgsec_name="PMCVCitedByPmcArticlesP" toggles="ui-portlet_content-12" role="button" aria-expanded="true" style="position: absolute; padding: 0px;"></a>
</div>
<div class="ui-helper-reset" aria-live="assertive"><div class="portlet_content ui-ncbitoggler ui-ncbitoggler-slave-open" id="ui-portlet_content-12" aria-hidden="false"><ul>
<li class="brieflinkpopper">
<a class="brieflinkpopperctrl" href="https://www.ncbi.nlm.nih.gov/pmc/?Db=medgen&amp;DbFrom=pmc&amp;Cmd=Link&amp;LinkName=pmc_medgen&amp;IdsFromResult=6157177" ref="reftype=MedGen&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|DiscoveryLinks&amp;TO=Entrez|Crosslink|Unknown%20Link%20Name&amp;rendering-type=normal" role="button" aria-expanded="false" aria-haspopup="true">MedGen</a><div class="brieflinkpop ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" id="ui-brieflinkpop-13" aria-live="assertive" aria-hidden="true" style="top: -100px; left: -100px; display: none;">MedGen<div class="brieflinkpopdesc">Related information in MedGen</div>
</div>
</li>
<li class="brieflinkpopper">
<a class="brieflinkpopperctrl" href="https://www.ncbi.nlm.nih.gov/pubmed/30298088/" ref="reftype=PubMed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|DiscoveryLinks&amp;TO=Entrez|PubMed|Record&amp;rendering-type=normal" role="button" aria-expanded="false" aria-haspopup="true">PubMed</a><div class="brieflinkpop ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" id="ui-brieflinkpop-14" aria-live="assertive" aria-hidden="true" style="top: -100px; left: -100px; display: none;">PubMed<div class="brieflinkpopdesc">PubMed citations for these articles</div>
</div>
</li>
<li class="brieflinkpopper">
<a class="brieflinkpopperctrl" href="https://www.ncbi.nlm.nih.gov/pmc/?Db=taxonomy&amp;DbFrom=pmc&amp;Cmd=Link&amp;LinkName=pmc_taxonomy&amp;IdsFromResult=6157177" ref="reftype=Taxonomy&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|DiscoveryLinks&amp;TO=Entrez|Crosslink|Taxonomy&amp;rendering-type=normal" role="button" aria-expanded="false" aria-haspopup="true">Taxonomy</a><div class="brieflinkpop ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" id="ui-brieflinkpop-15" aria-live="assertive" aria-hidden="true" style="top: -100px; left: -100px; display: none;">Taxonomy<div class="brieflinkpopdesc">Related taxonomy entry</div>
</div>
</li>
</ul></div></div>
</div><div class="portlet">
<div class="portlet_head">
<div class="portlet_title"><h3><span>Recent Activity</span></h3></div>
<a name="Shutter" sid="1" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" class="portlet_shutter ui-ncbitoggler-open" title="Show/hide content" remembercollapsed="true" pgsec_name="recent_activity" toggles="ui-portlet_content-16" role="button" aria-expanded="true" style="position: absolute; padding: 0px;"></a>
</div>
<div class="ui-helper-reset" aria-live="assertive"><div class="portlet_content ui-ncbitoggler ui-ncbitoggler-slave-open" id="ui-portlet_content-16" aria-hidden="false"><div xmlns:np="http://ncbi.gov/portal/XSLT/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" id="HTDisplay" class="">
<div class="action">
<a href="javascript:historyDisplayState(&#39;ClearHT&#39;)">Clear</a><a href="javascript:historyDisplayState(&#39;HTOff&#39;)" class="HTOn">Turn Off</a><a href="javascript:historyDisplayState(&#39;HTOn&#39;)" class="HTOff">Turn On</a>
</div>
<ul id="activity"><li class="ra_rcd ralinkpopper two_line">
<a class="htb ralinkpopperctrl" ref="log$=activity&amp;linkpos=1" href="https://www.ncbi.nlm.nih.gov/portal/utils/pageresolver.fcgi?recordid=5ce941e6dde08956e488370c">Medical Image Classification Based on Deep Features Extracted by Deep Model and ...</a><div class="ralinkpop offscreen_noflow">Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron‬<div class="brieflinkpopdesc">Computational Intelligence and Neuroscience. 2018; 2018()</div>
</div>
<div class="tertiary"></div>
</li></ul>
<p class="HTOn">Your browsing activity is empty.</p>
<p class="HTOff">Activity recording is turned off.</p>
<p id="turnOn" class="HTOff"><a href="javascript:historyDisplayState(&#39;HTOn&#39;)">Turn recording back on</a></p>
<a class="seemore" href="https://www.ncbi.nlm.nih.gov/sites/myncbi/recentactivity">See more...</a>
</div></div></div>
</div>
<div>
<div class="portlet brieflink pmc_para_cit" id="crb--__p3" name="crb--__p3" rid="__p3" style="display: none;">
<div class="portlet_head"></div>
<div class="portlet_content" id="ui-portlet_content-17"><ul>
<li class="expanded" reference_id="B1">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/21664806/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">Visual pattern mining in histology image collections using bag of features.</a><span class="one_line_source">[Artif Intell Med.  2011]</span><div class="alt-note">
<div class="authors">Cruz-Roa A, Caicedo JC, González FA</div>
<div class="citation">Artif Intell Med. 2011 Jun; 52(2):91-106.</div>
</div>
</li>
<li class="expanded" reference_id="B2">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/28117445/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">Dermatologist-level classification of skin cancer with deep neural networks.</a><span class="one_line_source">[Nature.  2017]</span><div class="alt-note">
<div class="authors">Esteva A, Kuprel B, Novoa RA, Ko J, Swetter SM, Blau HM, Thrun S</div>
<div class="citation">Nature. 2017 Feb 2; 542(7639):115-118.</div>
</div>
</li>
<li class="expanded" reference_id="B4">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/16764513/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">A fast learning algorithm for deep belief nets.</a><span class="one_line_source">[Neural Comput.  2006]</span><div class="alt-note">
<div class="authors">Hinton GE, Osindero S, Teh YW</div>
<div class="citation">Neural Comput. 2006 Jul; 18(7):1527-54.</div>
</div>
</li>
</ul></div>
</div>
<div class="portlet brieflink pmc_para_cit" id="crb--__p5" name="crb--__p5" rid="__p5" style="display: none;">
<div class="portlet_head"></div>
<div class="portlet_content" id="ui-portlet_content-18"><ul><li class="expanded" reference_id="B16">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/26886976/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning.</a><span class="one_line_source">[IEEE Trans Med Imaging.  2016]</span><div class="alt-note">
<div class="authors">Shin HC, Roth HR, Gao M, Lu L, Xu Z, Nogues I, Yao J, Mollura D, Summers RM</div>
<div class="citation">IEEE Trans Med Imaging. 2016 May; 35(5):1285-98.</div>
</div>
</li></ul></div>
</div>
<div class="portlet brieflink pmc_para_cit" id="crb--__p14" name="crb--__p14" rid="__p14" style="display: none;">
<div class="portlet_head"></div>
<div class="portlet_content" id="ui-portlet_content-19"><ul>
<li class="expanded" reference_id="B10">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/26513811/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">Content-Adaptive Region-Based Color Texture Descriptors for Medical Images.</a><span class="one_line_source">[IEEE J Biomed Health Inform.  2017]</span><div class="alt-note">
<div class="authors">Riaz F, Hassan A, Nisar R, Dinis-Ribeiro M, Coimbra MT</div>
<div class="citation">IEEE J Biomed Health Inform. 2017 Jan; 21(1):162-171.</div>
</div>
</li>
<li class="expanded" reference_id="B11">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/20117177/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">Computer aided diagnosis system for the Alzheimer's disease based on partial least squares and random forest SPECT image classification.</a><span class="one_line_source">[Neurosci Lett.  2010]</span><div class="alt-note">
<div class="authors">Ramírez J, Górriz JM, Segovia F, Chaves R, Salas-Gonzalez D, López M, Alvarez I, Padilla P</div>
<div class="citation">Neurosci Lett. 2010 Mar 19; 472(2):99-103.</div>
</div>
</li>
<li class="expanded" reference_id="B16">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/26886976/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning.</a><span class="one_line_source">[IEEE Trans Med Imaging.  2016]</span><div class="alt-note">
<div class="authors">Shin HC, Roth HR, Gao M, Lu L, Xu Z, Nogues I, Yao J, Mollura D, Summers RM</div>
<div class="citation">IEEE Trans Med Imaging. 2016 May; 35(5):1285-98.</div>
</div>
</li>
</ul></div>
</div>
<div class="portlet brieflink pmc_para_cit" id="crb--__p15" name="crb--__p15" rid="__p15" style="display: none;">
<div class="portlet_head"></div>
<div class="portlet_content" id="ui-portlet_content-20"><ul>
<li class="expanded" reference_id="B8">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/18703311/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">An improved Internet-based melanoma screening system with dermatologist-like tumor area extraction algorithm.</a><span class="one_line_source">[Comput Med Imaging Graph.  2008]</span><div class="alt-note">
<div class="authors">Iyatomi H, Oka H, Celebi ME, Hashimoto M, Hagiwara M, Tanaka M, Ogawa K</div>
<div class="citation">Comput Med Imaging Graph. 2008 Oct; 32(7):566-79.</div>
</div>
</li>
<li class="expanded" reference_id="B9">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/21036538/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">Detection of granularity in dermoscopy images of malignant melanoma using color and texture features.</a><span class="one_line_source">[Comput Med Imaging Graph.  2011]</span><div class="alt-note">
<div class="authors">Stoecker WV, Wronkiewiecz M, Chowdhury R, Stanley RJ, Xu J, Bangert A, Shrestha B, Calcara DA, Rabinovitz HS, Oliviero M, Ahmed F, Perry LA, Drugge R</div>
<div class="citation">Comput Med Imaging Graph. 2011 Mar; 35(2):144-7.</div>
</div>
</li>
<li class="expanded" reference_id="B10">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/26513811/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">Content-Adaptive Region-Based Color Texture Descriptors for Medical Images.</a><span class="one_line_source">[IEEE J Biomed Health Inform.  2017]</span><div class="alt-note">
<div class="authors">Riaz F, Hassan A, Nisar R, Dinis-Ribeiro M, Coimbra MT</div>
<div class="citation">IEEE J Biomed Health Inform. 2017 Jan; 21(1):162-171.</div>
</div>
</li>
</ul></div>
</div>
<div class="portlet brieflink pmc_para_cit" id="crb--__p16" name="crb--__p16" rid="__p16" style="display: none;">
<div class="portlet_head"></div>
<div class="portlet_content" id="ui-portlet_content-21"><ul><li class="expanded" reference_id="B11">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/20117177/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">Computer aided diagnosis system for the Alzheimer's disease based on partial least squares and random forest SPECT image classification.</a><span class="one_line_source">[Neurosci Lett.  2010]</span><div class="alt-note">
<div class="authors">Ramírez J, Górriz JM, Segovia F, Chaves R, Salas-Gonzalez D, López M, Alvarez I, Padilla P</div>
<div class="citation">Neurosci Lett. 2010 Mar 19; 472(2):99-103.</div>
</div>
</li></ul></div>
</div>
<div class="portlet brieflink pmc_para_cit" id="crb--__p18" name="crb--__p18" rid="__p18" style="display: none;">
<div class="portlet_head"></div>
<div class="portlet_content" id="ui-portlet_content-22"><ul>
<li class="expanded" reference_id="B20">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/26340772/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">PCANet: A Simple Deep Learning Baseline for Image Classification?</a><span class="one_line_source">[IEEE Trans Image Process.  2015]</span><div class="alt-note">
<div class="authors">Chan TH, Jia K, Gao S, Lu J, Zeng Z, Ma Y</div>
<div class="citation">IEEE Trans Image Process. 2015 Dec; 24(12):5017-32.</div>
</div>
</li>
<li class="expanded" reference_id="B16">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/26886976/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning.</a><span class="one_line_source">[IEEE Trans Med Imaging.  2016]</span><div class="alt-note">
<div class="authors">Shin HC, Roth HR, Gao M, Lu L, Xu Z, Nogues I, Yao J, Mollura D, Summers RM</div>
<div class="citation">IEEE Trans Med Imaging. 2016 May; 35(5):1285-98.</div>
</div>
</li>
<li class="expanded" reference_id="B22">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/24877616/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">Scattering features for lung cancer detection in fibered confocal fluorescence microscopy images.</a><span class="one_line_source">[Artif Intell Med.  2014]</span><div class="alt-note">
<div class="authors">Rakotomamonjy A, Petitjean C, Salaün M, Thiberville L</div>
<div class="citation">Artif Intell Med. 2014 Jun; 61(2):105-18.</div>
</div>
</li>
<li class="expanded" reference_id="B23">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/23787341/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">Invariant scattering convolution networks.</a><span class="one_line_source">[IEEE Trans Pattern Anal Mach Intell.  2013]</span><div class="alt-note">
<div class="authors">Bruna J, Mallat S</div>
<div class="citation">IEEE Trans Pattern Anal Mach Intell. 2013 Aug; 35(8):1872-86.</div>
</div>
</li>
</ul></div>
</div>
<div class="portlet brieflink pmc_para_cit" id="crb--__p26" name="crb--__p26" rid="__p26" style="display: none;">
<div class="portlet_head"></div>
<div class="portlet_content" id="ui-portlet_content-23"><ul>
<li class="expanded" reference_id="B8">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/18703311/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">An improved Internet-based melanoma screening system with dermatologist-like tumor area extraction algorithm.</a><span class="one_line_source">[Comput Med Imaging Graph.  2008]</span><div class="alt-note">
<div class="authors">Iyatomi H, Oka H, Celebi ME, Hashimoto M, Hagiwara M, Tanaka M, Ogawa K</div>
<div class="citation">Comput Med Imaging Graph. 2008 Oct; 32(7):566-79.</div>
</div>
</li>
<li class="expanded" reference_id="B31">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/17250534/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">A relative color approach to color discrimination for malignant melanoma detection in dermoscopy images.</a><span class="one_line_source">[Skin Res Technol.  2007]</span><div class="alt-note">
<div class="authors">Stanley RJ, Stoecker WV, Moss RH</div>
<div class="citation">Skin Res Technol. 2007 Feb; 13(1):62-72.</div>
</div>
</li>
<li class="expanded" reference_id="B32">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/17387001/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">A methodological approach to the classification of dermoscopy images.</a><span class="one_line_source">[Comput Med Imaging Graph.  2007]</span><div class="alt-note">
<div class="authors">Celebi ME, Kingravi HA, Uddin B, Iyatomi H, Aslandogan YA, Stoecker WV, Moss RH</div>
<div class="citation">Comput Med Imaging Graph. 2007 Sep; 31(6):362-73.</div>
</div>
</li>
<li class="expanded" reference_id="B33">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/12237900/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">Automated diagnosis of pigmented skin lesions.</a><span class="one_line_source">[Int J Cancer.  2002]</span><div class="alt-note">
<div class="authors">Rubegni P, Cevenini G, Burroni M, Perotti R, Dell'Eva G, Sbano P, Miracco C, Luzi P, Tosi P, Barbini P, Andreassi L</div>
<div class="citation">Int J Cancer. 2002 Oct 20; 101(6):576-80.</div>
</div>
</li>
</ul></div>
</div>
<div class="portlet brieflink pmc_para_cit" id="crb--__p47" name="crb--__p47" rid="__p47" style="display: none;">
<div class="portlet_head"></div>
<div class="portlet_content" id="ui-portlet_content-24"><ul><li class="expanded" reference_id="B40">
<a href="https://www.ncbi.nlm.nih.gov/pubmed/25705325/" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record&amp;rendering-type=normal">MULTI-MODAL DATA FUSION SCHEMES FOR INTEGRATED CLASSIFICATION OF IMAGING AND NON-IMAGING BIOMEDICAL DATA.</a><span class="one_line_source">[Proc IEEE Int Symp Biomed Imaging.  2011]</span><div class="alt-note">
<div class="authors">Tiwari P, Viswanath S, Lee G, Madabhushi A</div>
<div class="citation">Proc IEEE Int Symp Biomed Imaging. 2011 Mar-Apr; 2011():165-168.</div>
</div>
</li></ul></div>
</div>
</div>   
    </div>
                
        <!-- Custom content below discovery portlets -->
        <div class="col7">
            
        </div>
    </div>
</div>

<!-- Custom content after all -->
<div class="col8">
    
</div>
<div class="col9">
    
</div>

<script src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/jquery.scrollTo-1.4.2.js.download"></script>
<script>
    (function($){
        $('.skiplink').each(function(i, item){
            var href = $($(item).attr('href'));
            href.attr('tabindex', '-1').addClass('skiptarget'); // ensure the target can receive focus
            $(item).on('click', function(event){
                event.preventDefault();
                $.scrollTo(href, 0, {
                    onAfter: function(){
                        href.focus();
                    }
                });
            });
        });
    })(jQuery);
</script>



<div id="body-link-poppers"><div id="body-link-popper-B1" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">1. <span class="element-citation">Cruzroa A., Caicedo J. C., Gonzalez F. A. Visual pattern mining in histology image collections using bag of features. <span><span class="ref-journal"><em>Artificial Intelligence in Medicine</em>. </span>2011;<span class="ref-vol">52</span>(2):91–106. doi: 10.1016/j.artmed.2011.04.010.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/21664806" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1016%2Fj.artmed.2011.04.010" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Artificial+Intelligence+in+Medicine&amp;title=Visual+pattern+mining+in+histology+image+collections+using+bag+of+features&amp;author=A.+Cruzroa&amp;author=J.+C.+Caicedo&amp;author=F.+A.+Gonzalez&amp;volume=52&amp;issue=2&amp;publication_year=2011&amp;pages=91-106&amp;pmid=21664806&amp;doi=10.1016/j.artmed.2011.04.010&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B1">Ref list</a>]</div><div id="body-link-popper-B2" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">2. <span class="element-citation">Esteva A., Kuprel B., Novoa R. A., et al.  Dermatologist-level classification of skin cancer with deep neural networks. <span><span class="ref-journal"><em>Nature</em>. </span>2017;<span class="ref-vol">542</span>(7639):115–118. doi: 10.1038/nature21056.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/28117445" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1038%2Fnature21056" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Nature&amp;title=Dermatologist-level+classification+of+skin+cancer+with+deep+neural+networks&amp;author=A.+Esteva&amp;author=B.+Kuprel&amp;author=R.+A.+Novoa&amp;volume=542&amp;issue=7639&amp;publication_year=2017&amp;pages=115-118&amp;pmid=28117445&amp;doi=10.1038/nature21056&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B2">Ref list</a>]</div><div id="body-link-popper-B3" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">3. <span class="element-citation">Zare M. R., Mueen A., Awedh M., Seng W. C. Automatic classification of medical x-ray images: hybrid generative-discriminative approach. <span><span class="ref-journal"><em>IET Image Processing</em>. </span>2013;<span class="ref-vol">7</span>(5):523–532. doi: 10.1049/iet-ipr.2013.0049.</span> [<a href="https://dx.doi.org/10.1049%2Fiet-ipr.2013.0049" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IET+Image+Processing&amp;title=Automatic+classification+of+medical+x-ray+images:+hybrid+generative-discriminative+approach&amp;author=M.+R.+Zare&amp;author=A.+Mueen&amp;author=M.+Awedh&amp;author=W.+C.+Seng&amp;volume=7&amp;issue=5&amp;publication_year=2013&amp;pages=523-532&amp;doi=10.1049/iet-ipr.2013.0049&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B3">Ref list</a>]</div><div id="body-link-popper-B4" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">4. <span class="element-citation">Hinton G. E., Osindero S., Teh Y.-W. A fast learning algorithm for deep belief nets. <span><span class="ref-journal"><em>Neural Computation</em>. </span>2006;<span class="ref-vol">18</span>(7):1527–1554. doi: 10.1162/neco.2006.18.7.1527.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/16764513" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1162%2Fneco.2006.18.7.1527" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Neural+Computation&amp;title=A+fast+learning+algorithm+for+deep+belief+nets&amp;author=G.+E.+Hinton&amp;author=S.+Osindero&amp;author=Y.-W.+Teh&amp;volume=18&amp;issue=7&amp;publication_year=2006&amp;pages=1527-1554&amp;pmid=16764513&amp;doi=10.1162/neco.2006.18.7.1527&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B4">Ref list</a>]</div><div id="body-link-popper-B5" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">5. <span class="element-citation">Krizhevsky A., Sutskever I., Hinton G. E. Imagenet classification with deep convolutional neural networks. <span><span class="ref-journal"><em>Communications of the ACM</em>. </span>2012;<span class="ref-vol">60</span>(6):1097–1105. doi: 10.1145/3065386.</span> [<a href="https://dx.doi.org/10.1145%2F3065386" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Communications+of+the+ACM&amp;title=Imagenet+classification+with+deep+convolutional+neural+networks&amp;author=A.+Krizhevsky&amp;author=I.+Sutskever&amp;author=G.+E.+Hinton&amp;volume=60&amp;issue=6&amp;publication_year=2012&amp;pages=1097-1105&amp;doi=10.1145/3065386&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B5">Ref list</a>]</div><div id="body-link-popper-B6" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">6. <span class="element-citation">Simonyan K., Zisserman A. Very deep convolutional networks for large-scale image recognition.   <a href="http://arxiv.org/abs/1409.1556" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">http://arxiv.org/abs/1409.1556</a>.</span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B6">Ref list</a>]</div><div id="body-link-popper-B7" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">7. <span class="element-citation">Barata C., Ruela M., Francisco M., Mendonça T., Marques J. S. Two systems for the detection of melanomas in dermoscopy images using texture and color features. <span><span class="ref-journal"><em>IEEE Systems Journal</em>. </span>2014;<span class="ref-vol">8</span>(3):965–979. doi: 10.1109/jsyst.2013.2271540.</span> [<a href="https://dx.doi.org/10.1109%2Fjsyst.2013.2271540" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IEEE+Systems+Journal&amp;title=Two+systems+for+the+detection+of+melanomas+in+dermoscopy+images+using+texture+and+color+features&amp;author=C.+Barata&amp;author=M.+Ruela&amp;author=M.+Francisco&amp;author=T.+Mendon%C3%A7a&amp;author=J.+S.+Marques&amp;volume=8&amp;issue=3&amp;publication_year=2014&amp;pages=965-979&amp;doi=10.1109/jsyst.2013.2271540&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B7">Ref list</a>]</div><div id="body-link-popper-B13" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">13. <span class="element-citation">Zhang Y., Dong Z., Liu A., et al.  Magnetic resonance brain image classification via stationary wavelet transform and generalized eigenvalue proximal support vector machine. <span><span class="ref-journal"><em>Journal of Medical Imaging and Health Informatics</em>. </span>2015;<span class="ref-vol">5</span>(7):1395–1403. doi: 10.1166/jmihi.2015.1542.</span> [<a href="https://dx.doi.org/10.1166%2Fjmihi.2015.1542" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Journal+of+Medical+Imaging+and+Health+Informatics&amp;title=Magnetic+resonance+brain+image+classification+via+stationary+wavelet+transform+and+generalized+eigenvalue+proximal+support+vector+machine&amp;author=Y.+Zhang&amp;author=Z.+Dong&amp;author=A.+Liu&amp;volume=5&amp;issue=7&amp;publication_year=2015&amp;pages=1395-1403&amp;doi=10.1166/jmihi.2015.1542&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B13">Ref list</a>]</div><div id="body-link-popper-B14" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">14. <span class="element-citation">Li Q., Cai W., Wang X., Zhou Y., Feng D. D., Chen M. Medical image classification with convolutional neural network. Proceedings of 13th International Conference on Control Automation Robotics &amp; Vision (ICARCV); December 2014; Singapore. IEEE; pp. 844–848. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Li+Q.+Cai+W.+Wang+X.+Zhou+Y.+Feng+D.+D.+Chen+M.+Medical+image+classification+with+convolutional+neural+network+Proceedings+of+13th+International+Conference+on+Control+Automation+Robotics+&amp;+Vision+(ICARCV)+December+2014+Singapore+IEEE+844+848+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B14">Ref list</a>]</div><div id="body-link-popper-B16" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">16. <span class="element-citation">Shin H.-C., Roth H. R., Gao M., et al.  Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning. <span><span class="ref-journal"><em>IEEE Transactions on Medical Imaging</em>. </span>2016;<span class="ref-vol">35</span>(5):1285–1298. doi: 10.1109/tmi.2016.2528162.</span> <span class="nowrap">[<a class="int-reflink" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4890616/">PMC free article</a>]</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/26886976" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1109%2Ftmi.2016.2528162" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IEEE+Transactions+on+Medical+Imaging&amp;title=Deep+convolutional+neural+networks+for+computer-aided+detection:+CNN+architectures,+dataset+characteristics+and+transfer+learning&amp;author=H.-C.+Shin&amp;author=H.+R.+Roth&amp;author=M.+Gao&amp;volume=35&amp;issue=5&amp;publication_year=2016&amp;pages=1285-1298&amp;pmid=26886976&amp;doi=10.1109/tmi.2016.2528162&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B16">Ref list</a>]</div><div id="body-link-popper-B17" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">17. <span class="element-citation">Ahn E., Kumar A., Kim J., Li C., Feng D., Fulham M. X-ray image classification using domain transferred convolutional neural networks and local sparse spatial pyramid. Proceedings of 2016 IEEE International Symposium on Biomedical Imaging (ISBI); April 2016; Prague, Czech Republic. IEEE; pp. 855–858. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Ahn+E.+Kumar+A.+Kim+J.+Li+C.+Feng+D.+Fulham+M.+X-ray+image+classification+using+domain+transferred+convolutional+neural+networks+and+local+sparse+spatial+pyramid+Proceedings+of+2016+IEEE+International+Symposium+on+Biomedical+Imaging+(ISBI)+April+2016+Prague,+Czech+Republic+IEEE+855+858+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B17">Ref list</a>]</div><div id="body-link-popper-B10" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">10. <span class="element-citation">Riaz F., Hassan A., Nisar R., Dinis-Ribeiro M., Coimbra M. Content-adaptive region-based color texture descriptors for medical images. <span><span class="ref-journal"><em>IEEE Journal of Biomedical and Health Informatics</em>. </span>2017;<span class="ref-vol">21</span>(1):162–171. doi: 10.1109/jbhi.2015.2492464.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/26513811" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1109%2Fjbhi.2015.2492464" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IEEE+Journal+of+Biomedical+and+Health+Informatics&amp;title=Content-adaptive+region-based+color+texture+descriptors+for+medical+images&amp;author=F.+Riaz&amp;author=A.+Hassan&amp;author=R.+Nisar&amp;author=M.+Dinis-Ribeiro&amp;author=M.+Coimbra&amp;volume=21&amp;issue=1&amp;publication_year=2017&amp;pages=162-171&amp;pmid=26513811&amp;doi=10.1109/jbhi.2015.2492464&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B10">Ref list</a>]</div><div id="body-link-popper-B11" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">11. <span class="element-citation">Ramirez J., Gorriz J. M., Segovia F., et al.  Computer aided diagnosis system for the alzheimer’s disease based on partial least squares and random forest spect image classification. <span><span class="ref-journal"><em>Neuroscience Letters</em>. </span>2010;<span class="ref-vol">472</span>(2):99–103. doi: 10.1016/j.neulet.2010.01.056.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/20117177" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1016%2Fj.neulet.2010.01.056" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Neuroscience+Letters&amp;title=Computer+aided+diagnosis+system+for+the+alzheimer%E2%80%99s+disease+based+on+partial+least+squares+and+random+forest+spect+image+classification&amp;author=J.+Ramirez&amp;author=J.+M.+Gorriz&amp;author=F.+Segovia&amp;volume=472&amp;issue=2&amp;publication_year=2010&amp;pages=99-103&amp;pmid=20117177&amp;doi=10.1016/j.neulet.2010.01.056&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B11">Ref list</a>]</div><div id="body-link-popper-B12" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">12. <span class="element-citation">Zhang Y., Chen S., Wang S., Yang J., Phillips P. Magnetic resonance brain image classification based on weighted-type fractional fourier transform and nonparallel support vector machine. <span><span class="ref-journal"><em>International Journal of Imaging Systems and Technology</em>. </span>2015;<span class="ref-vol">25</span>(4):317–327. doi: 10.1002/ima.22144.</span> [<a href="https://dx.doi.org/10.1002%2Fima.22144" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=International+Journal+of+Imaging+Systems+and+Technology&amp;title=Magnetic+resonance+brain+image+classification+based+on+weighted-type+fractional+fourier+transform+and+nonparallel+support+vector+machine&amp;author=Y.+Zhang&amp;author=S.+Chen&amp;author=S.+Wang&amp;author=J.+Yang&amp;author=P.+Phillips&amp;volume=25&amp;issue=4&amp;publication_year=2015&amp;pages=317-327&amp;doi=10.1002/ima.22144&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B12">Ref list</a>]</div><div id="body-link-popper-B19" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">19. <span class="element-citation">Fernando B., Fromont E., Muselet D., Sebban M. Discriminative feature fusion for image classification. Proceedings of 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); June 2012; Providence, RI, USA. IEEE; pp. 3434–3441. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Fernando+B.+Fromont+E.+Muselet+D.+Sebban+M.+Discriminative+feature+fusion+for+image+classification+Proceedings+of+2012+IEEE+Conference+on+Computer+Vision+and+Pattern+Recognition+(CVPR)+June+2012+Providence,+RI,+USA+IEEE+3434+3441+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B19">Ref list</a>]</div><div id="body-link-popper-B8" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">8. <span class="element-citation">Iyatomi H., Oka H., Celebi M. E., et al.  An improved internet-based melanoma screening system with dermatologist-like tumor area extraction algorithm. <span><span class="ref-journal"><em>Computerized Medical Imaging and Graphics</em>. </span>2008;<span class="ref-vol">32</span>(7):566–579. doi: 10.1016/j.compmedimag.2008.06.005.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/18703311" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1016%2Fj.compmedimag.2008.06.005" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Computerized+Medical+Imaging+and+Graphics&amp;title=An+improved+internet-based+melanoma+screening+system+with+dermatologist-like+tumor+area+extraction+algorithm&amp;author=H.+Iyatomi&amp;author=H.+Oka&amp;author=M.+E.+Celebi&amp;volume=32&amp;issue=7&amp;publication_year=2008&amp;pages=566-579&amp;pmid=18703311&amp;doi=10.1016/j.compmedimag.2008.06.005&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B8">Ref list</a>]</div><div id="body-link-popper-B9" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">9. <span class="element-citation">Stoecker W. V., Wronkiewiecz M., Chowdhury R., et al.  Detection of granularity in dermoscopy images of malignant melanoma using color and texture features. <span><span class="ref-journal"><em>Computerized Medical Imaging and Graphics</em>. </span>2011;<span class="ref-vol">35</span>(2):144–147. doi: 10.1016/j.compmedimag.2010.09.005.</span> <span class="nowrap">[<a class="int-reflink" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3159567/">PMC free article</a>]</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/21036538" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1016%2Fj.compmedimag.2010.09.005" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Computerized+Medical+Imaging+and+Graphics&amp;title=Detection+of+granularity+in+dermoscopy+images+of+malignant+melanoma+using+color+and+texture+features&amp;author=W.+V.+Stoecker&amp;author=M.+Wronkiewiecz&amp;author=R.+Chowdhury&amp;volume=35&amp;issue=2&amp;publication_year=2011&amp;pages=144-147&amp;pmid=21036538&amp;doi=10.1016/j.compmedimag.2010.09.005&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B9">Ref list</a>]</div><div id="body-link-popper-B20" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">20. <span class="element-citation">Chan T., Jia K., Gao S., Lu J., Zeng Z., Ma Y. PCANet: A simple deep learning baseline for image classification? <span><span class="ref-journal"><em>IEEE Transactions on Image Processing</em>. </span>2015;<span class="ref-vol">24</span>(12):5017–5032. doi: 10.1109/tip.2015.2475625.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/26340772" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1109%2Ftip.2015.2475625" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IEEE+Transactions+on+Image+Processing&amp;title=PCANet:+A+simple+deep+learning+baseline+for+image+classification?&amp;author=T.+Chan&amp;author=K.+Jia&amp;author=S.+Gao&amp;author=J.+Lu&amp;author=Z.+Zeng&amp;volume=24&amp;issue=12&amp;publication_year=2015&amp;pages=5017-5032&amp;pmid=26340772&amp;doi=10.1109/tip.2015.2475625&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B20">Ref list</a>]</div><div id="body-link-popper-B21" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">21. <span class="element-citation">Zeng R., Wu J., Shao Z., et al.  Color image classification via quaternion principal component analysis network. <span><span class="ref-journal"><em>Neurocomputing</em>. </span>2016;<span class="ref-vol">216</span>:416–428. doi: 10.1016/j.neucom.2016.08.006.</span> [<a href="https://dx.doi.org/10.1016%2Fj.neucom.2016.08.006" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Neurocomputing&amp;title=Color+image+classification+via+quaternion+principal+component+analysis+network&amp;author=R.+Zeng&amp;author=J.+Wu&amp;author=Z.+Shao&amp;volume=216&amp;publication_year=2016&amp;pages=416-428&amp;doi=10.1016/j.neucom.2016.08.006&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B21">Ref list</a>]</div><div id="body-link-popper-B15" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">15. <span class="element-citation">Bar Y., Diamant I., Wolf L., Greenspan H. Deep learning with non-medical training used for chest pathology identification. Proceedings of SPIE Medical Imaging; February 2015; Orlando, FL, USA. International Society for Optics and Photonics; p. p. 94140V. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Bar+Y.+Diamant+I.+Wolf+L.+Greenspan+H.+Deep+learning+with+non-medical+training+used+for+chest+pathology+identification+Proceedings+of+SPIE+Medical+Imaging+February+2015+Orlando,+FL,+USA+International+Society+for+Optics+and+Photonics+p.+94140V+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B15">Ref list</a>]</div><div id="body-link-popper-B22" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">22. <span class="element-citation">Rakotomamonjy A., Petitjean C., Salaun M., Thiberville L. Scattering features for lung cancer detection in fibered confocal fluorescence microscopy images. <span><span class="ref-journal"><em>Artificial Intelligence in Medicine</em>. </span>2014;<span class="ref-vol">61</span>(2):105–118. doi: 10.1016/j.artmed.2014.05.003.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/24877616" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1016%2Fj.artmed.2014.05.003" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Artificial+Intelligence+in+Medicine&amp;title=Scattering+features+for+lung+cancer+detection+in+fibered+confocal+fluorescence+microscopy+images&amp;author=A.+Rakotomamonjy&amp;author=C.+Petitjean&amp;author=M.+Salaun&amp;author=L.+Thiberville&amp;volume=61&amp;issue=2&amp;publication_year=2014&amp;pages=105-118&amp;pmid=24877616&amp;doi=10.1016/j.artmed.2014.05.003&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B22">Ref list</a>]</div><div id="body-link-popper-B23" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">23. <span class="element-citation">Bruna J., Mallat S. Invariant scattering convolution networks. <span><span class="ref-journal"><em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>. </span>2013;<span class="ref-vol">35</span>(8):1872–1886. doi: 10.1109/tpami.2012.230.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/23787341" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1109%2Ftpami.2012.230" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&amp;title=Invariant+scattering+convolution+networks&amp;author=J.+Bruna&amp;author=S.+Mallat&amp;volume=35&amp;issue=8&amp;publication_year=2013&amp;pages=1872-1886&amp;pmid=23787341&amp;doi=10.1109/tpami.2012.230&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B23">Ref list</a>]</div><div id="body-link-popper-B24" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">24. <span class="element-citation">Cruzroa A., Basavanhally A., Gonzalez F. A., et al.  Automatic detection of invasive ductal carcinoma in whole slide images with convolutional neural networks. Proceedings of SPIE; May 2014; Houston, TX, USA. p. p. 904103. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Cruzroa+A.+Basavanhally+A.+Gonzalez+F.+A.+Automatic+detection+of+invasive+ductal+carcinoma+in+whole+slide+images+with+convolutional+neural+networks+Proceedings+of+SPIE+May+2014+Houston,+TX,+USA+p.+904103+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B24">Ref list</a>]</div><div id="body-link-popper-B25" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">25. <span class="element-citation">Sun Y., Wang X., Tang X. Deep Learning face representation from predicting 10,000 classes. Proceeding of 2014 IEEE Conference on Computer Vision and Pattern Recognition; June 2014; Columbus, OH, USA. pp. 1891–1898. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Sun+Y.+Wang+X.+Tang+X.+Deep+Learning+face+representation+from+predicting+10,000+classes+Proceeding+of+2014+IEEE+Conference+on+Computer+Vision+and+Pattern+Recognition+June+2014+Columbus,+OH,+USA+1891+1898+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B25">Ref list</a>]</div><div id="body-link-popper-B26" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">26. <span class="element-citation">Li Z., Liu Y., Hayward R. F., Walker R. A. Color and texture feature fusion using kernel pca with application to object-based vegetation species classification. Proceedings of 2010 IEEE International Conference on Image Processing; September 2010; Hong Kong, China. pp. 2701–2704. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Li+Z.+Liu+Y.+Hayward+R.+F.+Walker+R.+A.+Color+and+texture+feature+fusion+using+kernel+pca+with+application+to+object-based+vegetation+species+classification+Proceedings+of+2010+IEEE+International+Conference+on+Image+Processing+September+2010+Hong+Kong,+China+2701+2704+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B26">Ref list</a>]</div><div id="body-link-popper-B27" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">27. <span class="element-citation">Lecun Y., Boser B., Denker J. S., et al.  Backpropagation applied to handwritten zip code recognition. <span><span class="ref-journal"><em>Neural Computation</em>. </span>1989;<span class="ref-vol">1</span>(4):541–551. doi: 10.1162/neco.1989.1.4.541.</span> [<a href="https://dx.doi.org/10.1162%2Fneco.1989.1.4.541" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Neural+Computation&amp;title=Backpropagation+applied+to+handwritten+zip+code+recognition&amp;author=Y.+Lecun&amp;author=B.+Boser&amp;author=J.+S.+Denker&amp;volume=1&amp;issue=4&amp;publication_year=1989&amp;pages=541-551&amp;doi=10.1162/neco.1989.1.4.541&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B27">Ref list</a>]</div><div id="body-link-popper-B28" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">28. <span class="element-citation">Nair V., Hinton G. E. Rectified linear units improve restricted boltzmann machines. Proceedings of International Conference on Machine Learning; June 2010; Haifa, Israel. pp. 807–814. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Nair+V.+Hinton+G.+E.+Rectified+linear+units+improve+restricted+boltzmann+machines+Proceedings+of+International+Conference+on+Machine+Learning+June+2010+Haifa,+Israel+807+814+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B28">Ref list</a>]</div><div id="body-link-popper-B29" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">29. <span class="element-citation">Mishkin D., Matas J. All you need is a good init.   <a href="http://arxiv.org/abs/1511.06422" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">http://arxiv.org/abs/1511.06422</a>.</span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B29">Ref list</a>]</div><div id="body-link-popper-B30" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">30. <span class="element-citation">Szegedy C., Liu W., Jia Y., et al.  Going deeper with convolutions. Proceedings of 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); June 2015; Boston, MA, USA. pp. 1–9. <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Szegedy+C.+Liu+W.+Jia+Y.+Going+deeper+with+convolutions+Proceedings+of+2015+IEEE+Conference+on+Computer+Vision+and+Pattern+Recognition+(CVPR)+June+2015+Boston,+MA,+USA+1+9+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B30">Ref list</a>]</div><div id="body-link-popper-B31" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">31. <span class="element-citation">Stanley R. J., Stoecker W. V., Moss R. H. A relative color approach to color discrimination for malignant melanoma detection in dermoscopy images. <span><span class="ref-journal"><em>Skin Research and Technology</em>. </span>2007;<span class="ref-vol">13</span>(1):62–72. doi: 10.1111/j.1600-0846.2007.00192.x.</span> <span class="nowrap">[<a class="int-reflink" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3184887/">PMC free article</a>]</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/17250534" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1111%2Fj.1600-0846.2007.00192.x" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Skin+Research+and+Technology&amp;title=A+relative+color+approach+to+color+discrimination+for+malignant+melanoma+detection+in+dermoscopy+images&amp;author=R.+J.+Stanley&amp;author=W.+V.+Stoecker&amp;author=R.+H.+Moss&amp;volume=13&amp;issue=1&amp;publication_year=2007&amp;pages=62-72&amp;pmid=17250534&amp;doi=10.1111/j.1600-0846.2007.00192.x&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B31">Ref list</a>]</div><div id="body-link-popper-B32" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">32. <span class="element-citation">Celebi M. E., Kingravi H. A., Uddin B., et al.  A methodological approach to the classification of dermoscopy images. <span><span class="ref-journal"><em>Computerized Medical Imaging and Graphics</em>. </span>2007;<span class="ref-vol">31</span>(6):362–373. doi: 10.1016/j.compmedimag.2007.01.003.</span> <span class="nowrap">[<a class="int-reflink" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3192405/">PMC free article</a>]</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/17387001" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1016%2Fj.compmedimag.2007.01.003" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Computerized+Medical+Imaging+and+Graphics&amp;title=A+methodological+approach+to+the+classification+of+dermoscopy+images&amp;author=M.+E.+Celebi&amp;author=H.+A.+Kingravi&amp;author=B.+Uddin&amp;volume=31&amp;issue=6&amp;publication_year=2007&amp;pages=362-373&amp;pmid=17387001&amp;doi=10.1016/j.compmedimag.2007.01.003&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B32">Ref list</a>]</div><div id="body-link-popper-B33" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">33. <span class="element-citation">Rubegni P., Cevenini G., Burroni M., et al.  Automated diagnosis of pigmented skin lesions. <span><span class="ref-journal"><em>International Journal of Cancer</em>. </span>2002;<span class="ref-vol">101</span>(6):576–580. doi: 10.1002/ijc.10620.</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/12237900" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="https://dx.doi.org/10.1002%2Fijc.10620" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=International+Journal+of+Cancer&amp;title=Automated+diagnosis+of+pigmented+skin+lesions&amp;author=P.+Rubegni&amp;author=G.+Cevenini&amp;author=M.+Burroni&amp;volume=101&amp;issue=6&amp;publication_year=2002&amp;pages=576-580&amp;pmid=12237900&amp;doi=10.1002/ijc.10620&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B33">Ref list</a>]</div><div id="body-link-popper-B34" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">34. <span class="element-citation">Ramteke R., Khachane M. Automatic medical image classification and abnormality detection using k-nearest neighbour. <span><span class="ref-journal"><em>International Journal of Advanced Computer Reasearch</em>. </span>2012;<span class="ref-vol">2</span>(4):190–196.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=International+Journal+of+Advanced+Computer+Reasearch&amp;title=Automatic+medical+image+classification+and+abnormality+detection+using+k-nearest+neighbour&amp;author=R.+Ramteke&amp;author=M.+Khachane&amp;volume=2&amp;issue=4&amp;publication_year=2012&amp;pages=190-196&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B34">Ref list</a>]</div><div id="body-link-popper-B35" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">35. <span class="element-citation">Huang J., Ling C. X. Using auc and accuracy in evaluating learning algorithms. <span><span class="ref-journal"><em>IEEE Transactions on knowledge and Data Engineering</em>. </span>2005;<span class="ref-vol">17</span>(3):299–310. doi: 10.1109/tkde.2005.50.</span> [<a href="https://dx.doi.org/10.1109%2Ftkde.2005.50" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IEEE+Transactions+on+knowledge+and+Data+Engineering&amp;title=Using+auc+and+accuracy+in+evaluating+learning+algorithms&amp;author=J.+Huang&amp;author=C.+X.+Ling&amp;volume=17&amp;issue=3&amp;publication_year=2005&amp;pages=299-310&amp;doi=10.1109/tkde.2005.50&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B35">Ref list</a>]</div><div id="body-link-popper-B36" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">36. <span class="element-citation">Guo Y., Yao A., Chen Y.  <span class="ref-journal"><em>Advances in Neural Information Processing Systems</em>.</span> Cambridge, MA, USA: MIT Press; 2016. Dynamic network surgery for efficient DNNs; pp. 1379–1387. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?title=Advances+in+Neural+Information+Processing+Systems&amp;author=Y.+Guo&amp;author=A.+Yao&amp;author=Y.+Chen&amp;publication_year=2016&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B36">Ref list</a>]</div><div id="body-link-popper-B37" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">37. <span class="element-citation">Han S., Pool J., Tran J., Dally W. J. Learning both weights and connections for efficient neural networks. 2015. pp. 1135–1143.  <a href="https://arxiv.org/abs/1506.02626" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">https://arxiv.org/abs/1506.02626</a>.</span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B37">Ref list</a>]</div><div id="body-link-popper-B38" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">38. <span class="element-citation">Lin M., Chen Q., Yan S. Network in network. Proceedings of International Conference on Learning Representations; April 2014; Banff, Canada.  <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Lin+M.+Chen+Q.+Yan+S.+Network+in+network+Proceedings+of+International+Conference+on+Learning+Representations+April+2014+Banff,+Canada+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B38">Ref list</a>]</div><div id="body-link-popper-B39" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">39. <span class="element-citation">Ma G., Yang X., Zhang B., Shi Z. Multi-feature fusion deep networks. <span><span class="ref-journal"><em>Neurocomputing</em>. </span>2016;<span class="ref-vol">218</span>:164–171. doi: 10.1016/j.neucom.2016.08.059.</span> [<a href="https://dx.doi.org/10.1016%2Fj.neucom.2016.08.059" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Neurocomputing&amp;title=Multi-feature+fusion+deep+networks&amp;author=G.+Ma&amp;author=X.+Yang&amp;author=B.+Zhang&amp;author=Z.+Shi&amp;volume=218&amp;publication_year=2016&amp;pages=164-171&amp;doi=10.1016/j.neucom.2016.08.059&amp;" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B39">Ref list</a>]</div><div id="body-link-popper-B40" class="body-link-popper ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic" style="display: none; width: 30em; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true">40. <span class="element-citation">Tiwari P., Viswanath S., Lee G., Madabhushi A. Multi-modal data fusion schemes for integrated classification of imaging and non-imaging biomedical data. Proceedings of 2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro; March 2011; Chicago, Illinois, USA. pp. 165–168. <span class="nowrap">[<a class="int-reflink" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4335721/">PMC free article</a>]</span> [<a href="https://www.ncbi.nlm.nih.gov/pubmed/25705325" target="pmc_ext" ref="reftype=pubmed&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar?q=Tiwari+P.+Viswanath+S.+Lee+G.+Madabhushi+A.+Multi-modal+data+fusion+schemes+for+integrated+classification+of+imaging+and+non-imaging+biomedical+data+Proceedings+of+2011+IEEE+International+Symposium+on+Biomedical+Imaging:+From+Nano+to+Macro+March+2011+Chicago,+Illinois,+USA+165+168+" target="pmc_ext" ref="reftype=other&amp;article-id=6157177&amp;issue-id=306880&amp;journal-id=526&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar" role="button" aria-expanded="false" aria-haspopup="true">Google Scholar</a>]</span></span> [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#B40">Ref list</a>]</div></div>
                        </div>
                        <div class="bottom">
                            
                            <div id="NCBIFooter_dynamic">
    <a id="help-desk-link" class="help_desk" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/" target="_blank">Support Center</a>
    <a id="help-desk-link" class="help_desk" href="https://support.ncbi.nlm.nih.gov/ics/support/KBList.asp?Time=2019-05-25T09:23:50-04:00&amp;Snapshot=%2Fprojects%2FPMC%2FPMCViewer@4.46&amp;Host=portal102&amp;ncbi_phid=CE895B0ACE93DFA10000000001BF0111&amp;ncbi_session=CE895B0ACE941E51_0447SID&amp;from=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6157177%2F&amp;Db=pmc&amp;folderID=132&amp;Ncbi_App=pmc&amp;Page=literature&amp;style=classic&amp;deptID=28049" target="_blank">Support Center</a>
    
</div>

                            <div class="footer" id="footer">
    
    <div class="subfooter"> </div><script type="text/javascript" src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/preloaderWidget.js.download"> </script>
    <div id="external-disclaimer" class="offscreen_noflow">
        External link. Please review our <a href="https://www.nlm.nih.gov/privacy.html">privacy policy</a>.
    </div>    
    <div id="ncbifooter" class="contact_info">      
        <div id="footer-contents-right">
            <div id="nlm_thumb_logo">
                <a href="https://www.nlm.nih.gov/" title="NLM">NLM</a>
            </div>
            <div id="nih_thumb_logo">
                <a href="https://www.nih.gov/" title="NIH">NIH</a>
            </div>
            <div id="hhs_thumb_logo">
                <a href="https://www.hhs.gov/" title="DHHS">DHHS</a>
            </div>
            <div id="usagov_thumb_logo">
                <a href="https://www.usa.gov/" title="USA.gov">USA.gov</a>
            </div>         
        </div>
        
        <div id="footer-contents-left">
            <p class="address vcard">
                <span class="url">
                    <a class="fn url newdomain" href="https://www.ncbi.nlm.nih.gov/">National Center for
                        Biotechnology Information</a>,
                </span> <span class="org url newdomain"><a href="https://www.nlm.nih.gov/">U.S. National Library of Medicine</a></span>
                <span class="adr">
                    <span class="street-address">8600 Rockville Pike</span>, <span class="locality">Bethesda</span>
                    <span class="region">MD</span>, <span class="postal-code">20894</span>
                    <span class="country-name">USA</span>
                </span>
            </p>
            
            <a href="https://www.ncbi.nlm.nih.gov/home/about/policies.shtml">Policies and Guidelines</a> | <a href="https://www.ncbi.nlm.nih.gov/home/about/contact.shtml">Contact</a>
        </div>
    </div>
    <script type="text/javascript" src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/InstrumentPageStarterJS.js.download"> </script>    
    <script type="text/javascript" src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/hfjs2.js.download"> </script>
</div>
                        </div>
                    </div>
                    <!--/.page-->
                </div>
                <!--/.wrap-->
            </div><!-- /.twelve_col -->
        <div class="ui-ncbiautocomplete-holder shadow ui-ncbiautocomplete-holder-clearfix" aria-live="assertive" style="display: none; top: 64.6001px; left: 458.8px; width: 670px; z-index: 1001;"><ul class="ui-ncbiautocomplete-options" role="listbox" aria-activedescendant="term"></ul><div class="ui-ncbiautocomplete-actions shadow" style="display: block;"><a class="ui-ncbiautocomplete-link-pref" style="display: none;">Preferences</a><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157177/#" class="ui-ncbiautocomplete-link-off ui-ncbiautocomplete-link-pref-right" style="display: block;">Turn off</a></div></div><div style="display: none; top: -100px; left: -100px;" aria-live="assertive" aria-hidden="true" class="ui-helper-reset ui-ncbipopper-wrapper ui-ncbipopper-basic">External link. Please review our <a href="https://www.nlm.nih.gov/privacy.html">privacy policy</a>.</div></div>
        <!-- /.grid -->

        <span class="PAFAppResources"></span>
        
        <!-- BESelector tab -->
        
        
        
        <noscript><img alt="statistics" src="/stat?jsdisabled=true&amp;ncbi_db=pmc&amp;ncbi_pdid=article&amp;ncbi_acc=&amp;ncbi_domain=cin&amp;ncbi_report=record&amp;ncbi_type=fulltext&amp;ncbi_objectid=&amp;ncbi_pcid=/articles/PMC6157177/&amp;ncbi_app=pmc" /></noscript>
        
        
        <!-- usually for JS scripts at page bottom -->
        <!--<component id="PageFixtures" label="styles"></component>-->
    

<!-- CE895B0ACE941E51_0447SID /projects/PMC/PMCViewer@4.46 portal102 v4.1.r585844 Mon, May 06 2019 02:53:16 -->

<script type="text/javascript" src="./Medical Image Classification Based on Deep Features Extracted by Deep Model and Statistic Feature Fusion with Multilayer Perceptron__files/4065628.js.download" snapshot="pmc"></script>
<div class="ui-dialog ui-widget ui-widget-content ui-corner-all ui-front" tabindex="-1" role="dialog" aria-describedby="epubDialog" aria-labelledby="ui-id-1" style="display: none;"><div class="ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix"><span id="ui-id-1" class="ui-dialog-title">Making articles easier to read in PMC</span><button type="button" class="ui-button ui-widget ui-state-default ui-corner-all ui-button-icon-only ui-dialog-titlebar-close" role="button" title="Close"><span class="ui-button-icon-primary ui-icon ui-icon-closethick"></span><span class="ui-button-text">Close</span></button></div><div id="epubDialog" style="display: block;" class="ui-dialog-content ui-widget-content">  <p>We are experimenting with display styles that make it easier to read articles     in PMC.     Our first effort uses eBook readers, which have several "ease of reading"     features already built in.</p>  <p>These PMC articles are best viewed in the <em>iBooks     reader</em>. You may notice problems with the display of certain parts of an article     in other eReaders.</p>  <div class="ui-dialog-buttonpane ui-widget-content ui-helper-clearfix"><button id="cancelEpub" class="ui-state-default ui-corner-all">Cancel</button><button id="downloadEpub" style="float: left" class="ui-state-default ui-corner-all">Download article</button></div>          </div></div><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_Size3, sans-serif;"></div></div></body></html>